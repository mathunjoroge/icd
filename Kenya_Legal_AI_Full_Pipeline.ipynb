{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mathunjoroge/icd/blob/master/Kenya_Legal_AI_Full_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01189c12-3736-4a4b-858c-91e04e6b6f1d",
        "id": "r4BfeHtc9c-F"
      },
      "source": [
        "# Install required libraries with specific versions\n",
        "!pip install -q --upgrade pip\n",
        "!pip install -q --upgrade git+https://github.com/unslothai/unsloth.git@main\n",
        "!pip install -q --upgrade transformers trl peft accelerate datasets bitsandbytes"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "id": "r4BfeHtc9c-F"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfiECg469gQt"
      },
      "source": [
        "# Install required libraries with specific versions\n",
        "!pip install -q --upgrade pip\n",
        "!pip install -q --upgrade git+https://github.com/unslothai/unsloth.git@main\n",
        "!pip install -q --upgrade transformers trl peft accelerate datasets bitsandbytes"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "YfiECg469gQt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "007c8229",
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "007c8229"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime\n",
        "from typing import List, Set, Dict, Optional\n",
        "from urllib.parse import urljoin\n",
        "import io\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from webdriver_manager.chrome import ChromeDriverManager\n",
        "\n",
        "# Try to import PDF libraries, but make them optional\n",
        "try:\n",
        "    import PyPDF2\n",
        "    PDF_SUPPORT = True\n",
        "except ImportError:\n",
        "    PDF_SUPPORT = False\n",
        "    print(\"Warning: PyPDF2 not installed. PDF content extraction will be limited.\")\n",
        "\n",
        "try:\n",
        "    import pdfplumber\n",
        "    PDFPLUMBER_SUPPORT = True\n",
        "except ImportError:\n",
        "    PDFPLUMBER_SUPPORT = False\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                               CONFIGURATION                                 #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    BASE_PROJECT_DIR: str = os.path.join(os.path.expanduser(\"~\"), \"projects\", \"kenya_law\")\n",
        "\n",
        "    LOG_DIR: str = field(init=False)\n",
        "    DATA_DIR: str = field(init=False)\n",
        "    LOG_FILE: str = field(init=False)\n",
        "    DATA_FILE: str = field(init=False)\n",
        "    DEBUG_DIR: str = field(init=False)\n",
        "    CONSTITUTION_FILE: str = field(init=False)\n",
        "    ACTS_FILE: str = field(init=False)\n",
        "    SUBSIDIARY_FILE: str = field(init=False)\n",
        "    COUNTIES_FILE: str = field(init=False)\n",
        "    PDF_DIR: str = field(init=False)  # NEW: Directory to store downloaded PDFs\n",
        "\n",
        "    MAX_CASES: int = None\n",
        "    MAX_PAGES: int = None\n",
        "    MAX_COUNTY_LAWS: int = 50\n",
        "    REQUEST_TIMEOUT: int = 30\n",
        "    SELENIUM_TIMEOUT: int = 45\n",
        "    YEAR_START: int = 2020\n",
        "\n",
        "    BASE_URL: str = \"https://kenyalaw.org\"\n",
        "    NEW_BASE_URL: str = \"https://new.kenyalaw.org\"\n",
        "    SEARCH_URL: str = \"https://new.kenyalaw.org/search/\"\n",
        "    JUDGMENTS_URL: str = \"https://new.kenyalaw.org/judgments/\"\n",
        "    COUNTIES_URL: str = \"https://new.kenyalaw.org/legislation/counties\"\n",
        "    ACTS_TOC_URL: str = \"https://new.kenyalaw.org/legislation/\"\n",
        "\n",
        "    LOCAL_CHROMEDRIVER_PATH: Optional[str] = None\n",
        "    CHROME_HEADLESS: bool = True\n",
        "\n",
        "    KEYWORDS: List[str] = field(default_factory=lambda: [\n",
        "        \"constitution\", \"human rights\", \"land\", \"election\", \"criminal\", \"civil\",\n",
        "        \"jurisdiction\", \"appeal\", \"judicial review\", \"injunction\", \"contract\"\n",
        "    ])\n",
        "\n",
        "    MAX_SCRAPE_WORKERS: int = 10\n",
        "    ENABLE_PDF_EXTRACTION: bool = True  # NEW: Control PDF processing\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        self.LOG_DIR = os.path.join(self.BASE_PROJECT_DIR, \"logs\")\n",
        "        self.DATA_DIR = os.path.join(self.BASE_PROJECT_DIR, \"data\")\n",
        "        self.DEBUG_DIR = os.path.join(self.BASE_PROJECT_DIR, \"debug\")\n",
        "        self.PDF_DIR = os.path.join(self.DATA_DIR, \"pdfs\")  # NEW\n",
        "        self.LOG_FILE = os.path.join(self.LOG_DIR, f\"kenyalaw_scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
        "        self.DATA_FILE = os.path.join(self.DATA_DIR, \"kenya_law_training_data.jsonl\")\n",
        "        self.CONSTITUTION_FILE = os.path.join(self.DATA_DIR, \"constitution.json\")\n",
        "        self.ACTS_FILE = os.path.join(self.DATA_DIR, \"acts_of_kenya.json\")\n",
        "        self.SUBSIDIARY_FILE = os.path.join(self.DATA_DIR, \"subsidiary_legislation.json\")\n",
        "        self.COUNTIES_FILE = os.path.join(self.DATA_DIR, \"county_legislation.json\")\n",
        "\n",
        "        for d in [self.LOG_DIR, self.DATA_DIR, self.DEBUG_DIR, self.PDF_DIR]:\n",
        "            os.makedirs(d, exist_ok=True)\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                                 LOGGING                                    #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "def setup_logging(log_file: str) -> logging.Logger:\n",
        "    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
        "    logger = logging.getLogger(\"KenyaLaw-Scraper-v6.0-FULL\")\n",
        "    logger.setLevel(logging.INFO)\n",
        "    logger.handlers.clear()\n",
        "    fh = logging.FileHandler(log_file, encoding=\"utf-8\")\n",
        "    ch = logging.StreamHandler()\n",
        "    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
        "    fh.setFormatter(formatter)\n",
        "    ch.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "    logger.addHandler(ch)\n",
        "    return logger\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                               DATA HANDLER                                 #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "class DataHandler:\n",
        "    def __init__(self, cfg: Config, log: logging.Logger):\n",
        "        self.cfg = cfg\n",
        "        self.log = log\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def load_existing_case_ids(self) -> Set[str]:\n",
        "        if not os.path.exists(self.cfg.DATA_FILE):\n",
        "            return set()\n",
        "        ids = set()\n",
        "        try:\n",
        "            with open(self.cfg.DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "                for line_num, line in enumerate(f, 1):\n",
        "                    line = line.strip()\n",
        "                    if not line: continue\n",
        "                    try:\n",
        "                        data = json.loads(line)\n",
        "                        if \"case_id\" in data:\n",
        "                            ids.add(data[\"case_id\"])\n",
        "                    except json.JSONDecodeError:\n",
        "                        self.log.warning(f\"Bad JSON at line {line_num}\")\n",
        "            self.log.info(f\"Loaded {len(ids)} existing case IDs\")\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Failed to load IDs: {e}\")\n",
        "        return ids\n",
        "\n",
        "    def save_case(self, case: Dict) -> bool:\n",
        "        try:\n",
        "            json_line = json.dumps(case, ensure_ascii=False)\n",
        "            with self.lock:\n",
        "                with open(self.cfg.DATA_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "                    f.write(json_line + \"\\n\")\n",
        "            self.log.info(f\"Saved case {case['case_id']} ({len(case['text'].split())} words)\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Save failed: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                              PDF HANDLING                                  #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "class PDFHandler:\n",
        "    def __init__(self, cfg: Config, log: logging.Logger):\n",
        "        self.cfg = cfg\n",
        "        self.log = log\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_url: str, pdf_content: bytes) -> Optional[str]:\n",
        "        \"\"\"Extract text from PDF content using multiple methods\"\"\"\n",
        "        if not self.cfg.ENABLE_PDF_EXTRACTION:\n",
        "            return \"PDF_CONTENT_AVAILABLE_BUT_EXTRACTION_DISABLED\"\n",
        "\n",
        "        # Save PDF file for reference\n",
        "        pdf_filename = self._save_pdf_file(pdf_url, pdf_content)\n",
        "\n",
        "        extracted_text = None\n",
        "\n",
        "        # Try pdfplumber first (better for scanned PDFs with OCR)\n",
        "        if PDFPLUMBER_SUPPORT:\n",
        "            extracted_text = self._extract_with_pdfplumber(pdf_content)\n",
        "\n",
        "        # Fallback to PyPDF2\n",
        "        if not extracted_text and PDF_SUPPORT:\n",
        "            extracted_text = self._extract_with_pypdf2(pdf_content)\n",
        "\n",
        "        # Final fallback\n",
        "        if not extracted_text:\n",
        "            extracted_text = self._extract_fallback(pdf_content, pdf_filename)\n",
        "\n",
        "        return extracted_text\n",
        "\n",
        "    def _save_pdf_file(self, pdf_url: str, pdf_content: bytes) -> str:\n",
        "        \"\"\"Save PDF file to disk for reference\"\"\"\n",
        "        try:\n",
        "            # Create filename from URL\n",
        "            filename = re.sub(r'[^a-zA-Z0-9]', '_', pdf_url) + '.pdf'\n",
        "            filepath = os.path.join(self.cfg.PDF_DIR, filename)\n",
        "\n",
        "            with open(filepath, 'wb') as f:\n",
        "                f.write(pdf_content)\n",
        "\n",
        "            return filename\n",
        "        except Exception as e:\n",
        "            self.log.warning(f\"Failed to save PDF file: {e}\")\n",
        "            return \"unknown.pdf\"\n",
        "\n",
        "    def _extract_with_pdfplumber(self, pdf_content: bytes) -> Optional[str]:\n",
        "        \"\"\"Extract text using pdfplumber (better for scanned PDFs)\"\"\"\n",
        "        try:\n",
        "            text_parts = []\n",
        "            with pdfplumber.open(io.BytesIO(pdf_content)) as pdf:\n",
        "                for page in pdf.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        text_parts.append(page_text.strip())\n",
        "\n",
        "            if text_parts:\n",
        "                full_text = '\\n'.join(text_parts)\n",
        "                if len(full_text.split()) > 10:  # Ensure we have substantial text\n",
        "                    return full_text\n",
        "        except Exception as e:\n",
        "            self.log.debug(f\"pdfplumber extraction failed: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _extract_with_pypdf2(self, pdf_content: bytes) -> Optional[str]:\n",
        "        \"\"\"Extract text using PyPDF2\"\"\"\n",
        "        try:\n",
        "            text_parts = []\n",
        "            pdf_file = io.BytesIO(pdf_content)\n",
        "            reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "            for page in reader.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text_parts.append(page_text.strip())\n",
        "\n",
        "            if text_parts:\n",
        "                full_text = '\\n'.join(text_parts)\n",
        "                if len(full_text.split()) > 10:\n",
        "                    return full_text\n",
        "        except Exception as e:\n",
        "            self.log.debug(f\"PyPDF2 extraction failed: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _extract_fallback(self, pdf_content: bytes, pdf_filename: str) -> str:\n",
        "        \"\"\"Fallback method when PDF text extraction fails\"\"\"\n",
        "        self.log.warning(f\"PDF text extraction failed for {pdf_filename}. Content saved to disk.\")\n",
        "        return f\"PDF_CONTENT_UNABLE_TO_EXTRACT_TEXT_SAVED_AS_{pdf_filename}\"\n",
        "\n",
        "    def is_pdf_url(self, url: str) -> bool:\n",
        "        \"\"\"Check if URL points to a PDF document\"\"\"\n",
        "        return url.lower().endswith('.pdf') or '/pdf/' in url.lower()\n",
        "\n",
        "    def download_pdf(self, session: requests.Session, pdf_url: str) -> Optional[bytes]:\n",
        "        \"\"\"Download PDF content\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\",\n",
        "                \"Accept\": \"application/pdf, */*\",\n",
        "                \"Referer\": \"https://new.kenyalaw.org/\"\n",
        "            }\n",
        "\n",
        "            response = session.get(pdf_url, timeout=30, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            content_type = response.headers.get('content-type', '').lower()\n",
        "            if 'pdf' in content_type or response.content[:4] == b'%PDF':\n",
        "                return response.content\n",
        "            else:\n",
        "                self.log.warning(f\"URL {pdf_url} doesn't contain PDF data\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Failed to download PDF from {pdf_url}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                      STATIC CONTENT SCRAPERS (Constitution & Acts)          #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "def save_constitution_data(cfg: Config, log: logging.Logger, data: Dict[str, str]) -> None:\n",
        "    try:\n",
        "        with open(cfg.CONSTITUTION_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "        log.info(f\"Constitution saved → {cfg.CONSTITUTION_FILE}\")\n",
        "    except Exception as e:\n",
        "        log.error(f\"Failed to save constitution: {e}\")\n",
        "\n",
        "def scrape_constitution(cfg: Config, log: logging.Logger) -> None:\n",
        "    log.info(\"Scraping Constitution of Kenya (2010)...\")\n",
        "    if os.path.exists(cfg.CONSTITUTION_FILE):\n",
        "        log.info(f\"Constitution already exists → {cfg.CONSTITUTION_FILE}. Skipping.\")\n",
        "        return\n",
        "\n",
        "    # Try multiple potential sources\n",
        "    SOURCES = [\n",
        "        \"https://new.kenyalaw.org/akn/ke/act/2010/constitution/eng@2010-09-03\",\n",
        "        \"https://new.kenyalaw.org/akn/ke/act/2010/constitution\",\n",
        "        \"https://kenyalaw.org/kl/index.php?id=398\"\n",
        "    ]\n",
        "\n",
        "    session = requests.Session()\n",
        "    session.headers.update({\n",
        "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\",\n",
        "        \"Accept\": \"application/xml, text/xml, text/html, */*\",\n",
        "        \"Referer\": \"https://new.kenyalaw.org/\",\n",
        "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "    })\n",
        "\n",
        "    for source_url in SOURCES:\n",
        "        try:\n",
        "            log.info(f\"Trying source: {source_url}\")\n",
        "            resp = session.get(source_url, timeout=60)\n",
        "            resp.raise_for_status()\n",
        "\n",
        "            content_type = resp.headers.get('content-type', '').lower()\n",
        "\n",
        "            if 'xml' in content_type:\n",
        "                # Parse as XML\n",
        "                soup = BeautifulSoup(resp.content, \"xml\")\n",
        "                body = soup.find(\"body\")\n",
        "                if not body:\n",
        "                    log.warning(f\"No <body> found in XML from {source_url}\")\n",
        "                    continue\n",
        "\n",
        "                data = {}\n",
        "                current_title = \"Preamble\"\n",
        "                current_lines = []\n",
        "\n",
        "                # Extract all relevant elements\n",
        "                for elem in body.find_all(['heading', 'num', 'p', 'chapter', 'part', 'section', 'article']):\n",
        "                    if elem.name in ['heading', 'chapter', 'part']:\n",
        "                        # Save previous section\n",
        "                        if current_lines:\n",
        "                            data[current_title] = \"\\n\".join(current_lines).strip()\n",
        "                            current_lines = []\n",
        "                        # Start new section\n",
        "                        current_title = elem.get_text(strip=True)\n",
        "                        if not current_title:\n",
        "                            current_title = \"Untitled Section\"\n",
        "\n",
        "                    elif elem.name == 'p' and elem.get_text(strip=True):\n",
        "                        text = elem.get_text(strip=True)\n",
        "                        if len(text) > 10:  # Filter out very short paragraphs\n",
        "                            current_lines.append(text)\n",
        "\n",
        "                # Save the last section\n",
        "                if current_lines:\n",
        "                    data[current_title] = \"\\n\".join(current_lines).strip()\n",
        "\n",
        "            else:\n",
        "                # Parse as HTML\n",
        "                soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "\n",
        "                # Try different content selectors\n",
        "                content_selectors = [\n",
        "                    \"div.act-content\",\n",
        "                    \"div.content\",\n",
        "                    \"article\",\n",
        "                    \"main\",\n",
        "                    \"div.container\",\n",
        "                    \"#content\"\n",
        "                ]\n",
        "\n",
        "                content = None\n",
        "                for selector in content_selectors:\n",
        "                    content = soup.select_one(selector)\n",
        "                    if content:\n",
        "                        break\n",
        "\n",
        "                if not content:\n",
        "                    log.warning(f\"No content found with selectors in {source_url}\")\n",
        "                    continue\n",
        "\n",
        "                # Clean up the content\n",
        "                for element in content.select(\"script, style, nav, header, footer, .nav, .header, .footer\"):\n",
        "                    element.decompose()\n",
        "\n",
        "                # Extract text and structure\n",
        "                data = {}\n",
        "                current_section = \"Constitution of Kenya\"\n",
        "                sections = [current_section]\n",
        "                text_lines = []\n",
        "\n",
        "                for element in content.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'div']):\n",
        "                    text = element.get_text(strip=True)\n",
        "                    if not text:\n",
        "                        continue\n",
        "\n",
        "                    # Detect section headers\n",
        "                    if element.name in ['h1', 'h2', 'h3', 'h4']:\n",
        "                        if text_lines:  # Save previous section\n",
        "                            data[current_section] = \"\\n\".join(text_lines).strip()\n",
        "                            text_lines = []\n",
        "                        current_section = text\n",
        "                        sections.append(current_section)\n",
        "                    elif len(text) > 20:  # Substantial content\n",
        "                        text_lines.append(text)\n",
        "\n",
        "                # Save the last section\n",
        "                if text_lines:\n",
        "                    data[current_section] = \"\\n\".join(text_lines).strip()\n",
        "\n",
        "            # Filter out empty or very short sections\n",
        "            filtered_data = {}\n",
        "            for title, content in data.items():\n",
        "                if content and len(content.split()) >= 10:\n",
        "                    clean_content = re.sub(r'\\s+', ' ', content).strip()\n",
        "                    filtered_data[title] = clean_content\n",
        "\n",
        "            # If we have substantial content, save it\n",
        "            if filtered_data and sum(len(c.split()) for c in filtered_data.values()) > 500:\n",
        "                save_constitution_data(cfg, log, filtered_data)\n",
        "                total_words = sum(len(v.split()) for v in filtered_data.values())\n",
        "                log.info(f\"SUCCESS: Constitution scraped → {len(filtered_data)} sections, {total_words:,} words\")\n",
        "                return\n",
        "            else:\n",
        "                log.warning(f\"Insufficient content from {source_url}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            log.warning(f\"Source {source_url} failed: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Fallback: manual structure if all sources fail\n",
        "    log.warning(\"All automated sources failed. Creating placeholder structure.\")\n",
        "    fallback_data = {\n",
        "        \"Preamble\": \"We, the people of Kenya—ACKNOWLEDGING the supremacy of the Almighty God of all creation...\",\n",
        "        \"Chapter One - Sovereignty of the People\": \"1. (1) All sovereign power belongs to the people of Kenya...\",\n",
        "        \"Chapter Two - The Republic\": \"4. (1) Kenya is a sovereign Republic. (2) The Republic of Kenya shall be a multi-party democratic state...\",\n",
        "        \"Note\": \"This is a placeholder. The actual constitution text could not be scraped automatically. Consider manual entry.\"\n",
        "    }\n",
        "    save_constitution_data(cfg, log, fallback_data)\n",
        "    log.info(\"Created fallback constitution structure\")\n",
        "\n",
        "def save_acts_data(cfg: Config, log: logging.Logger, acts: Dict[str, str], subs: Dict[str, str]) -> None:\n",
        "    with open(cfg.ACTS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(acts, f, ensure_ascii=False, indent=2)\n",
        "    log.info(f\"Saved {len(acts)} Acts → {cfg.ACTS_FILE}\")\n",
        "\n",
        "    with open(cfg.SUBSIDIARY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(subs, f, ensure_ascii=False, indent=2)\n",
        "    log.info(f\"Saved {len(subs)} Subsidiary Laws → {cfg.SUBSIDIARY_FILE}\")\n",
        "\n",
        "def save_counties_data(cfg: Config, log: logging.Logger, counties_data: Dict[str, Dict]) -> None:\n",
        "    try:\n",
        "        with open(cfg.COUNTIES_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(counties_data, f, ensure_ascii=False, indent=2)\n",
        "        log.info(f\"County legislation saved → {cfg.COUNTIES_FILE}\")\n",
        "    except Exception as e:\n",
        "        log.error(f\"Failed to save county legislation: {e}\")\n",
        "\n",
        "\n",
        "def scrape_acts_of_kenya(cfg: Config, log: logging.Logger) -> None:\n",
        "    log.info(\"Scraping ALL Acts + Subsidiary Legislation (NEW SITE)...\")\n",
        "    if os.path.exists(cfg.ACTS_FILE) and os.path.exists(cfg.SUBSIDIARY_FILE):\n",
        "        log.info(\"Acts & Subsidiary files exist. Skipping.\")\n",
        "        return\n",
        "\n",
        "    session = requests.Session()\n",
        "    session.headers.update({\n",
        "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\",\n",
        "        \"Accept\": \"text/html\",\n",
        "        \"Referer\": cfg.ACTS_TOC_URL\n",
        "    })\n",
        "\n",
        "    acts_data = {}\n",
        "    subsidiary_data = {}\n",
        "    total_acts = 0\n",
        "    total_subs = 0\n",
        "\n",
        "    try:\n",
        "        resp = session.get(cfg.ACTS_TOC_URL, timeout=cfg.REQUEST_TIMEOUT)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "\n",
        "        main_rows = soup.select('tr.has-children')\n",
        "        log.info(f\"Found {len(main_rows)} parent Acts\")\n",
        "\n",
        "        for row in main_rows:\n",
        "            btn = row.select_one('button[data-bs-toggle=\"collapse\"]')\n",
        "            if not btn:\n",
        "                continue\n",
        "            target = btn.get(\"data-bs-target\", \"\").lstrip(\"#\")\n",
        "            if not target:\n",
        "                continue\n",
        "\n",
        "            link = row.select_one('td.cell-title a')\n",
        "            if not link:\n",
        "                continue\n",
        "            title = link.get_text(strip=True)\n",
        "            href = link.get(\"href\")\n",
        "            url = urljoin(cfg.ACTS_TOC_URL, href)\n",
        "            citation = row.select_one('td.cell-citation')\n",
        "            cap = citation.get_text(strip=True) if citation else \"\"\n",
        "\n",
        "            key = f\"[{cap}] {title}\".strip()\n",
        "            if key in acts_data:\n",
        "                continue\n",
        "\n",
        "            # Scrape main Act\n",
        "            try:\n",
        "                time.sleep(0.8)\n",
        "                r = session.get(url, timeout=cfg.REQUEST_TIMEOUT)\n",
        "                r.raise_for_status()\n",
        "                s = BeautifulSoup(r.text, \"lxml\")\n",
        "                content = s.select_one(\"div.act-content, article, main\")\n",
        "                if content:\n",
        "                    for el in content.select(\"script, style, nav, header, footer, .act-tools\"):\n",
        "                        el.decompose()\n",
        "                    text = content.get_text(separator=\"\\n\", strip=True)\n",
        "                    text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "                    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "                    if len(text.split()) > 200:\n",
        "                        acts_data[key] = text\n",
        "                        total_acts += 1\n",
        "                        log.info(f\"  Success: Act: {key}\")\n",
        "            except Exception as e:\n",
        "                log.error(f\"Failed Act {key}: {e}\")\n",
        "\n",
        "            # Scrape subsidiary\n",
        "            tbody = soup.find(\"tbody\", id=target)\n",
        "            if not tbody:\n",
        "                continue\n",
        "\n",
        "            for sub_row in tbody.select('tr'):\n",
        "                sub_link = sub_row.select_one('td.cell-title a')\n",
        "                if not sub_link:\n",
        "                    continue\n",
        "                sub_title = sub_link.get_text(strip=True)\n",
        "                sub_href = sub_link.get(\"href\")\n",
        "                sub_url = urljoin(cfg.ACTS_TOC_URL, sub_href)\n",
        "                sub_cite = sub_row.select_one('td.cell-citation')\n",
        "                sub_cite_text = sub_cite.get_text(strip=True) if sub_cite else \"\"\n",
        "\n",
        "                sub_key = f\"{sub_cite_text} {sub_title}\".strip()\n",
        "                full_key = f\"[{cap}] {title} → {sub_key}\"\n",
        "\n",
        "                try:\n",
        "                    time.sleep(0.8)\n",
        "                    r = session.get(sub_url, timeout=cfg.REQUEST_TIMEOUT)\n",
        "                    r.raise_for_status()\n",
        "                    s = BeautifulSoup(r.text, \"lxml\")\n",
        "                    content = s.select_one(\"div.act-content, article\")\n",
        "                    if content:\n",
        "                        for el in content.select(\"script, style, nav, header, footer\"):\n",
        "                            el.decompose()\n",
        "                        text = content.get_text(separator=\"\\n\", strip=True)\n",
        "                        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "                        if len(text.split()) > 100:\n",
        "                            subsidiary_data[full_key] = text\n",
        "                            total_subs += 1\n",
        "                            log.info(f\"    Success: Subsidiary: {sub_key}\")\n",
        "                except Exception as e:\n",
        "                    log.error(f\"Failed subsidiary {sub_key}: {e}\")\n",
        "\n",
        "        save_acts_data(cfg, log, acts_data, subsidiary_data)\n",
        "        log.info(f\"SUCCESS: {total_acts} Acts + {total_subs} Subsidiary Laws saved!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log.error(f\"Acts scrape failed: {e}\", exc_info=True)\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                           COUNTY LEGISLATION SCRAPER                        #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "def scrape_county_legislation(cfg: Config, log: logging.Logger) -> None:\n",
        "    \"\"\"Scrape county legislation from all 47 counties\"\"\"\n",
        "    log.info(\"Scraping County Legislation from all 47 counties...\")\n",
        "    if os.path.exists(cfg.COUNTIES_FILE):\n",
        "        log.info(f\"County legislation already exists → {cfg.COUNTIES_FILE}. Skipping.\")\n",
        "        return\n",
        "\n",
        "    session = requests.Session()\n",
        "    pdf_handler = PDFHandler(cfg, log)  # NEW: PDF handler\n",
        "\n",
        "    session.headers.update({\n",
        "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\",\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
        "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "        \"Referer\": cfg.NEW_BASE_URL\n",
        "    })\n",
        "\n",
        "    counties_data = {}\n",
        "    total_county_laws = 0\n",
        "\n",
        "    try:\n",
        "        # Get the main counties page\n",
        "        log.info(f\"Accessing counties page: {cfg.COUNTIES_URL}\")\n",
        "        resp = session.get(cfg.COUNTIES_URL, timeout=cfg.REQUEST_TIMEOUT)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "\n",
        "        # Extract all county links\n",
        "        county_links = []\n",
        "        flow_columns = soup.select('.flow-columns-group')\n",
        "\n",
        "        for column in flow_columns:\n",
        "            links = column.select('a[href^=\"/legislation/ke-\"]')\n",
        "            for link in links:\n",
        "                county_name = link.get_text(strip=True)\n",
        "                county_url = urljoin(cfg.NEW_BASE_URL, link.get('href'))\n",
        "                county_links.append((county_name, county_url))\n",
        "\n",
        "        log.info(f\"Found {len(county_links)} counties to process\")\n",
        "\n",
        "        # Process each county\n",
        "        for county_name, county_url in county_links:\n",
        "            try:\n",
        "                log.info(f\"Processing county: {county_name}\")\n",
        "                time.sleep(1)  # Be respectful\n",
        "\n",
        "                # Get county page\n",
        "                county_resp = session.get(county_url, timeout=cfg.REQUEST_TIMEOUT)\n",
        "                county_resp.raise_for_status()\n",
        "                county_soup = BeautifulSoup(county_resp.text, \"lxml\")\n",
        "\n",
        "                # Extract county laws\n",
        "                county_laws = {}\n",
        "\n",
        "                # Look for laws in tables or lists\n",
        "                law_elements = county_soup.select('tr.has-children, .legislation-item, .law-item')\n",
        "\n",
        "                if not law_elements:\n",
        "                    # Try alternative selectors\n",
        "                    law_elements = county_soup.select('a[href*=\"/akn/ke/act/\"]')\n",
        "\n",
        "                laws_processed = 0\n",
        "\n",
        "                for law_element in law_elements:\n",
        "                    if laws_processed >= cfg.MAX_COUNTY_LAWS:\n",
        "                        break\n",
        "\n",
        "                    try:\n",
        "                        # Extract law link and title\n",
        "                        if law_element.name == 'tr':\n",
        "                            link_elem = law_element.select_one('td.cell-title a')\n",
        "                        else:\n",
        "                            link_elem = law_element\n",
        "\n",
        "                        if not link_elem or not link_elem.get('href'):\n",
        "                            continue\n",
        "\n",
        "                        law_title = link_elem.get_text(strip=True)\n",
        "                        law_url = urljoin(cfg.NEW_BASE_URL, link_elem.get('href'))\n",
        "\n",
        "                        # Skip if it's not a direct law link\n",
        "                        if '/akn/ke/act/' not in law_url:\n",
        "                            continue\n",
        "\n",
        "                        # Scrape the actual law content (now with PDF support)\n",
        "                        time.sleep(0.5)\n",
        "                        law_content = scrape_county_law_content(session, pdf_handler, law_url, log)\n",
        "\n",
        "                        if law_content and len(law_content.split()) > 50:  # Reduced threshold for PDFs\n",
        "                            county_laws[law_title] = {\n",
        "                                'url': law_url,\n",
        "                                'content': law_content,\n",
        "                                'word_count': len(law_content.split()),\n",
        "                                'content_type': 'pdf' if pdf_handler.is_pdf_url(law_url) else 'html'\n",
        "                            }\n",
        "                            laws_processed += 1\n",
        "                            total_county_laws += 1\n",
        "                            log.info(f\"    ✓ County law: {law_title} ({len(law_content.split())} words) [{county_laws[law_title]['content_type'].upper()}]\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        log.warning(f\"Failed to process county law in {county_name}: {e}\")\n",
        "                        continue\n",
        "\n",
        "                # Add county data\n",
        "                if county_laws:\n",
        "                    counties_data[county_name] = {\n",
        "                        'county_url': county_url,\n",
        "                        'laws': county_laws,\n",
        "                        'total_laws': len(county_laws),\n",
        "                        'scraped_at': datetime.now().isoformat()\n",
        "                    }\n",
        "                    log.info(f\"  ✓ {county_name}: {len(county_laws)} laws\")\n",
        "\n",
        "            except Exception as e:\n",
        "                log.error(f\"Failed to process county {county_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Save counties data\n",
        "        save_counties_data(cfg, log, counties_data)\n",
        "        log.info(f\"SUCCESS: County legislation scraped → {len(counties_data)} counties, {total_county_laws} total laws\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log.error(f\"County legislation scrape failed: {e}\", exc_info=True)\n",
        "\n",
        "def scrape_county_law_content(session: requests.Session, pdf_handler: PDFHandler, law_url: str, log: logging.Logger) -> Optional[str]:\n",
        "    \"\"\"Scrape content of an individual county law with PDF support\"\"\"\n",
        "\n",
        "    # Check if it's a PDF URL\n",
        "    if pdf_handler.is_pdf_url(law_url):\n",
        "        log.info(f\"  Detected PDF document: {law_url}\")\n",
        "        pdf_content = pdf_handler.download_pdf(session, law_url)\n",
        "        if pdf_content:\n",
        "            return pdf_handler.extract_text_from_pdf(law_url, pdf_content)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    # Handle HTML content\n",
        "    try:\n",
        "        resp = session.get(law_url, timeout=30)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "\n",
        "        # Check if the page contains a PDF link\n",
        "        pdf_links = soup.select('a[href$=\".pdf\"], a[href*=\"/pdf/\"]')\n",
        "        for pdf_link in pdf_links:\n",
        "            pdf_url = urljoin(law_url, pdf_link.get('href'))\n",
        "            log.info(f\"  Found embedded PDF link: {pdf_url}\")\n",
        "            pdf_content = pdf_handler.download_pdf(session, pdf_url)\n",
        "            if pdf_content:\n",
        "                return pdf_handler.extract_text_from_pdf(pdf_url, pdf_content)\n",
        "\n",
        "        # Try multiple content selectors for HTML laws\n",
        "        content_selectors = [\n",
        "            \"div.act-content\",\n",
        "            \"div.fr-view\",\n",
        "            \"div.content\",\n",
        "            \"article\",\n",
        "            \"main\",\n",
        "            \".law-content\",\n",
        "            \".document-content\"\n",
        "        ]\n",
        "\n",
        "        content = None\n",
        "        for selector in content_selectors:\n",
        "            content = soup.select_one(selector)\n",
        "            if content:\n",
        "                break\n",
        "\n",
        "        if not content:\n",
        "            # Fallback: get body content\n",
        "            content = soup.find('main') or soup.find('article') or soup.find('body')\n",
        "\n",
        "        if content:\n",
        "            # Clean up content\n",
        "            for element in content.select(\"script, style, nav, header, footer, .nav, .header, .footer, .tools\"):\n",
        "                element.decompose()\n",
        "\n",
        "            # Extract text\n",
        "            text = content.get_text(separator=\"\\n\", strip=True)\n",
        "            text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "            return text if len(text.split()) > 50 else None\n",
        "\n",
        "    except Exception as e:\n",
        "        log.warning(f\"Failed to scrape county law content from {law_url}: {e}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                                 CASE LAW SCRAPER                            #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "class KenyaLawScraper:\n",
        "    def __init__(self, cfg: Config, log: logging.Logger):\n",
        "        self.cfg = cfg\n",
        "        self.log = log\n",
        "        self.keywords = {k.lower() for k in cfg.KEYWORDS}\n",
        "        self.seen_case_ids = set()\n",
        "        self.driver = None\n",
        "        self.session = self._create_session()\n",
        "\n",
        "    def _create_session(self):\n",
        "        s = requests.Session()\n",
        "        retry = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
        "        adapter = HTTPAdapter(max_retries=retry)\n",
        "        s.mount(\"http://\", adapter)\n",
        "        s.mount(\"https://\", adapter)\n",
        "        s.headers.update({\n",
        "            \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\",\n",
        "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n",
        "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "            \"Accept-Encoding\": \"gzip, deflate, br\"\n",
        "        })\n",
        "        return s\n",
        "\n",
        "    def _setup_driver(self):\n",
        "        options = Options()\n",
        "        if self.cfg.CHROME_HEADLESS:\n",
        "            options.add_argument(\"--headless\")\n",
        "        options.add_argument(\"--no-sandbox\")\n",
        "        options.add_argument(\"--disable-dev-shm-usage\")\n",
        "        options.add_argument(\"--window-size=1920,1080\")\n",
        "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "        options.add_experimental_option('useAutomationExtension', False)\n",
        "\n",
        "        try:\n",
        "            service = Service(ChromeDriverManager().install())\n",
        "            driver = webdriver.Chrome(service=service, options=options)\n",
        "        except Exception as e:\n",
        "            self.log.warning(f\"ChromeDriverManager failed: {e}, trying direct Chrome\")\n",
        "            driver = webdriver.Chrome(options=options)\n",
        "\n",
        "        driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
        "        driver.execute_cdp_cmd('Network.setUserAgentOverride', {\n",
        "            \"userAgent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\"\n",
        "        })\n",
        "        return driver\n",
        "\n",
        "    def _case_id_from_url(self, url: str) -> Optional[str]:\n",
        "        # Extract case ID from new URL format: /akn/ke/judgment/kehc/2025/15858/eng@2025-11-07\n",
        "        patterns = [\n",
        "            r\"/akn/ke/judgment/[^/]+/(\\d+)/(?:eng@|\\d+)\",\n",
        "            r\"/judgments/view/(\\d+)\",\n",
        "            r\"/caselaw/cases/view/(\\d+)\"\n",
        "        ]\n",
        "        for pattern in patterns:\n",
        "            m = re.search(pattern, url)\n",
        "            if m:\n",
        "                return m.group(1)\n",
        "        return None\n",
        "\n",
        "    def fetch_case_urls_selenium(self) -> List[str]:\n",
        "        self.log.info(\"Collecting case URLs from new Kenya Law site...\")\n",
        "        urls = set()\n",
        "\n",
        "        try:\n",
        "            self.driver = self._setup_driver()\n",
        "            self.driver.get(self.cfg.JUDGMENTS_URL)\n",
        "            time.sleep(5)\n",
        "\n",
        "            # Wait for page to load\n",
        "            WebDriverWait(self.driver, 10).until(\n",
        "                EC.presence_of_element_located((By.TAG_NAME, \"table\"))\n",
        "            )\n",
        "\n",
        "            # Get initial page URLs\n",
        "            soup = BeautifulSoup(self.driver.page_source, \"lxml\")\n",
        "            self._extract_urls_from_page(soup, urls)\n",
        "\n",
        "            # Handle pagination\n",
        "            page_count = 0\n",
        "            while page_count < (self.cfg.MAX_PAGES or 10):  # Limit pages if specified\n",
        "                try:\n",
        "                    # Look for next button\n",
        "                    next_buttons = self.driver.find_elements(By.XPATH,\n",
        "                        \"//a[contains(text(), 'Next') or contains(@class, 'next') or contains(@aria-label, 'next')]\")\n",
        "\n",
        "                    if not next_buttons:\n",
        "                        break\n",
        "\n",
        "                    next_btn = next_buttons[0]\n",
        "                    if \"disabled\" in next_btn.get_attribute(\"class\") or not next_btn.is_enabled():\n",
        "                        break\n",
        "\n",
        "                    # Click next page\n",
        "                    self.driver.execute_script(\"arguments[0].click();\", next_btn)\n",
        "                    time.sleep(4)\n",
        "\n",
        "                    # Wait for new content to load\n",
        "                    WebDriverWait(self.driver, 10).until(\n",
        "                        EC.presence_of_element_located((By.TAG_NAME, \"table\"))\n",
        "                    )\n",
        "\n",
        "                    # Extract URLs from new page\n",
        "                    soup = BeautifulSoup(self.driver.page_source, \"lxml\")\n",
        "                    self._extract_urls_from_page(soup, urls)\n",
        "\n",
        "                    page_count += 1\n",
        "                    self.log.info(f\"Processed page {page_count}, total URLs: {len(urls)}\")\n",
        "\n",
        "                except (TimeoutException, NoSuchElementException) as e:\n",
        "                    self.log.info(\"No more pages or pagination failed\")\n",
        "                    break\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Error collecting URLs: {e}\")\n",
        "        finally:\n",
        "            if self.driver:\n",
        "                self.driver.quit()\n",
        "\n",
        "        final = list(urls)\n",
        "        if self.cfg.MAX_CASES:\n",
        "            final = final[:self.cfg.MAX_CASES]\n",
        "        self.log.info(f\"Collected {len(final)} case URLs\")\n",
        "        return final\n",
        "\n",
        "    def _extract_urls_from_page(self, soup: BeautifulSoup, urls: set) -> None:\n",
        "        \"\"\"Extract case URLs from a page\"\"\"\n",
        "        # Look for case links in tables\n",
        "        table_links = soup.find_all(\"a\", href=re.compile(r\"/akn/ke/judgment/\"))\n",
        "        for link in table_links:\n",
        "            href = link.get(\"href\")\n",
        "            if href:\n",
        "                full_url = urljoin(self.cfg.NEW_BASE_URL, href)\n",
        "                urls.add(full_url)\n",
        "\n",
        "        # Also check for any judgment links\n",
        "        judgment_links = soup.select('a[href*=\"/judgment/\"]')\n",
        "        for link in judgment_links:\n",
        "            href = link.get(\"href\")\n",
        "            if href and \"/akn/ke/judgment/\" in href:\n",
        "                full_url = urljoin(self.cfg.NEW_BASE_URL, href)\n",
        "                urls.add(full_url)\n",
        "\n",
        "    def scrape_one_case(self, url: str) -> Optional[Dict]:\n",
        "        case_id = self._case_id_from_url(url)\n",
        "        if not case_id or case_id in self.seen_case_ids:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            self.log.info(f\"Scraping case: {url}\")\n",
        "            resp = self.session.get(url, timeout=self.cfg.REQUEST_TIMEOUT)\n",
        "            resp.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "\n",
        "            # Extract case title\n",
        "            title_elem = soup.find(\"h1\") or soup.find(\"title\")\n",
        "            case_name = title_elem.get_text(strip=True) if title_elem else \"Unknown Case\"\n",
        "\n",
        "            # Extract case content - try multiple selectors for new site\n",
        "            content_selectors = [\n",
        "                \"div.fr-view\",  # Rich text content\n",
        "                \"div.content\",\n",
        "                \"article\",\n",
        "                \"main\",\n",
        "                \".judgment-content\",\n",
        "                \".case-content\"\n",
        "            ]\n",
        "\n",
        "            content = None\n",
        "            for selector in content_selectors:\n",
        "                content = soup.select_one(selector)\n",
        "                if content:\n",
        "                    break\n",
        "\n",
        "            if not content:\n",
        "                # Fallback: get main content area\n",
        "                content = soup.find(\"main\") or soup.find(\"article\") or soup.find(\"div\", class_=re.compile(\"content\"))\n",
        "\n",
        "            if not content:\n",
        "                self.log.warning(f\"No content found for case {case_id}\")\n",
        "                return None\n",
        "\n",
        "            # Clean up content\n",
        "            for element in content.select(\"script, style, nav, header, footer, .nav, .header, .footer, .tools, .act-tools\"):\n",
        "                element.decompose()\n",
        "\n",
        "            # Extract text\n",
        "            text = content.get_text(separator=\"\\n\", strip=True)\n",
        "            text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "            if len(text.split()) < 100:\n",
        "                self.log.warning(f\"Case {case_id} has insufficient text: {len(text.split())} words\")\n",
        "                return None\n",
        "\n",
        "            # Extract metadata\n",
        "            metadata = self._extract_case_metadata(soup)\n",
        "\n",
        "            data = {\n",
        "                \"case_id\": case_id,\n",
        "                \"case_name\": case_name,\n",
        "                \"url\": url,\n",
        "                \"text\": text,\n",
        "                \"text_length_words\": len(text.split()),\n",
        "                \"scraped_at\": datetime.now().isoformat(),\n",
        "                \"metadata\": metadata\n",
        "            }\n",
        "\n",
        "            self.seen_case_ids.add(case_id)\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Case failed {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _extract_case_metadata(self, soup: BeautifulSoup) -> Dict[str, str]:\n",
        "        \"\"\"Extract case metadata from the page\"\"\"\n",
        "        metadata = {}\n",
        "\n",
        "        try:\n",
        "            # Look for common metadata patterns\n",
        "            meta_selectors = {\n",
        "                \"court\": [\"span.court\", \"div.court\", \"td.cell-court\"],\n",
        "                \"date\": [\"span.date\", \"div.date\", \"td.cell-date\", \"time\"],\n",
        "                \"case_number\": [\"span.case-number\", \"div.case-number\", \"td.cell-case-number\"],\n",
        "                \"judges\": [\"span.judges\", \"div.judges\", \"p.judges\"],\n",
        "                \"citation\": [\"span.citation\", \"div.citation\", \"td.cell-citation\"]\n",
        "            }\n",
        "\n",
        "            for key, selectors in meta_selectors.items():\n",
        "                for selector in selectors:\n",
        "                    element = soup.select_one(selector)\n",
        "                    if element:\n",
        "                        metadata[key] = element.get_text(strip=True)\n",
        "                        break\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log.debug(f\"Metadata extraction failed: {e}\")\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    def run_case_scrape(self, urls: List[str], handler: DataHandler) -> int:\n",
        "        self.log.info(f\"Scraping {len(urls)} cases...\")\n",
        "        saved = 0\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=self.cfg.MAX_SCRAPE_WORKERS) as executor:\n",
        "            future_to_url = {executor.submit(self.scrape_one_case, url): url for url in urls}\n",
        "\n",
        "            for future in as_completed(future_to_url):\n",
        "                url = future_to_url[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    if result and handler.save_case(result):\n",
        "                        saved += 1\n",
        "                except Exception as e:\n",
        "                    self.log.error(f\"Case scraping failed for {url}: {e}\")\n",
        "\n",
        "        return saved\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                                   MAIN                                      #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "def main() -> None:\n",
        "    cfg = Config()\n",
        "    log = setup_logging(cfg.LOG_FILE)\n",
        "    log.info(\"=== KenyaLaw Scraper v6.0 FULL (Acts + Subsidiary + Cases + Counties with PDF support) ===\")\n",
        "\n",
        "    # Check PDF support\n",
        "    if cfg.ENABLE_PDF_EXTRACTION:\n",
        "        if not PDF_SUPPORT and not PDFPLUMBER_SUPPORT:\n",
        "            log.warning(\"PDF extraction enabled but no PDF libraries found. Install: pip install pypdf2 pdfplumber\")\n",
        "        else:\n",
        "            log.info(f\"PDF extraction enabled: PyPDF2={PDF_SUPPORT}, pdfplumber={PDFPLUMBER_SUPPORT}\")\n",
        "\n",
        "    # Scrape static content\n",
        "    scrape_constitution(cfg, log)\n",
        "    scrape_acts_of_kenya(cfg, log)\n",
        "    scrape_county_legislation(cfg, log)  # Now with PDF support\n",
        "\n",
        "    # Scrape case law\n",
        "    handler = DataHandler(cfg, log)\n",
        "    scraper = KenyaLawScraper(cfg, log)\n",
        "    scraper.seen_case_ids = handler.load_existing_case_ids()\n",
        "\n",
        "    urls = scraper.fetch_case_urls_selenium()\n",
        "    new_urls = [u for u in urls if scraper._case_id_from_url(u) not in scraper.seen_case_ids]\n",
        "\n",
        "    if new_urls:\n",
        "        log.info(f\"Found {len(new_urls)} new cases to scrape\")\n",
        "        saved = scraper.run_case_scrape(new_urls, handler)\n",
        "        log.info(f\"Completed: {saved} new cases saved.\")\n",
        "    else:\n",
        "        log.info(\"No new cases found.\")\n",
        "\n",
        "    log.info(\"=== ALL DONE ===\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QYKlRztueKlI"
      },
      "id": "QYKlRztueKlI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "240d3895"
      },
      "source": [
        "## Create a chat interface\n",
        "\n",
        "### Subtask:\n",
        "Implement a simple loop or function that allows the user to input queries and receive responses from the model."
      ],
      "id": "240d3895"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3870f3b6"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement a loop to interact with the user, get their input, call the generate_response function, print the response, and provide an exit mechanism."
      ],
      "id": "3870f3b6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e690c873"
      },
      "source": [
        "print(\"Kenya Legal Chatbot (Type 'quit' or 'exit' to end)\")\n",
        "\n",
        "while True:\n",
        "    query = input(\"\\nYour query: \")\n",
        "    if query.lower() in ['quit', 'exit']:\n",
        "        print(\"Exiting chatbot. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    if not query.strip():\n",
        "        print(\"Please enter a query.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Ensure model and tokenizer are accessible (assuming they are loaded globally)\n",
        "        response = generate_response(query, model)\n",
        "        print(\"\\nChatbot:\", response)\n",
        "    except NameError:\n",
        "        print(\"Error: Model and tokenizer are not loaded. Please run the model loading step.\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during response generation: {e}\")\n",
        "        break"
      ],
      "id": "e690c873",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9bc9810"
      },
      "source": [
        "**Reasoning**:\n",
        "The `cfg` object is now defined. Proceed with listing the contents of the output directory."
      ],
      "id": "b9bc9810"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08eb3f3c",
        "outputId": "142db8c1-dc12-49bf-fc74-17aa62e180c5"
      },
      "source": [
        "# List the contents of the output directory\n",
        "!ls -l $cfg.OUTPUT_DIR"
      ],
      "id": "08eb3f3c",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Nov 10 16:37 gguf\n",
            "drwxr-xr-x 2 root root 4096 Nov 10 16:37 hf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb9eb1be"
      },
      "source": [
        "## Create a chat interface\n",
        "\n",
        "### Subtask:\n",
        "Implement a simple loop or function that allows the user to input queries and receive responses from the model."
      ],
      "id": "cb9eb1be"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57362eff"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement a loop to interact with the user, get their input, call the generate_response function, print the response, and provide an exit mechanism."
      ],
      "id": "57362eff"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33fbdd01",
        "outputId": "e21579d7-9e8a-40bd-a9f3-5d24713cd385"
      },
      "source": [
        "print(\"Kenya Legal Chatbot (Type 'quit' or 'exit' to end)\")\n",
        "\n",
        "while True:\n",
        "    query = input(\"\\nYour query: \")\n",
        "    if query.lower() in ['quit', 'exit']:\n",
        "        print(\"Exiting chatbot. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    if not query.strip():\n",
        "        print(\"Please enter a query.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Ensure model and tokenizer are accessible (assuming they are loaded globally)\n",
        "        response = generate_response(query, model)\n",
        "        print(\"\\nChatbot:\", response)\n",
        "    except NameError:\n",
        "        print(\"Error: Model and tokenizer are not loaded. Please run the model loading step.\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during response generation: {e}\")\n",
        "        break"
      ],
      "id": "33fbdd01",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kenya Legal Chatbot (Type 'quit' or 'exit' to end)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2e69c87"
      },
      "source": [
        "## Define a generation function\n",
        "\n",
        "### Subtask:\n",
        "Create a Python function that takes a user query, formats it into the appropriate prompt for the model, and generates a response."
      ],
      "id": "a2e69c87"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d91d7f5"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `generate_response` function as instructed, implementing the logic for formatting the prompt, tokenizing the input, generating the response using the loaded model, decoding the output, and extracting the relevant response text."
      ],
      "id": "7d91d7f5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7af15a5"
      },
      "source": [
        "import torch\n",
        "\n",
        "def generate_response(query: str, model) -> str:\n",
        "    \"\"\"\n",
        "    Generates a legal response to a user query using the fine-tuned model.\n",
        "\n",
        "    Args:\n",
        "        query: The user's input query string.\n",
        "        model: The loaded language model object.\n",
        "        tokenizer: The loaded tokenizer object.\n",
        "\n",
        "    Returns:\n",
        "        The extracted response string from the model.\n",
        "    \"\"\"\n",
        "    # Construct the prompt in Alpaca format\n",
        "    prompt = f\"\"\"### Instruction:\n",
        "You are a helpful and knowledgeable Kenyan legal expert. Provide accurate information and guidance based on Kenyan law.\n",
        "\n",
        "### Input:\n",
        "{query}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate the response\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=2048,  # Adjust as needed\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        use_cache=True,\n",
        "        pad_token_id=tokenizer.eos_token_id # Set pad_token_id\n",
        "    )\n",
        "\n",
        "    # Decode the generated tokens\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the response part\n",
        "    # The response should start after the \"### Response:\" tag\n",
        "    response_tag = \"### Response:\\n\"\n",
        "    response_start_index = generated_text.find(response_tag)\n",
        "\n",
        "    if response_start_index != -1:\n",
        "        response = generated_text[response_start_index + len(response_tag):].strip()\n",
        "    else:\n",
        "        # Fallback if the tag is not found (shouldn't happen with correct prompt)\n",
        "        response = generated_text.strip()\n",
        "\n",
        "    return response"
      ],
      "id": "f7af15a5",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08b06d4c"
      },
      "source": [
        "**Reasoning**:\n",
        "The `cfg` object was not defined in the current kernel session. Re-run the code cell that defines the `Config` class and creates the `cfg` object."
      ],
      "id": "08b06d4c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59fde1a1"
      },
      "source": [
        "from dataclasses import dataclass, field\n",
        "import os\n",
        "from typing import List, Set, Dict, Optional\n",
        "from datetime import datetime\n",
        "import json\n",
        "import re\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    BASE_PROJECT_DIR: str = os.path.join(os.path.expanduser(\"~\"), \"projects\", \"kenya_law\")\n",
        "\n",
        "    LOG_DIR: str = field(init=False)\n",
        "    DATA_DIR: str = field(init=False)\n",
        "    LOG_FILE: str = field(init=False)\n",
        "    DATA_FILE: str = field(init=False)\n",
        "    DEBUG_DIR: str = field(init=False)\n",
        "    CONSTITUTION_FILE: str = field(init=False)\n",
        "    ACTS_FILE: str = field(init=False)\n",
        "    SUBSIDIARY_FILE: str = field(init=False)\n",
        "    COUNTIES_FILE: str = field(init=False)\n",
        "    PDF_DIR: str = field(init=False)\n",
        "\n",
        "    MAX_CASES: int = None\n",
        "    MAX_PAGES: int = None\n",
        "    MAX_COUNTY_LAWS: int = 50\n",
        "    REQUEST_TIMEOUT: int = 30\n",
        "    SELENIUM_TIMEOUT: int = 45\n",
        "    YEAR_START: int = 2020\n",
        "\n",
        "    BASE_URL: str = \"https://kenyalaw.org\"\n",
        "    NEW_BASE_URL: str = \"https://new.kenyalaw.org\"\n",
        "    SEARCH_URL: str = \"https://new.kenyalaw.org/search/\"\n",
        "    JUDGMENTS_URL: str = \"https://new.kenyalaw.org/judgments/\"\n",
        "    COUNTIES_URL: str = \"https://new.kenyalaw.org/legislation/counties\"\n",
        "    ACTS_TOC_URL: str = \"https://new.kenyalaw.org/legislation/\"\n",
        "\n",
        "    LOCAL_CHROMEDRIVER_PATH: Optional[str] = None\n",
        "    CHROME_HEADLESS: bool = True\n",
        "\n",
        "    KEYWORDS: List[str] = field(default_factory=lambda: [\n",
        "        \"constitution\", \"human rights\", \"land\", \"election\", \"criminal\", \"civil\",\n",
        "        \"jurisdiction\", \"appeal\", \"judicial review\", \"injunction\", \"contract\"\n",
        "    ])\n",
        "\n",
        "    MAX_SCRAPE_WORKERS: int = 10\n",
        "    ENABLE_PDF_EXTRACTION: bool = True\n",
        "\n",
        "    # Training Config specific additions\n",
        "    OUTPUT_DIR: str = field(init=False)\n",
        "    GGUF_OUTPUT: str = field(init=False)\n",
        "    HF_OUTPUT: str = field(init=False)\n",
        "    MODEL_NAME: str = \"unsloth/Mistral-7B-Instruct-v0.3-bnb-4bit\"\n",
        "    MAX_SEQ_LENGTH: int = 8192\n",
        "\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        self.LOG_DIR = os.path.join(os.path.expanduser(\"~\"), \"projects\", \"kenya_law\")\n",
        "        self.DATA_DIR = os.path.join(self.BASE_PROJECT_DIR, \"data\")\n",
        "        self.DEBUG_DIR = os.path.join(self.BASE_PROJECT_DIR, \"debug\")\n",
        "        self.PDF_DIR = os.path.join(self.DATA_DIR, \"pdfs\")\n",
        "        self.LOG_FILE = os.path.join(self.LOG_DIR, f\"kenyalaw_scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
        "        self.DATA_FILE = os.path.join(self.DATA_DIR, \"kenya_law_training_data.jsonl\")\n",
        "        self.CONSTITUTION_FILE = os.path.join(self.DATA_DIR, \"constitution.json\")\n",
        "        self.ACTS_FILE = os.path.join(self.DATA_DIR, \"acts_of_kenya.json\")\n",
        "        self.SUBSIDIARY_FILE = os.path.join(self.DATA_DIR, \"subsidiary_legislation.json\")\n",
        "        self.COUNTIES_FILE = os.path.join(self.DATA_DIR, \"county_legislation.json\")\n",
        "\n",
        "        for d in [self.LOG_DIR, self.DATA_DIR, self.DEBUG_DIR, self.PDF_DIR]:\n",
        "            os.makedirs(d, exist_ok=True)\n",
        "\n",
        "        # Training Config specific paths\n",
        "        self.OUTPUT_DIR = os.path.join(self.BASE_PROJECT_DIR, \"kenya-legal-llm\")\n",
        "        self.GGUF_OUTPUT = os.path.join(self.OUTPUT_DIR, \"gguf\")\n",
        "        self.HF_OUTPUT = os.path.join(self.OUTPUT_DIR, \"hf\")\n",
        "\n",
        "        # Ensure output directories exist\n",
        "        for d in [self.OUTPUT_DIR, self.GGUF_OUTPUT, self.HF_OUTPUT]:\n",
        "             os.makedirs(d, exist_ok=True)\n",
        "\n",
        "\n",
        "# Re-instantiate cfg\n",
        "cfg = Config()"
      ],
      "id": "59fde1a1",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d78fcf8"
      },
      "source": [
        "**Reasoning**:\n",
        "The user wants to see the contents of the directory where the models are saved. This path is stored in `cfg.OUTPUT_DIR`. I need to ensure `cfg` is defined and then use a shell command to list the directory contents."
      ],
      "id": "4d78fcf8"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "6d1b0f62",
        "outputId": "cedb3d5f-c8ea-401a-8028-ba52b2c584cd"
      },
      "source": [
        "# Ensure cfg is defined\n",
        "try:\n",
        "    cfg\n",
        "except NameError:\n",
        "    # If cfg is not defined, re-run the cell that defines it\n",
        "    get_ipython().run_cell('LNunD8hEVxu5')\n",
        "\n",
        "# List the contents of the output directory\n",
        "!ls -l $cfg.OUTPUT_DIR"
      ],
      "id": "6d1b0f62",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'LNunD8hEVxu5' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3960025468.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcfg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'cfg' is not defined",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2301104429.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLNunD8hEVxu5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'LNunD8hEVxu5' is not defined"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ls: cannot access '.OUTPUT_DIR': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "19155b84",
        "outputId": "ce08811b-c3bd-45f0-ea73-4eb9553e4b1d"
      },
      "source": [
        "%run -i e690c873"
      ],
      "id": "19155b84",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Exception",
          "evalue": "File `'e690c873.py'` not found.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    713\u001b[0m             \u001b[0mfpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_lst\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 714\u001b[0;31m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_finder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    715\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/utils/path.py\u001b[0m in \u001b[0;36mget_py_filename\u001b[0;34m(name, force_win32)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'File `%r` not found.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: File `'e690c873.py'` not found.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2022088139.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'run'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-i e690c873'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[1;32m   2416\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2417\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2418\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2419\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-52>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[1;32m    723\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'nt'\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"^'.*'$\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m                 \u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'For Windows, use double quotes to wrap a filename: %run \"mypath\\\\myfile.py\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 725\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    726\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mException\u001b[0m: File `'e690c873.py'` not found."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8006efed"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt failed because the `model` and `tokenizer` objects were not accessible in the current scope. Looking back at the notebook state, the model loading code is in cell `f53ec056`. Execute this cell to load the model and tokenizer into the current runtime."
      ],
      "id": "8006efed"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fbf6040"
      },
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Define the path to the saved Hugging Face model directory\n",
        "model_path = cfg.HF_OUTPUT\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=str(model_path),  # Use the path to the saved model\n",
        "    max_seq_length=cfg.MAX_SEQ_LENGTH,\n",
        "    dtype=None,  # Auto detect\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "id": "2fbf6040",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9c6f896"
      },
      "source": [
        "**Reasoning**:\n",
        "The `cfg` object was not defined in the current kernel session. Re-run the code cell that defines the `Config` class and creates the `cfg` object."
      ],
      "id": "b9c6f896"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1850480f"
      },
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime\n",
        "from typing import List, Set, Dict, Optional\n",
        "from urllib.parse import urljoin\n",
        "import io\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "# Try to import webdriver_manager, but make it optional\n",
        "try:\n",
        "    from webdriver_manager.chrome import ChromeDriverManager\n",
        "    WEBDRIVER_MANAGER_SUPPORT = True\n",
        "except ImportError:\n",
        "    WEBDRIVER_MANAGER_SUPPORT = False\n",
        "    print(\"Warning: webdriver_manager not installed. Chrome driver will not be automatically managed.\")\n",
        "\n",
        "\n",
        "# Try to import PDF libraries, but make them optional\n",
        "try:\n",
        "    import PyPDF2\n",
        "    PDF_SUPPORT = True\n",
        "except ImportError:\n",
        "    PDF_SUPPORT = False\n",
        "    print(\"Warning: PyPDF2 not installed. PDF content extraction will be limited.\")\n",
        "\n",
        "try:\n",
        "    import pdfplumber\n",
        "    PDFPLUMBER_SUPPORT = True\n",
        "except ImportError:\n",
        "    PDFPLUMBER_SUPPORT = False\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                               CONFIGURATION                                 #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    BASE_PROJECT_DIR: str = os.path.join(os.path.expanduser(\"~\"), \"projects\", \"kenya_law\")\n",
        "\n",
        "    LOG_DIR: str = field(init=False)\n",
        "    DATA_DIR: str = field(init=False)\n",
        "    LOG_FILE: str = field(init=False)\n",
        "    DATA_FILE: str = field(init=False)\n",
        "    DEBUG_DIR: str = field(init=False)\n",
        "    CONSTITUTION_FILE: str = field(init=False)\n",
        "    ACTS_FILE: str = field(init=False)\n",
        "    SUBSIDIARY_FILE: str = field(init=False)\n",
        "    COUNTIES_FILE: str = field(init=False)\n",
        "    PDF_DIR: str = field(init=False)  # NEW: Directory to store downloaded PDFs\n",
        "\n",
        "    MAX_CASES: int = None\n",
        "    MAX_PAGES: int = None\n",
        "    MAX_COUNTY_LAWS: int = 50\n",
        "    REQUEST_TIMEOUT: int = 30\n",
        "    SELENIUM_TIMEOUT: int = 45\n",
        "    YEAR_START: int = 2020\n",
        "\n",
        "    BASE_URL: str = \"https://kenyalaw.org\"\n",
        "    NEW_BASE_URL: str = \"https://new.kenyalaw.org\"\n",
        "    SEARCH_URL: str = \"https://new.kenyalaw.org/search/\"\n",
        "    JUDGMENTS_URL: str = \"https://new.kenyalaw.org/judgments/\"\n",
        "    COUNTIES_URL: str = \"https://new.kenyalaw.org/legislation/counties\"\n",
        "    ACTS_TOC_URL: str = \"https://new.kenyalaw.org/legislation/\"\n",
        "\n",
        "    LOCAL_CHROMEDRIVER_PATH: Optional[str] = None\n",
        "    CHROME_HEADLESS: bool = True\n",
        "\n",
        "    KEYWORDS: List[str] = field(default_factory=lambda: [\n",
        "        \"constitution\", \"human rights\", \"land\", \"election\", \"criminal\", \"civil\",\n",
        "        \"jurisdiction\", \"appeal\", \"judicial review\", \"injunction\", \"contract\"\n",
        "    ])\n",
        "\n",
        "    MAX_SCRAPE_WORKERS: int = 10\n",
        "    ENABLE_PDF_EXTRACTION: bool = True  # NEW: Control PDF processing\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        self.LOG_DIR = os.path.join(os.path.expanduser(\"~\"), \"projects\", \"kenya_law\")\n",
        "        self.DATA_DIR = os.path.join(self.BASE_PROJECT_DIR, \"data\")\n",
        "        self.DEBUG_DIR = os.path.join(self.BASE_PROJECT_DIR, \"debug\")\n",
        "        self.PDF_DIR = os.path.join(self.DATA_DIR, \"pdfs\")  # NEW\n",
        "        self.LOG_FILE = os.path.join(self.LOG_DIR, f\"kenyalaw_scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
        "        self.DATA_FILE = os.path.join(self.DATA_DIR, \"kenya_law_training_data.jsonl\")\n",
        "        self.CONSTITUTION_FILE = os.path.join(self.DATA_DIR, \"constitution.json\")\n",
        "        self.ACTS_FILE = os.path.join(self.DATA_DIR, \"acts_of_kenya.json\")\n",
        "        self.SUBSIDIARY_FILE = os.path.join(self.DATA_DIR, \"subsidiary_legislation.json\")\n",
        "        self.COUNTIES_FILE = os.path.join(self.DATA_DIR, \"county_legislation.json\")\n",
        "\n",
        "        for d in [self.LOG_DIR, self.DATA_DIR, self.DEBUG_DIR, self.PDF_DIR]:\n",
        "            os.makedirs(d, exist_ok=True)\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                                 LOGGING                                    #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "def setup_logging(log_file: str) -> logging.Logger:\n",
        "    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
        "    logger = logging.getLogger(\"KenyaLaw-Scraper-v6.0-FULL\")\n",
        "    logger.setLevel(logging.INFO)\n",
        "    logger.handlers.clear()\n",
        "    fh = logging.FileHandler(log_file, encoding=\"utf-8\")\n",
        "    ch = logging.StreamHandler()\n",
        "    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
        "    fh.setFormatter(formatter)\n",
        "    ch.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "    logger.addHandler(ch)\n",
        "    return logger\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                               DATA HANDLER                                 #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "class DataHandler:\n",
        "    def __init__(self, cfg: Config, log: logging.Logger):\n",
        "        self.cfg = cfg\n",
        "        self.log = log\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def load_existing_case_ids(self) -> Set[str]:\n",
        "        if not os.path.exists(self.cfg.DATA_FILE):\n",
        "            return set()\n",
        "        ids = set()\n",
        "        try:\n",
        "            with open(self.cfg.DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "                for line_num, line in enumerate(f, 1):\n",
        "                    line = line.strip()\n",
        "                    if not line: continue\n",
        "                    try:\n",
        "                        data = json.loads(line)\n",
        "                        if \"case_id\" in data:\n",
        "                            ids.add(data[\"case_id\"])\n",
        "                    except json.JSONDecodeError:\n",
        "                        self.log.warning(f\"Bad JSON at line {line_num}\")\n",
        "            self.log.info(f\"Loaded {len(ids)} existing case IDs\")\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Failed to load IDs: {e}\")\n",
        "        return ids\n",
        "\n",
        "    def save_case(self, case: Dict) -> bool:\n",
        "        try:\n",
        "            json_line = json.dumps(case, ensure_ascii=False)\n",
        "            with self.lock:\n",
        "                with open(self.cfg.DATA_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "                    f.write(json_line + \"\\n\")\n",
        "            self.log.info(f\"Saved case {case['case_id']} ({len(case['text'].split())} words)\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Save failed: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                              PDF HANDLING                                  #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "class PDFHandler:\n",
        "    def __init__(self, cfg: Config, log: logging.Logger):\n",
        "        self.cfg = cfg\n",
        "        self.log = log\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_url: str, pdf_content: bytes) -> Optional[str]:\n",
        "        \"\"\"Extract text from PDF content using multiple methods\"\"\"\n",
        "        if not self.cfg.ENABLE_PDF_EXTRACTION:\n",
        "            return \"PDF_CONTENT_AVAILABLE_BUT_EXTRACTION_DISABLED\"\n",
        "\n",
        "        # Save PDF file for reference\n",
        "        pdf_filename = self._save_pdf_file(pdf_url, pdf_content)\n",
        "\n",
        "        extracted_text = None\n",
        "\n",
        "        # Try pdfplumber first (better for scanned PDFs with OCR)\n",
        "        if PDFPLUMBER_SUPPORT:\n",
        "            extracted_text = self._extract_with_pdfplumber(pdf_content)\n",
        "\n",
        "        # Fallback to PyPDF2\n",
        "        if not extracted_text and PDF_SUPPORT:\n",
        "            extracted_text = self._extract_with_pypdf2(pdf_content)\n",
        "\n",
        "        # Final fallback\n",
        "        if not extracted_text:\n",
        "            extracted_text = self._extract_fallback(pdf_content, pdf_filename)\n",
        "\n",
        "        return extracted_text\n",
        "\n",
        "    def _save_pdf_file(self, pdf_url: str, pdf_content: bytes) -> str:\n",
        "        \"\"\"Save PDF file to disk for reference\"\"\"\n",
        "        try:\n",
        "            # Create filename from URL\n",
        "            filename = re.sub(r'[^a-zA-Z0-9]', '_', pdf_url) + '.pdf'\n",
        "            filepath = os.path.join(self.cfg.PDF_DIR, filename)\n",
        "\n",
        "            with open(filepath, 'wb') as f:\n",
        "                f.write(pdf_content)\n",
        "\n",
        "            return filename\n",
        "        except Exception as e:\n",
        "            self.log.warning(f\"Failed to save PDF file: {e}\")\n",
        "            return \"unknown.pdf\"\n",
        "\n",
        "    def _extract_with_pdfplumber(self, pdf_content: bytes) -> Optional[str]:\n",
        "        \"\"\"Extract text using pdfplumber (better for scanned PDFs)\"\"\"\n",
        "        try:\n",
        "            text_parts = []\n",
        "            with pdfplumber.open(io.BytesIO(pdf_content)) as pdf:\n",
        "                for page in pdf.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        text_parts.append(page_text.strip())\n",
        "\n",
        "            if text_parts:\n",
        "                full_text = '\\n'.join(text_parts)\n",
        "                if len(full_text.split()) > 10:  # Ensure we have substantial text\n",
        "                    return full_text\n",
        "        except Exception as e:\n",
        "            self.log.debug(f\"pdfplumber extraction failed: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _extract_with_pypdf2(self, pdf_content: bytes) -> Optional[str]:\n",
        "        \"\"\"Extract text using PyPDF2\"\"\"\n",
        "        try:\n",
        "            text_parts = []\n",
        "            pdf_file = io.BytesIO(pdf_content)\n",
        "            reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "            for page in reader.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text_parts.append(page_text.strip())\n",
        "\n",
        "            if text_parts:\n",
        "                full_text = '\\n'.join(text_parts)\n",
        "                if len(full_text.split()) > 10:\n",
        "                    return full_text\n",
        "        except Exception as e:\n",
        "            self.log.debug(f\"PyPDF2 extraction failed: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _extract_fallback(self, pdf_content: bytes, pdf_filename: str) -> str:\n",
        "        \"\"\"Fallback method when PDF text extraction fails\"\"\"\n",
        "        self.log.warning(f\"PDF text extraction failed for {pdf_filename}. Content saved to disk.\")\n",
        "        return f\"PDF_CONTENT_UNABLE_TO_EXTRACT_TEXT_SAVED_AS_{pdf_filename}\"\n",
        "\n",
        "    def is_pdf_url(self, url: str) -> bool:\n",
        "        \"\"\"Check if URL points to a PDF document\"\"\"\n",
        "        return url.lower().endswith('.pdf') or '/pdf/' in url.lower()\n",
        "\n",
        "    def download_pdf(self, session: requests.Session, pdf_url: str) -> Optional[bytes]:\n",
        "        \"\"\"Download PDF content\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\",\n",
        "                \"Accept\": \"application/pdf, */*\",\n",
        "                \"Referer\": \"https://new.kenyalaw.org/\"\n",
        "            }\n",
        "\n",
        "            response = session.get(pdf_url, timeout=30, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            content_type = response.headers.get('content-type', '').lower()\n",
        "            if 'pdf' in content_type or response.content[:4] == b'%PDF':\n",
        "                return response.content\n",
        "            else:\n",
        "                self.log.warning(f\"URL {pdf_url} doesn't contain PDF data\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Failed to download PDF from {pdf_url}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                      STATIC CONTENT SCRAPERS (Constitution & Acts)          #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "def save_constitution_data(cfg: Config, log: logging.Logger, data: Dict[str, str]) -> None:\n",
        "    try:\n",
        "        with open(cfg.CONSTITUTION_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "        log.info(f\"Constitution saved → {cfg.CONSTITUTION_FILE}\")\n",
        "    except Exception as e:\n",
        "        log.error(f\"Failed to save constitution: {e}\")\n",
        "\n",
        "def scrape_constitution(cfg: Config, log: logging.Logger) -> None:\n",
        "    log.info(\"Scraping Constitution of Kenya (2010)...\")\n",
        "    if os.path.exists(cfg.CONSTITUTION_FILE):\n",
        "        log.info(f\"Constitution already exists → {cfg.CONSTITUTION_FILE}. Skipping.\")\n",
        "        return\n",
        "\n",
        "    # Try multiple potential sources\n",
        "    SOURCES = [\n",
        "        \"https://new.kenyalaw.org/akn/ke/act/2010/constitution/eng@2010-09-03\",\n",
        "        \"https://new.kenyalaw.org/akn/ke/act/2010/constitution\",\n",
        "        \"https://kenyalaw.org/kl/index.php?id=398\"\n",
        "    ]\n",
        "\n",
        "    session = requests.Session()\n",
        "    session.headers.update({\n",
        "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\",\n",
        "        \"Accept\": \"application/xml, text/xml, text/html, */*\",\n",
        "        \"Referer\": \"https://new.kenyalaw.org/\",\n",
        "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "    })\n",
        "\n",
        "    for source_url in SOURCES:\n",
        "        try:\n",
        "            log.info(f\"Trying source: {source_url}\")\n",
        "            resp = session.get(source_url, timeout=60)\n",
        "            resp.raise_for_status()\n",
        "\n",
        "            content_type = resp.headers.get('content-type', '').lower()\n",
        "\n",
        "            if 'xml' in content_type:\n",
        "                # Parse as XML\n",
        "                soup = BeautifulSoup(resp.content, \"xml\")\n",
        "                body = soup.find(\"body\")\n",
        "                if not body:\n",
        "                    log.warning(f\"No <body> found in XML from {source_url}\")\n",
        "                    continue\n",
        "\n",
        "                data = {}\n",
        "                current_title = \"Preamble\"\n",
        "                current_lines = []\n",
        "\n",
        "                # Extract all relevant elements\n",
        "                for elem in body.find_all(['heading', 'num', 'p', 'chapter', 'part', 'section', 'article']):\n",
        "                    if elem.name in ['heading', 'chapter', 'part']:\n",
        "                        # Save previous section\n",
        "                        if current_lines:\n",
        "                            data[current_title] = \"\\n\".join(current_lines).strip()\n",
        "                            current_lines = []\n",
        "                        # Start new section\n",
        "                        current_title = elem.get_text(strip=True)\n",
        "                        if not current_title:\n",
        "                            current_title = \"Untitled Section\"\n",
        "\n",
        "                    elif elem.name == 'p' and elem.get_text(strip=True):\n",
        "                        text = elem.get_text(strip=True)\n",
        "                        if len(text) > 10:  # Filter out very short paragraphs\n",
        "                            current_lines.append(text)\n",
        "\n",
        "                # Save the last section\n",
        "                if current_lines:\n",
        "                    data[current_title] = \"\\n\".join(current_lines).strip()\n",
        "\n",
        "            else:\n",
        "                # Parse as HTML\n",
        "                soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "\n",
        "                # Try different content selectors\n",
        "                content_selectors = [\n",
        "                    \"div.act-content\",\n",
        "                    \"div.content\",\n",
        "                    \"article\",\n",
        "                    \"main\",\n",
        "                    \"div.container\",\n",
        "                    \"#content\"\n",
        "                ]\n",
        "\n",
        "                content = None\n",
        "                for selector in content_selectors:\n",
        "                    content = soup.select_one(selector)\n",
        "                    if content:\n",
        "                        break\n",
        "\n",
        "                if not content:\n",
        "                    log.warning(f\"No content found with selectors in {source_url}\")\n",
        "                    continue\n",
        "\n",
        "                # Clean up the content\n",
        "                for element in content.select(\"script, style, nav, header, footer, .nav, .header, .footer\"):\n",
        "                    element.decompose()\n",
        "\n",
        "                # Extract text and structure\n",
        "                data = {}\n",
        "                current_section = \"Constitution of Kenya\"\n",
        "                sections = [current_section]\n",
        "                text_lines = []\n",
        "\n",
        "                for element in content.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'div']):\n",
        "                    text = element.get_text(strip=True)\n",
        "                    if not text:\n",
        "                        continue\n",
        "\n",
        "                    # Detect section headers\n",
        "                    if element.name in ['h1', 'h2', 'h3', 'h4']:\n",
        "                        if text_lines:  # Save previous section\n",
        "                            data[current_section] = \"\\n\".join(text_lines).strip()\n",
        "                            text_lines = []\n",
        "                        current_section = text\n",
        "                        sections.append(current_section)\n",
        "                    elif len(text) > 20:  # Substantial content\n",
        "                        text_lines.append(text)\n",
        "\n",
        "                # Save the last section\n",
        "                if text_lines:\n",
        "                    data[current_section] = \"\\n\".join(text_lines).strip()\n",
        "\n",
        "            # Filter out empty or very short sections\n",
        "            filtered_data = {}\n",
        "            for title, content in data.items():\n",
        "                if content and len(content.split()) >= 10:\n",
        "                    clean_content = re.sub(r'\\s+', ' ', content).strip()\n",
        "                    filtered_data[title] = clean_content\n",
        "\n",
        "            # If we have substantial content, save it\n",
        "            if filtered_data and sum(len(c.split()) for c in filtered_data.values()) > 500:\n",
        "                save_constitution_data(cfg, log, filtered_data)\n",
        "                total_words = sum(len(v.split()) for v in filtered_data.values())\n",
        "                log.info(f\"SUCCESS: Constitution scraped → {len(filtered_data)} sections, {total_words:,} words\")\n",
        "                return\n",
        "            else:\n",
        "                log.warning(f\"Insufficient content from {source_url}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            log.warning(f\"Source {source_url} failed: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Fallback: manual structure if all sources fail\n",
        "    log.warning(\"All automated sources failed. Creating placeholder structure.\")\n",
        "    fallback_data = {\n",
        "        \"Preamble\": \"We, the people of Kenya—ACKNOWLEDGING the supremacy of the Almighty God of all creation...\",\n",
        "        \"Chapter One - Sovereignty of the People\": \"1. (1) All sovereign power belongs to the people of Kenya...\",\n",
        "        \"Chapter Two - The Republic\": \"4. (1) Kenya is a sovereign Republic. (2) The Republic of Kenya shall be a multi-party democratic state...\",\n",
        "        \"Note\": \"This is a placeholder. The actual constitution text could not be scraped automatically. Consider manual entry.\"\n",
        "    }\n",
        "    save_constitution_data(cfg, log, fallback_data)\n",
        "    log.info(\"Created fallback constitution structure\")\n",
        "\n",
        "def save_acts_data(cfg: Config, log: logging.Logger, acts: Dict[str, str], subs: Dict[str, str]) -> None:\n",
        "    with open(cfg.ACTS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(acts, f, ensure_ascii=False, indent=2)\n",
        "    log.info(f\"Saved {len(acts)} Acts → {cfg.ACTS_FILE}\")\n",
        "\n",
        "    with open(cfg.SUBSIDIARY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(subs, f, ensure_ascii=False, indent=2)\n",
        "    log.info(f\"Saved {len(subs)} Subsidiary Laws → {cfg.SUBSIDIARY_FILE}\")\n",
        "\n",
        "def save_counties_data(cfg: Config, log: logging.Logger, counties_data: Dict[str, Dict]) -> None:\n",
        "    try:\n",
        "        with open(cfg.COUNTIES_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(counties_data, f, ensure_ascii=False, indent=2)\n",
        "        log.info(f\"County legislation saved → {cfg.COUNTIES_FILE}\")\n",
        "    except Exception as e:\n",
        "        log.error(f\"Failed to save county legislation: {e}\")\n",
        "\n",
        "\n",
        "def scrape_county_legislation(cfg: Config, log: logging.Logger) -> None:\n",
        "    \"\"\"Scrape county legislation from all 47 counties\"\"\"\n",
        "    log.info(\"Scraping County Legislation from all 47 counties...\")\n",
        "    if os.path.exists(cfg.COUNTIES_FILE):\n",
        "        log.info(f\"County legislation already exists → {cfg.COUNTIES_FILE}. Skipping.\")\n",
        "        return\n",
        "\n",
        "    session = requests.Session()\n",
        "    pdf_handler = PDFHandler(cfg, log)  # NEW: PDF handler\n",
        "\n",
        "    session.headers.update({\n",
        "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\",\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "        \"Referer\": cfg.NEW_BASE_URL\n",
        "    })\n",
        "\n",
        "    counties_data = {}\n",
        "    total_county_laws = 0\n",
        "\n",
        "    try:\n",
        "        # Get the main counties page\n",
        "        log.info(f\"Accessing counties page: {cfg.COUNTIES_URL}\")\n",
        "        resp = session.get(cfg.COUNTIES_URL, timeout=cfg.REQUEST_TIMEOUT)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "\n",
        "        # Extract all county links\n",
        "        county_links = []\n",
        "        flow_columns = soup.select('.flow-columns-group')\n",
        "\n",
        "        for column in flow_columns:\n",
        "            links = column.select('a[href^=\"/legislation/ke-\"]')\n",
        "            for link in links:\n",
        "                county_name = link.get_text(strip=True)\n",
        "                county_url = urljoin(cfg.NEW_BASE_URL, link.get('href'))\n",
        "                county_links.append((county_name, county_url))\n",
        "\n",
        "        log.info(f\"Found {len(county_links)} counties to process\")\n",
        "\n",
        "        # Process each county\n",
        "        for county_name, county_url in county_links:\n",
        "            try:\n",
        "                log.info(f\"Processing county: {county_name}\")\n",
        "                time.sleep(1)  # Be respectful\n",
        "\n",
        "                # Get county page\n",
        "                county_resp = session.get(county_url, timeout=cfg.REQUEST_TIMEOUT)\n",
        "                county_resp.raise_for_status()\n",
        "                county_soup = BeautifulSoup(county_resp.text, \"lxml\")\n",
        "\n",
        "                # Extract county laws\n",
        "                county_laws = {}\n",
        "\n",
        "                # Look for laws in tables or lists\n",
        "                law_elements = county_soup.select('tr.has-children, .legislation-item, .law-item')\n",
        "\n",
        "                if not law_elements:\n",
        "                    # Try alternative selectors\n",
        "                    law_elements = county_soup.select('a[href*=\"/akn/ke/act/\"]')\n",
        "\n",
        "                laws_processed = 0\n",
        "\n",
        "                for law_element in law_elements:\n",
        "                    if laws_processed >= cfg.MAX_COUNTY_LAWS:\n",
        "                        break\n",
        "\n",
        "                    try:\n",
        "                        # Extract law link and title\n",
        "                        if law_element.name == 'tr':\n",
        "                            link_elem = law_element.select_one('td.cell-title a')\n",
        "                        else:\n",
        "                            link_elem = law_element\n",
        "\n",
        "                        if not link_elem or not link_elem.get('href'):\n",
        "                            continue\n",
        "\n",
        "                        law_title = link_elem.get_text(strip=True)\n",
        "                        law_url = urljoin(cfg.NEW_BASE_URL, link_elem.get('href'))\n",
        "\n",
        "                        # Skip if it's not a direct law link\n",
        "                        if '/akn/ke/act/' not in law_url:\n",
        "                            continue\n",
        "\n",
        "                        # Scrape the actual law content (now with PDF support)\n",
        "                        time.sleep(0.5)\n",
        "                        law_content = scrape_county_law_content(session, pdf_handler, law_url, log)\n",
        "\n",
        "                        if law_content and len(law_content.split()) > 50:  # Reduced threshold for PDFs\n",
        "                            county_laws[law_title] = {\n",
        "                                'url': law_url,\n",
        "                                'content': law_content,\n",
        "                                'word_count': len(law_content.split()),\n",
        "                                'content_type': 'pdf' if pdf_handler.is_pdf_url(law_url) else 'html'\n",
        "                            }\n",
        "                            laws_processed += 1\n",
        "                            total_county_laws += 1\n",
        "                            log.info(f\"    ✓ County law: {law_title} ({len(law_content.split())} words) [{county_laws[law_title]['content_type'].upper()}]\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        log.warning(f\"Failed to process county law in {county_name}: {e}\")\n",
        "                        continue\n",
        "\n",
        "                # Add county data\n",
        "                if county_laws:\n",
        "                    counties_data[county_name] = {\n",
        "                        'county_url': county_url,\n",
        "                        'laws': county_laws,\n",
        "                        'total_laws': len(county_laws),\n",
        "                        'scraped_at': datetime.now().isoformat()\n",
        "                    }\n",
        "                    log.info(f\"  ✓ {county_name}: {len(county_laws)} laws\")\n",
        "\n",
        "            except Exception as e:\n",
        "                log.error(f\"Failed to process county {county_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Save counties data\n",
        "        save_counties_data(cfg, log, counties_data)\n",
        "        log.info(f\"SUCCESS: County legislation scraped → {len(counties_data)} counties, {total_county_laws} total laws\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log.error(f\"County legislation scrape failed: {e}\", exc_info=True)\n",
        "\n",
        "def scrape_county_law_content(session: requests.Session, pdf_handler: PDFHandler, law_url: str, log: logging.Logger) -> Optional[str]:\n",
        "    \"\"\"Scrape content of an individual county law with PDF support\"\"\"\n",
        "\n",
        "    # Check if it's a PDF URL\n",
        "    if pdf_handler.is_pdf_url(law_url):\n",
        "        log.info(f\"  Detected PDF document: {law_url}\")\n",
        "        pdf_content = pdf_handler.download_pdf(session, law_url)\n",
        "        if pdf_content:\n",
        "            return pdf_handler.extract_text_from_pdf(law_url, pdf_content)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    # Handle HTML content\n",
        "    try:\n",
        "        resp = session.get(law_url, timeout=30)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "\n",
        "        # Check if the page contains a PDF link\n",
        "        pdf_links = soup.select('a[href$=\".pdf\"], a[href*=\"/pdf/\"]')\n",
        "        for pdf_link in pdf_links:\n",
        "            pdf_url = urljoin(law_url, pdf_link.get('href'))\n",
        "            log.info(f\"  Found embedded PDF link: {pdf_url}\")\n",
        "            pdf_content = pdf_handler.download_pdf(session, pdf_url)\n",
        "            if pdf_content:\n",
        "                return pdf_handler.extract_text_from_pdf(pdf_url, pdf_content)\n",
        "\n",
        "        # Try multiple content selectors for HTML laws\n",
        "        content_selectors = [\n",
        "            \"div.act-content\",\n",
        "            \"div.fr-view\",\n",
        "            \"div.content\",\n",
        "            \"article\",\n",
        "            \"main\",\n",
        "            \".law-content\",\n",
        "            \".document-content\"\n",
        "        ]\n",
        "\n",
        "        content = None\n",
        "        for selector in content_selectors:\n",
        "            content = soup.select_one(selector)\n",
        "            if content:\n",
        "                break\n",
        "\n",
        "        if not content:\n",
        "            # Fallback: get body content\n",
        "            content = soup.find('main') or soup.find('article') or soup.find('body')\n",
        "\n",
        "        if content:\n",
        "            # Clean up content\n",
        "            for element in content.select(\"script, style, nav, header, footer, .nav, .header, .footer, .tools\"):\n",
        "                element.decompose()\n",
        "\n",
        "            # Extract text\n",
        "            text = content.get_text(separator=\"\\n\", strip=True)\n",
        "            text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "            return text if len(text.split()) > 50 else None\n",
        "\n",
        "    except Exception as e:\n",
        "        log.warning(f\"Failed to scrape county law content from {law_url}: {e}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                                 CASE LAW SCRAPER                            #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "class KenyaLawScraper:\n",
        "    def __init__(self, cfg: Config, log: logging.Logger):\n",
        "        self.cfg = cfg\n",
        "        self.log = log\n",
        "        self.keywords = {k.lower() for k in cfg.KEYWORDS}\n",
        "        self.seen_case_ids = set()\n",
        "        self.driver = None\n",
        "        self.session = self._create_session()\n",
        "\n",
        "    def _create_session(self):\n",
        "        s = requests.Session()\n",
        "        retry = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
        "        adapter = HTTPAdapter(max_retries=retry)\n",
        "        s.mount(\"http://\", adapter)\n",
        "        s.mount(\"https://\", adapter)\n",
        "        s.headers.update({\n",
        "            \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\",\n",
        "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n",
        "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "            \"Accept-Encoding\": \"gzip, deflate, br\"\n",
        "        })\n",
        "        return s\n",
        "\n",
        "    def _setup_driver(self):\n",
        "        options = Options()\n",
        "        if self.cfg.CHROME_HEADLESS:\n",
        "            options.add_argument(\"--headless\")\n",
        "        options.add_argument(\"--no-sandbox\")\n",
        "        options.add_argument(\"--disable-dev-shm-usage\")\n",
        "        options.add_argument(\"--window-size=1920,1080\")\n",
        "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "        options.add_experimental_option('useAutomationExtension', False)\n",
        "\n",
        "        try:\n",
        "            if WEBDRIVER_MANAGER_SUPPORT:\n",
        "                service = Service(ChromeDriverManager().install())\n",
        "            else:\n",
        "                # Fallback if webdriver_manager is not installed\n",
        "                driver_path = self.cfg.LOCAL_CHROMEDRIVER_PATH or \"/usr/bin/chromedriver\" # Assuming default path\n",
        "                service = Service(driver_path)\n",
        "\n",
        "            driver = webdriver.Chrome(service=service, options=options)\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Failed to setup Chrome driver: {e}\")\n",
        "            return None\n",
        "\n",
        "        driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
        "        driver.execute_cdp_cmd('Network.setUserAgentOverride', {\n",
        "            \"userAgent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\"\n",
        "        })\n",
        "        return driver\n",
        "\n",
        "    def _case_id_from_url(self, url: str) -> Optional[str]:\n",
        "        # Extract case ID from new URL format: /akn/ke/judgment/kehc/2025/15858/eng@2025-11-07\n",
        "        patterns = [\n",
        "            r\"/akn/ke/judgment/[^/]+/(\\d+)/(?:eng@|\\d+)\",\n",
        "            r\"/judgments/view/(\\d+)\",\n",
        "            r\"/caselaw/cases/view/(\\d+)\"\n",
        "        ]\n",
        "        for pattern in patterns:\n",
        "            m = re.search(pattern, url)\n",
        "            if m:\n",
        "                return m.group(1)\n",
        "        return None\n",
        "\n",
        "    def fetch_case_urls_selenium(self) -> List[str]:\n",
        "        self.log.info(\"Collecting case URLs from new Kenya Law site...\")\n",
        "        urls = set()\n",
        "\n",
        "        try:\n",
        "            self.driver = self._setup_driver()\n",
        "            if not self.driver:\n",
        "                self.log.error(\"Failed to setup Selenium driver. Cannot collect case URLs.\")\n",
        "                return []\n",
        "\n",
        "            self.driver.get(self.cfg.JUDGMENTS_URL)\n",
        "            time.sleep(5)\n",
        "\n",
        "            # Wait for page to load\n",
        "            WebDriverWait(self.driver, 10).until(\n",
        "                EC.presence_of_element_located((By.TAG_NAME, \"table\"))\n",
        "            )\n",
        "\n",
        "            # Get initial page URLs\n",
        "            soup = BeautifulSoup(self.driver.page_source, \"lxml\")\n",
        "            self._extract_urls_from_page(soup, urls)\n",
        "\n",
        "            # Handle pagination\n",
        "            page_count = 0\n",
        "            while page_count < (self.cfg.MAX_PAGES or 10):  # Limit pages if specified\n",
        "                try:\n",
        "                    # Look for next button\n",
        "                    next_buttons = self.driver.find_elements(By.XPATH,\n",
        "                        \"//a[contains(text(), 'Next') or contains(@class, 'next') or contains(@aria-label, 'next')]\")\n",
        "\n",
        "                    if not next_buttons:\n",
        "                        break\n",
        "\n",
        "                    next_btn = next_buttons[0]\n",
        "                    if \"disabled\" in next_btn.get_attribute(\"class\") or not next_btn.is_enabled():\n",
        "                        break\n",
        "\n",
        "                    # Click next page\n",
        "                    self.driver.execute_script(\"arguments[0].click();\", next_btn)\n",
        "                    time.sleep(4)\n",
        "\n",
        "                    # Wait for new content to load\n",
        "                    WebDriverWait(self.driver, 10).until(\n",
        "                        EC.presence_of_element_located((By.TAG_NAME, \"table\"))\n",
        "                    )\n",
        "\n",
        "                    # Extract URLs from new page\n",
        "                    soup = BeautifulSoup(self.driver.page_source, \"lxml\")\n",
        "                    self._extract_urls_from_page(soup, urls)\n",
        "\n",
        "                    page_count += 1\n",
        "                    self.log.info(f\"Processed page {page_count}, total URLs: {len(urls)}\")\n",
        "\n",
        "                except (TimeoutException, NoSuchElementException) as e:\n",
        "                    self.log.info(\"No more pages or pagination failed\")\n",
        "                    break\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Error collecting URLs: {e}\")\n",
        "        finally:\n",
        "            if self.driver:\n",
        "                self.driver.quit()\n",
        "\n",
        "        final = list(urls)\n",
        "        if self.cfg.MAX_CASES:\n",
        "            final = final[:self.cfg.MAX_CASES]\n",
        "        self.log.info(f\"Collected {len(final)} case URLs\")\n",
        "        return final\n",
        "\n",
        "    def scrape_one_case(self, url: str) -> Optional[Dict]:\n",
        "        case_id = self._case_id_from_url(url)\n",
        "        if not case_id or case_id in self.seen_case_ids:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            self.log.info(f\"Scraping case: {url}\")\n",
        "            resp = self.session.get(url, timeout=self.cfg.REQUEST_TIMEOUT)\n",
        "            resp.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "\n",
        "            # Extract case title\n",
        "            title_elem = soup.find(\"h1\") or soup.find(\"title\")\n",
        "            case_name = title_elem.get_text(strip=True) if title_elem else \"Unknown Case\"\n",
        "\n",
        "            # Extract case content - try multiple selectors for new site\n",
        "            content_selectors = [\n",
        "                \"div.fr-view\",  # Rich text content\n",
        "                \"div.content\",\n",
        "                \"article\",\n",
        "                \"main\",\n",
        "                \".judgment-content\",\n",
        "                \".case-content\"\n",
        "            ]\n",
        "\n",
        "            content = None\n",
        "            for selector in content_selectors:\n",
        "                content = soup.select_one(selector)\n",
        "                if content:\n",
        "                    break\n",
        "\n",
        "            if not content:\n",
        "                # Fallback: get main content area\n",
        "                content = soup.find(\"main\") or soup.find(\"article\") or soup.find(\"div\", class_=re.compile(\"content\"))\n",
        "\n",
        "            if not content:\n",
        "                self.log.warning(f\"No content found for case {case_id}\")\n",
        "                return None\n",
        "\n",
        "            # Clean up content\n",
        "            for element in content.select(\"script, style, nav, header, footer, .nav, .header, .footer, .tools, .act-tools\"):\n",
        "                element.decompose()\n",
        "\n",
        "            # Extract text\n",
        "            text = content.get_text(separator=\"\\n\", strip=True)\n",
        "            text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "            if len(text.split()) < 100:\n",
        "                self.log.warning(f\"Case {case_id} has insufficient text: {len(text.split())} words\")\n",
        "                return None\n",
        "\n",
        "            # Extract metadata\n",
        "            metadata = self._extract_case_metadata(soup)\n",
        "\n",
        "            data = {\n",
        "                \"case_id\": case_id,\n",
        "                \"case_name\": case_name,\n",
        "                \"url\": url,\n",
        "                \"text\": text,\n",
        "                \"text_length_words\": len(text.split()),\n",
        "                \"scraped_at\": datetime.now().isoformat(),\n",
        "                \"metadata\": metadata\n",
        "            }\n",
        "\n",
        "            self.seen_case_ids.add(case_id)\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Case failed {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _extract_case_metadata(self, soup: BeautifulSoup) -> Dict[str, str]:\n",
        "        \"\"\"Extract case metadata from the page\"\"\"\n",
        "        metadata = {}\n",
        "\n",
        "        try:\n",
        "            # Look for common metadata patterns\n",
        "            meta_selectors = {\n",
        "                \"court\": [\"span.court\", \"div.court\", \"td.cell-court\"],\n",
        "                \"date\": [\"span.date\", \"div.date\", \"td.cell-date\", \"time\"],\n",
        "                \"case_number\": [\"span.case-number\", \"div.case-number\", \"td.cell-case-number\"],\n",
        "                \"judges\": [\"span.judges\", \"div.judges\", \"p.judges\"],\n",
        "                \"citation\": [\"span.citation\", \"div.citation\", \"td.cell-citation\"]\n",
        "            }\n",
        "\n",
        "            for key, selectors in meta_selectors.items():\n",
        "                for selector in selectors:\n",
        "                    element = soup.select_one(selector)\n",
        "                    if element:\n",
        "                        metadata[key] = element.get_text(strip=True)\n",
        "                        break\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log.debug(f\"Metadata extraction failed: {e}\")\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    def run_case_scrape(self, urls: List[str], handler: DataHandler) -> int:\n",
        "        self.log.info(f\"Scraping {len(urls)} cases...\")\n",
        "        saved = 0\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=self.cfg.MAX_SCRAPE_WORKERS) as executor:\n",
        "            future_to_url = {executor.submit(self.scrape_one_case, url): url for url in urls}\n",
        "\n",
        "            for future in as_completed(future_to_url):\n",
        "                url = future_to_url[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    if result and handler.save_case(result):\n",
        "                        saved += 1\n",
        "                except Exception as e:\n",
        "                    self.log.error(f\"Case scraping failed for {url}: {e}\")\n",
        "\n",
        "        return saved\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                                   MAIN                                      #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "# Define cfg in the global scope for subsequent cells\n",
        "cfg = Config()"
      ],
      "id": "1850480f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "562cb2f5"
      },
      "source": [
        "%run /content/kenyalaw_scraper_full.py"
      ],
      "id": "562cb2f5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "139a2611"
      },
      "source": [
        "!pip install -q selenium"
      ],
      "id": "139a2611",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6a54957",
        "collapsed": true
      },
      "source": [
        "# train_legal.py\n",
        "# Full training pipeline for Kenyan Legal AI Model\n",
        "# Works with your KenyaLaw Scraper v6.0 output\n",
        "\n",
        "import json\n",
        "import os\n",
        "import unsloth\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Any\n",
        "from datasets import Dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "# ========================= CONFIG =========================\n",
        "class TrainConfig:\n",
        "    BASE_DIR = Path(os.path.expanduser(\"~/projects/kenya_law/data\"))\n",
        "    OUTPUT_DIR = Path(\"./kenya-legal-llm\")\n",
        "    MODEL_NAME = \"unsloth/Mistral-7B-Instruct-v0.3-bnb-4bit\"  # or \"unsloth/Qwen2-7B-7B-bnb-4bit\" # Corrected typo\n",
        "\n",
        "    MAX_SEQ_LENGTH = 8192\n",
        "    BATCH_SIZE = 2\n",
        "    GRADIENT_ACCUMULATION_STEPS = 8\n",
        "    LEARNING_RATE = 2e-4\n",
        "    NUM_EPOCHS = 3\n",
        "    WARMUP_STEPS = 10\n",
        "    LOGGING_STEPS = 10\n",
        "    SAVE_STEPS = 100\n",
        "    EVAL_STRATEGY = \"no\" # Set to \"no\" when no eval_dataset is provided\n",
        "\n",
        "    # Output formats\n",
        "    GGUF_OUTPUT = OUTPUT_DIR / \"gguf\"\n",
        "    HF_OUTPUT = OUTPUT_DIR / \"hf\"\n",
        "\n",
        "cfg = TrainConfig()\n",
        "cfg.OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "cfg.GGUF_OUTPUT.mkdir(exist_ok=True)\n",
        "cfg.HF_OUTPUT.mkdir(exist_ok=True)\n",
        "\n",
        "# ========================= LOAD DATA =========================\n",
        "def load_jsonl(file_path: Path) -> List[Dict]:\n",
        "    data = []\n",
        "    if not file_path.exists():\n",
        "        print(f\"Warning: {file_path} not found!\")\n",
        "        return data\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            if line.strip():\n",
        "                try:\n",
        "                    data.append(json.loads(line))\n",
        "                except:\n",
        "                    continue\n",
        "    print(f\"Loaded {len(data)} cases from {file_path.name}\")\n",
        "    return data\n",
        "\n",
        "def load_json(file_path: Path) -> Dict:\n",
        "    if not file_path.exists():\n",
        "        print(f\"Warning: {file_path} not found!\")\n",
        "        return {}\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "print(\"Loading Kenyan legal data...\")\n",
        "cases = load_jsonl(cfg.BASE_DIR / \"kenya_law_training_data.jsonl\")\n",
        "constitution = load_json(cfg.BASE_DIR / \"constitution.json\")\n",
        "acts = load_json(cfg.BASE_DIR / \"acts_of_kenya.json\")\n",
        "subsidiary = load_json(cfg.BASE_DIR / \"subsidiary_legislation.json\")\n",
        "counties = load_json(cfg.BASE_DIR / \"county_legislation.json\")\n",
        "\n",
        "# ========================= PREPARE DATA =========================\n",
        "def chunk_text(text: str, chunk_size: int = 1000, overlap: int = 200) -> List[str]:\n",
        "    words = text.split()\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        chunk = \" \".join(words[i:i + chunk_size])\n",
        "        chunks.append(chunk)\n",
        "        i += chunk_size - overlap\n",
        "    return chunks\n",
        "\n",
        "def create_instruction_samples() -> List[Dict[str, Any]]:\n",
        "    samples = []\n",
        "\n",
        "    # 1. Constitution\n",
        "    for title, content in constitution.items():\n",
        "        chunks = chunk_text(content, 1200)\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            samples.append({\n",
        "                \"instruction\": f\"You are a Kenyan constitutional law expert. Answer based on the Constitution of Kenya 2010.\",\n",
        "                \"input\": f\"Explain: {title}\" + (f\" (Part {i+1})\" if len(chunks) > 1 else \"\"),\n",
        "                \"output\": chunk[:4000]\n",
        "            })\n",
        "\n",
        "    # 2. Acts of Parliament\n",
        "    for title, content in acts.items():\n",
        "        if len(content.split()) > 200:\n",
        "            samples.append({\n",
        "                \"instruction\": \"You are a Kenyan lawyer. Cite and explain the relevant law.\",\n",
        "                \"input\": f\"What does the law say about: {title}?\",\n",
        "                \"output\": content[:6000]\n",
        "            })\n",
        "\n",
        "    # 3. Case Law (High-value)\n",
        "    for case in cases[:2000]:  # Use top 2000 cases\n",
        "        text = case.get(\"text\", \"\")\n",
        "        if len(text.split()) < 200:\n",
        "            continue\n",
        "        metadata = case.get(\"metadata\", {})\n",
        "        court = metadata.get(\"court\", \"Kenyan Court\")\n",
        "        date = metadata.get(\"date\", \"Unknown date\")\n",
        "\n",
        "        samples.append({\n",
        "            \"instruction\": \"You are a Kenyan judge. Analyze this case and give legal reasoning.\",\n",
        "            \"input\": f\"Case: {case['case_name']}\\nCourt: {court}\\nDate: {date}\\n\\n{text[:3000]}...\",\n",
        "            \"output\": f\"**Case Analysis:**\\n\\n**Citation:** {case['case_name']}\\n**Court:** {court}\\n**Date:** {date}\\n\\n**Legal Reasoning:**\\n{text[1000:8000]}\\n\\n**Held:** {text.split('Held:')[-1].split('JUDGMENT')[0] if 'Held:' in text else 'See full judgment.'}\"\n",
        "        })\n",
        "\n",
        "    # 4. County Laws (PDF + HTML)\n",
        "    for county, data in counties.items():\n",
        "        for law_name, law in data.get(\"laws\", {}).items():\n",
        "            content = law.get(\"content\", \"\")\n",
        "            if \"UNABLE_TO_EXTRACT\" in content or len(content.split()) < 100:\n",
        "                continue\n",
        "            samples.append({\n",
        "                \"instruction\": f\"You are a legal expert in {county} County legislation.\",\n",
        "                \"input\": f\"What does {county} law say about: {law_name}?\",\n",
        "                \"output\": content[:5000]\n",
        "            })\n",
        "\n",
        "    print(f\"Created {len(samples)} training samples\")\n",
        "    return samples\n",
        "\n",
        "# ========================= FORMAT FOR TRAINING =========================\n",
        "def format_alpaca(sample: Dict) -> str:\n",
        "    return f\"\"\"### Instruction:\n",
        "{sample['instruction']}\n",
        "\n",
        "### Input:\n",
        "{sample['input']}\n",
        "\n",
        "### Response:\n",
        "{sample['output']}\"\"\"\n",
        "\n",
        "print(\"Creating dataset...\")\n",
        "raw_samples = create_instruction_samples()\n",
        "formatted = [format_alpaca(s) for s in raw_samples]\n",
        "dataset = Dataset.from_dict({\"text\": formatted})\n",
        "\n",
        "# ========================= LOAD MODEL =========================\n",
        "print(f\"Loading {cfg.MODEL_NAME} with Unsloth...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=cfg.MODEL_NAME,\n",
        "    max_seq_length=cfg.MAX_SEQ_LENGTH,\n",
        "    dtype=None,  # Auto detect\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\", # Added device_map=\"auto\"\n",
        "    # llm_int8_enable_fp32_cpu_offload=True # Optional: Enable CPU offloading for fp32 modules\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=64,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0,\n",
        "    bias=\"none\",\n",
        "    use_gradient_checkpointing=\"unsloth\",\n",
        "    random_state=3407,\n",
        "    max_seq_length=cfg.MAX_SEQ_LENGTH,\n",
        ")\n",
        "\n",
        "# ========================= TRAINER =========================\n",
        "print(\"Setting up SFTTrainer...\")\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=cfg.MAX_SEQ_LENGTH,\n",
        "    dataset_num_proc=2,\n",
        "    packing=True,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=cfg.BATCH_SIZE,\n",
        "        gradient_accumulation_steps=cfg.GRADIENT_ACCUMULATION_STEPS,\n",
        "        warmup_steps=cfg.WARMUP_STEPS,\n",
        "        num_train_epochs=cfg.NUM_EPOCHS,\n",
        "        learning_rate=cfg.LEARNING_RATE,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "        logging_steps=cfg.LOGGING_STEPS,\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.01,\n",
        "        lr_scheduler_type=\"linear\",\n",
        "        seed=3407,\n",
        "        output_dir=str(cfg.OUTPUT_DIR),\n",
        "        report_to=\"none\",\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=cfg.SAVE_STEPS,\n",
        "        eval_strategy=cfg.EVAL_STRATEGY,\n",
        "        load_best_model_at_end=False, # Set to False when no eval dataset\n",
        "        tokenizer=tokenizer, # Added tokenizer here\n",
        "    ),\n",
        ")\n",
        "\n",
        "print(\"Starting training...\")\n",
        "trainer.train()\n",
        "\n",
        "# ========================= SAVE MODEL =========================\n",
        "print(\"Saving model...\")\n",
        "trainer.save_model(str(cfg.HF_OUTPUT))\n",
        "tokenizer.save_pretrained(str(cfg.HF_OUTPUT))\n",
        "\n",
        "# ========================= EXPORT TO GGUF (for Ollama) =========================\n",
        "print(\"Exporting to GGUF (Q5_K_M)...\")\n",
        "model.save_pretrained_gguf(\n",
        "    str(cfg.GGUF_OUTPUT),\n",
        "    tokenizer,\n",
        "    quantization_method=\"q5_k_m\",\n",
        ")"
      ],
      "id": "e6a54957",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "863b2ce4"
      },
      "source": [
        "!pip install -q unsloth"
      ],
      "id": "863b2ce4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "712739ca"
      },
      "source": [
        "# Task\n",
        "Create a chatbot using the fine-tuned model."
      ],
      "id": "712739ca"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0ea2309"
      },
      "source": [
        "## Load the trained model\n",
        "\n",
        "### Subtask:\n",
        "Load the fine-tuned model and tokenizer using Unsloth's `FastLanguageModel`.\n"
      ],
      "id": "f0ea2309"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08a37c72"
      },
      "source": [
        "**Reasoning**:\n",
        "Import FastLanguageModel and load the model and tokenizer from the saved Hugging Face directory using the defined config path.\n",
        "\n"
      ],
      "id": "08a37c72"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42c69a00"
      },
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Define the path to the saved Hugging Face model directory\n",
        "model_path = cfg.HF_OUTPUT\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=str(model_path),  # Use the path to the saved model\n",
        "    max_seq_length=cfg.MAX_SEQ_LENGTH,\n",
        "    dtype=None,  # Auto detect\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "id": "42c69a00",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a2fd4aa"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the missing `unsloth` library to resolve the ModuleNotFoundError.\n",
        "\n"
      ],
      "id": "8a2fd4aa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D1t_64kYO3eR"
      },
      "source": [
        "!pip install -q unsloth"
      ],
      "id": "D1t_64kYO3eR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "125c903d"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that `unsloth` is installed, attempt to load the fine-tuned model and tokenizer again.\n",
        "\n"
      ],
      "id": "125c903d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d638f364"
      },
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Define the path to the saved Hugging Face model directory\n",
        "model_path = cfg.HF_OUTPUT\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=str(model_path),  # Use the path to the saved model\n",
        "    max_seq_length=cfg.MAX_SEQ_LENGTH,\n",
        "    dtype=None,  # Auto detect\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "id": "d638f364",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9137b70"
      },
      "source": [
        "**Reasoning**:\n",
        "The `cfg` object was not defined in the current kernel session. Re-run the code cell that defines the `Config` class and creates the `cfg` object.\n",
        "\n"
      ],
      "id": "b9137b70"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8QasnxKPjVD"
      },
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime\n",
        "from typing import List, Set, Dict, Optional\n",
        "from urllib.parse import urljoin\n",
        "import io\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "# Try to import webdriver_manager, but make it optional\n",
        "try:\n",
        "    from webdriver_manager.chrome import ChromeDriverManager\n",
        "    WEBDRIVER_MANAGER_SUPPORT = True\n",
        "except ImportError:\n",
        "    WEBDRIVER_MANAGER_SUPPORT = False\n",
        "    print(\"Warning: webdriver_manager not installed. Chrome driver will not be automatically managed.\")\n",
        "\n",
        "\n",
        "# Try to import PDF libraries, but make them optional\n",
        "try:\n",
        "    import PyPDF2\n",
        "    PDF_SUPPORT = True\n",
        "except ImportError:\n",
        "    PDF_SUPPORT = False\n",
        "    print(\"Warning: PyPDF2 not installed. PDF content extraction will be limited.\")\n",
        "\n",
        "try:\n",
        "    import pdfplumber\n",
        "    PDFPLUMBER_SUPPORT = True\n",
        "except ImportError:\n",
        "    PDFPLUMBER_SUPPORT = False\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                               CONFIGURATION                                 #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    BASE_PROJECT_DIR: str = os.path.join(os.path.expanduser(\"~\"), \"projects\", \"kenya_law\")\n",
        "\n",
        "    LOG_DIR: str = field(init=False)\n",
        "    DATA_DIR: str = field(init=False)\n",
        "    LOG_FILE: str = field(init=False)\n",
        "    DATA_FILE: str = field(init=False)\n",
        "    DEBUG_DIR: str = field(init=False)\n",
        "    CONSTITUTION_FILE: str = field(init=False)\n",
        "    ACTS_FILE: str = field(init=False)\n",
        "    SUBSIDIARY_FILE: str = field(init=False)\n",
        "    COUNTIES_FILE: str = field(init=False)\n",
        "    PDF_DIR: str = field(init=False)  # NEW: Directory to store downloaded PDFs\n",
        "\n",
        "    MAX_CASES: int = None\n",
        "    MAX_PAGES: int = None\n",
        "    MAX_COUNTY_LAWS: int = 50\n",
        "    REQUEST_TIMEOUT: int = 30\n",
        "    SELENIUM_TIMEOUT: int = 45\n",
        "    YEAR_START: int = 2020\n",
        "\n",
        "    BASE_URL: str = \"https://kenyalaw.org\"\n",
        "    NEW_BASE_URL: str = \"https://new.kenyalaw.org\"\n",
        "    SEARCH_URL: str = \"https://new.kenyalaw.org/search/\"\n",
        "    JUDGMENTS_URL: str = \"https://new.kenyalaw.org/judgments/\"\n",
        "    COUNTIES_URL: str = \"https://new.kenyalaw.org/legislation/counties\"\n",
        "    ACTS_TOC_URL: str = \"https://new.kenyalaw.org/legislation/\"\n",
        "\n",
        "    LOCAL_CHROMEDRIVER_PATH: Optional[str] = None\n",
        "    CHROME_HEADLESS: bool = True\n",
        "\n",
        "    KEYWORDS: List[str] = field(default_factory=lambda: [\n",
        "        \"constitution\", \"human rights\", \"land\", \"election\", \"criminal\", \"civil\",\n",
        "        \"jurisdiction\", \"appeal\", \"judicial review\", \"injunction\", \"contract\"\n",
        "    ])\n",
        "\n",
        "    MAX_SCRAPE_WORKERS: int = 10\n",
        "    ENABLE_PDF_EXTRACTION: bool = True  # NEW: Control PDF processing\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        self.LOG_DIR = os.path.join(self.BASE_PROJECT_DIR, \"logs\")\n",
        "        self.DATA_DIR = os.path.join(self.BASE_PROJECT_DIR, \"data\")\n",
        "        self.DEBUG_DIR = os.path.join(self.BASE_PROJECT_DIR, \"debug\")\n",
        "        self.PDF_DIR = os.path.join(self.DATA_DIR, \"pdfs\")  # NEW\n",
        "        self.LOG_FILE = os.path.join(self.LOG_DIR, f\"kenyalaw_scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
        "        self.DATA_FILE = os.path.join(self.DATA_DIR, \"kenya_law_training_data.jsonl\")\n",
        "        self.CONSTITUTION_FILE = os.path.join(self.DATA_DIR, \"constitution.json\")\n",
        "        self.ACTS_FILE = os.path.join(self.DATA_DIR, \"acts_of_kenya.json\")\n",
        "        self.SUBSIDIARY_FILE = os.path.join(self.DATA_DIR, \"subsidiary_legislation.json\")\n",
        "        self.COUNTIES_FILE = os.path.join(self.DATA_DIR, \"county_legislation.json\")\n",
        "\n",
        "        for d in [self.LOG_DIR, self.DATA_DIR, self.DEBUG_DIR, self.PDF_DIR]:\n",
        "            os.makedirs(d, exist_ok=True)\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                                 LOGGING                                    #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "def setup_logging(log_file: str) -> logging.Logger:\n",
        "    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
        "    logger = logging.getLogger(\"KenyaLaw-Scraper-v6.0-FULL\")\n",
        "    logger.setLevel(logging.INFO)\n",
        "    logger.handlers.clear()\n",
        "    fh = logging.FileHandler(log_file, encoding=\"utf-8\")\n",
        "    ch = logging.StreamHandler()\n",
        "    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
        "    fh.setFormatter(formatter)\n",
        "    ch.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "    logger.addHandler(ch)\n",
        "    return logger\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                               DATA HANDLER                                 #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "class DataHandler:\n",
        "    def __init__(self, cfg: Config, log: logging.Logger):\n",
        "        self.cfg = cfg\n",
        "        self.log = log\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def load_existing_case_ids(self) -> Set[str]:\n",
        "        if not os.path.exists(self.cfg.DATA_FILE):\n",
        "            return set()\n",
        "        ids = set()\n",
        "        try:\n",
        "            with open(self.cfg.DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "                for line_num, line in enumerate(f, 1):\n",
        "                    line = line.strip()\n",
        "                    if not line: continue\n",
        "                    try:\n",
        "                        data = json.loads(line)\n",
        "                        if \"case_id\" in data:\n",
        "                            ids.add(data[\"case_id\"])\n",
        "                    except json.JSONDecodeError:\n",
        "                        self.log.warning(f\"Bad JSON at line {line_num}\")\n",
        "            self.log.info(f\"Loaded {len(ids)} existing case IDs\")\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Failed to load IDs: {e}\")\n",
        "        return ids\n",
        "\n",
        "    def save_case(self, case: Dict) -> bool:\n",
        "        try:\n",
        "            json_line = json.dumps(case, ensure_ascii=False)\n",
        "            with self.lock:\n",
        "                with open(self.cfg.DATA_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "                    f.write(json_line + \"\\n\")\n",
        "            self.log.info(f\"Saved case {case['case_id']} ({len(case['text'].split())} words)\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Save failed: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                              PDF HANDLING                                  #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "class PDFHandler:\n",
        "    def __init__(self, cfg: Config, log: logging.Logger):\n",
        "        self.cfg = cfg\n",
        "        self.log = log\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_url: str, pdf_content: bytes) -> Optional[str]:\n",
        "        \"\"\"Extract text from PDF content using multiple methods\"\"\"\n",
        "        if not self.cfg.ENABLE_PDF_EXTRACTION:\n",
        "            return \"PDF_CONTENT_AVAILABLE_BUT_EXTRACTION_DISABLED\"\n",
        "\n",
        "        # Save PDF file for reference\n",
        "        pdf_filename = self._save_pdf_file(pdf_url, pdf_content)\n",
        "\n",
        "        extracted_text = None\n",
        "\n",
        "        # Try pdfplumber first (better for scanned PDFs with OCR)\n",
        "        if PDFPLUMBER_SUPPORT:\n",
        "            extracted_text = self._extract_with_pdfplumber(pdf_content)\n",
        "\n",
        "        # Fallback to PyPDF2\n",
        "        if not extracted_text and PDF_SUPPORT:\n",
        "            extracted_text = self._extract_with_pypdf2(pdf_content)\n",
        "\n",
        "        # Final fallback\n",
        "        if not extracted_text:\n",
        "            extracted_text = self._extract_fallback(pdf_content, pdf_filename)\n",
        "\n",
        "        return extracted_text\n",
        "\n",
        "    def _save_pdf_file(self, pdf_url: str, pdf_content: bytes) -> str:\n",
        "        \"\"\"Save PDF file to disk for reference\"\"\"\n",
        "        try:\n",
        "            # Create filename from URL\n",
        "            filename = re.sub(r'[^a-zA-Z0-9]', '_', pdf_url) + '.pdf'\n",
        "            filepath = os.path.join(self.cfg.PDF_DIR, filename)\n",
        "\n",
        "            with open(filepath, 'wb') as f:\n",
        "                f.write(pdf_content)\n",
        "\n",
        "            return filename\n",
        "        except Exception as e:\n",
        "            self.log.warning(f\"Failed to save PDF file: {e}\")\n",
        "            return \"unknown.pdf\"\n",
        "\n",
        "    def _extract_with_pdfplumber(self, pdf_content: bytes) -> Optional[str]:\n",
        "        \"\"\"Extract text using pdfplumber (better for scanned PDFs)\"\"\"\n",
        "        try:\n",
        "            text_parts = []\n",
        "            with pdfplumber.open(io.BytesIO(pdf_content)) as pdf:\n",
        "                for page in pdf.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        text_parts.append(page_text.strip())\n",
        "\n",
        "            if text_parts:\n",
        "                full_text = '\\n'.join(text_parts)\n",
        "                if len(full_text.split()) > 10:  # Ensure we have substantial text\n",
        "                    return full_text\n",
        "        except Exception as e:\n",
        "            self.log.debug(f\"pdfplumber extraction failed: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _extract_with_pypdf2(self, pdf_content: bytes) -> Optional[str]:\n",
        "        \"\"\"Extract text using PyPDF2\"\"\"\n",
        "        try:\n",
        "            text_parts = []\n",
        "            pdf_file = io.BytesIO(pdf_content)\n",
        "            reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "            for page in reader.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text_parts.append(page_text.strip())\n",
        "\n",
        "            if text_parts:\n",
        "                full_text = '\\n'.join(text_parts)\n",
        "                if len(full_text.split()) > 10:\n",
        "                    return full_text\n",
        "        except Exception as e:\n",
        "            self.log.debug(f\"PyPDF2 extraction failed: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _extract_fallback(self, pdf_content: bytes, pdf_filename: str) -> str:\n",
        "        \"\"\"Fallback method when PDF text extraction fails\"\"\"\n",
        "        self.log.warning(f\"PDF text extraction failed for {pdf_filename}. Content saved to disk.\")\n",
        "        return f\"PDF_CONTENT_UNABLE_TO_EXTRACT_TEXT_SAVED_AS_{pdf_filename}\"\n",
        "\n",
        "    def is_pdf_url(self, url: str) -> bool:\n",
        "        \"\"\"Check if URL points to a PDF document\"\"\"\n",
        "        return url.lower().endswith('.pdf') or '/pdf/' in url.lower()\n",
        "\n",
        "    def download_pdf(self, session: requests.Session, pdf_url: str) -> Optional[bytes]:\n",
        "        \"\"\"Download PDF content\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\",\n",
        "                \"Accept\": \"application/pdf, */*\",\n",
        "                \"Referer\": \"https://new.kenyalaw.org/\"\n",
        "            }\n",
        "\n",
        "            response = session.get(pdf_url, timeout=30, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            content_type = response.headers.get('content-type', '').lower()\n",
        "            if 'pdf' in content_type or response.content[:4] == b'%PDF':\n",
        "                return response.content\n",
        "            else:\n",
        "                self.log.warning(f\"URL {pdf_url} doesn't contain PDF data\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Failed to download PDF from {pdf_url}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                      STATIC CONTENT SCRAPERS (Constitution & Acts)          #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "def save_constitution_data(cfg: Config, log: logging.Logger, data: Dict[str, str]) -> None:\n",
        "    try:\n",
        "        with open(cfg.CONSTITUTION_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "        log.info(f\"Constitution saved → {cfg.CONSTITUTION_FILE}\")\n",
        "    except Exception as e:\n",
        "        log.error(f\"Failed to save constitution: {e}\")\n",
        "\n",
        "def scrape_constitution(cfg: Config, log: logging.Logger) -> None:\n",
        "    log.info(\"Scraping Constitution of Kenya (2010)...\")\n",
        "    if os.path.exists(cfg.CONSTITUTION_FILE):\n",
        "        log.info(f\"Constitution already exists → {cfg.CONSTITUTION_FILE}. Skipping.\")\n",
        "        return\n",
        "\n",
        "    # Try multiple potential sources\n",
        "    SOURCES = [\n",
        "        \"https://new.kenyalaw.org/akn/ke/act/2010/constitution/eng@2010-09-03\",\n",
        "        \"https://new.kenyalaw.org/akn/ke/act/2010/constitution\",\n",
        "        \"https://kenyalaw.org/kl/index.php?id=398\"\n",
        "    ]\n",
        "\n",
        "    session = requests.Session()\n",
        "    session.headers.update({\n",
        "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\",\n",
        "        \"Accept\": \"application/xml, text/xml, text/html, */*\",\n",
        "        \"Referer\": \"https://new.kenyalaw.org/\",\n",
        "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "    })\n",
        "\n",
        "    for source_url in SOURCES:\n",
        "        try:\n",
        "            log.info(f\"Trying source: {source_url}\")\n",
        "            resp = session.get(source_url, timeout=60)\n",
        "            resp.raise_for_status()\n",
        "\n",
        "            content_type = resp.headers.get('content-type', '').lower()\n",
        "\n",
        "            if 'xml' in content_type:\n",
        "                # Parse as XML\n",
        "                soup = BeautifulSoup(resp.content, \"xml\")\n",
        "                body = soup.find(\"body\")\n",
        "                if not body:\n",
        "                    log.warning(f\"No <body> found in XML from {source_url}\")\n",
        "                    continue\n",
        "\n",
        "                data = {}\n",
        "                current_title = \"Preamble\"\n",
        "                current_lines = []\n",
        "\n",
        "                # Extract all relevant elements\n",
        "                for elem in body.find_all(['heading', 'num', 'p', 'chapter', 'part', 'section', 'article']):\n",
        "                    if elem.name in ['heading', 'chapter', 'part']:\n",
        "                        # Save previous section\n",
        "                        if current_lines:\n",
        "                            data[current_title] = \"\\n\".join(current_lines).strip()\n",
        "                            current_lines = []\n",
        "                        # Start new section\n",
        "                        current_title = elem.get_text(strip=True)\n",
        "                        if not current_title:\n",
        "                            current_title = \"Untitled Section\"\n",
        "\n",
        "                    elif elem.name == 'p' and elem.get_text(strip=True):\n",
        "                        text = elem.get_text(strip=True)\n",
        "                        if len(text) > 10:  # Filter out very short paragraphs\n",
        "                            current_lines.append(text)\n",
        "\n",
        "                # Save the last section\n",
        "                if current_lines:\n",
        "                    data[current_title] = \"\\n\".join(current_lines).strip()\n",
        "\n",
        "            else:\n",
        "                # Parse as HTML\n",
        "                soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "\n",
        "                # Try different content selectors\n",
        "                content_selectors = [\n",
        "                    \"div.act-content\",\n",
        "                    \"div.content\",\n",
        "                    \"article\",\n",
        "                    \"main\",\n",
        "                    \"div.container\",\n",
        "                    \"#content\"\n",
        "                ]\n",
        "\n",
        "                content = None\n",
        "                for selector in content_selectors:\n",
        "                    content = soup.select_one(selector)\n",
        "                    if content:\n",
        "                        break\n",
        "\n",
        "                if not content:\n",
        "                    log.warning(f\"No content found with selectors in {source_url}\")\n",
        "                    continue\n",
        "\n",
        "                # Clean up the content\n",
        "                for element in content.select(\"script, style, nav, header, footer, .nav, .header, .footer\"):\n",
        "                    element.decompose()\n",
        "\n",
        "                # Extract text and structure\n",
        "                data = {}\n",
        "                current_section = \"Constitution of Kenya\"\n",
        "                sections = [current_section]\n",
        "                text_lines = []\n",
        "\n",
        "                for element in content.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'div']):\n",
        "                    text = element.get_text(strip=True)\n",
        "                    if not text:\n",
        "                        continue\n",
        "\n",
        "                    # Detect section headers\n",
        "                    if element.name in ['h1', 'h2', 'h3', 'h4']:\n",
        "                        if text_lines:  # Save previous section\n",
        "                            data[current_section] = \"\\n\".join(text_lines).strip()\n",
        "                            text_lines = []\n",
        "                        current_section = text\n",
        "                        sections.append(current_section)\n",
        "                    elif len(text) > 20:  # Substantial content\n",
        "                        text_lines.append(text)\n",
        "\n",
        "                # Save the last section\n",
        "                if text_lines:\n",
        "                    data[current_section] = \"\\n\".join(text_lines).strip()\n",
        "\n",
        "            # Filter out empty or very short sections\n",
        "            filtered_data = {}\n",
        "            for title, content in data.items():\n",
        "                if content and len(content.split()) >= 10:\n",
        "                    clean_content = re.sub(r'\\s+', ' ', content).strip()\n",
        "                    filtered_data[title] = clean_content\n",
        "\n",
        "            # If we have substantial content, save it\n",
        "            if filtered_data and sum(len(c.split()) for c in filtered_data.values()) > 500:\n",
        "                save_constitution_data(cfg, log, filtered_data)\n",
        "                total_words = sum(len(v.split()) for v in filtered_data.values())\n",
        "                log.info(f\"SUCCESS: Constitution scraped → {len(filtered_data)} sections, {total_words:,} words\")\n",
        "                return\n",
        "            else:\n",
        "                log.warning(f\"Insufficient content from {source_url}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            log.warning(f\"Source {source_url} failed: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Fallback: manual structure if all sources fail\n",
        "    log.warning(\"All automated sources failed. Creating placeholder structure.\")\n",
        "    fallback_data = {\n",
        "        \"Preamble\": \"We, the people of Kenya—ACKNOWLEDGING the supremacy of the Almighty God of all creation...\",\n",
        "        \"Chapter One - Sovereignty of the People\": \"1. (1) All sovereign power belongs to the people of Kenya...\",\n",
        "        \"Chapter Two - The Republic\": \"4. (1) Kenya is a sovereign Republic. (2) The Republic of Kenya shall be a multi-party democratic state...\",\n",
        "        \"Note\": \"This is a placeholder. The actual constitution text could not be scraped automatically. Consider manual entry.\"\n",
        "    }\n",
        "    save_constitution_data(cfg, log, fallback_data)\n",
        "    log.info(\"Created fallback constitution structure\")\n",
        "\n",
        "def save_acts_data(cfg: Config, log: logging.Logger, acts: Dict[str, str], subs: Dict[str, str]) -> None:\n",
        "    with open(cfg.ACTS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(acts, f, ensure_ascii=False, indent=2)\n",
        "    log.info(f\"Saved {len(acts)} Acts → {cfg.ACTS_FILE}\")\n",
        "\n",
        "    with open(cfg.SUBSIDIARY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(subs, f, ensure_ascii=False, indent=2)\n",
        "    log.info(f\"Saved {len(subs)} Subsidiary Laws → {cfg.SUBSIDIARY_FILE}\")\n",
        "\n",
        "def save_counties_data(cfg: Config, log: logging.Logger, counties_data: Dict[str, Dict]) -> None:\n",
        "    try:\n",
        "        with open(cfg.COUNTIES_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(counties_data, f, ensure_ascii=False, indent=2)\n",
        "        log.info(f\"County legislation saved → {cfg.COUNTIES_FILE}\")\n",
        "    except Exception as e:\n",
        "        log.error(f\"Failed to save county legislation: {e}\")\n",
        "\n",
        "\n",
        "def scrape_county_legislation(cfg: Config, log: logging.Logger) -> None:\n",
        "    \"\"\"Scrape county legislation from all 47 counties\"\"\"\n",
        "    log.info(\"Scraping County Legislation from all 47 counties...\")\n",
        "    if os.path.exists(cfg.COUNTIES_FILE):\n",
        "        log.info(f\"County legislation already exists → {cfg.COUNTIES_FILE}. Skipping.\")\n",
        "        return\n",
        "\n",
        "    session = requests.Session()\n",
        "    pdf_handler = PDFHandler(cfg, log)  # NEW: PDF handler\n",
        "\n",
        "    session.headers.update({\n",
        "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\",\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "        \"Referer\": cfg.NEW_BASE_URL\n",
        "    })\n",
        "\n",
        "    counties_data = {}\n",
        "    total_county_laws = 0\n",
        "\n",
        "    try:\n",
        "        # Get the main counties page\n",
        "        log.info(f\"Accessing counties page: {cfg.COUNTIES_URL}\")\n",
        "        resp = session.get(cfg.COUNTIES_URL, timeout=cfg.REQUEST_TIMEOUT)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "\n",
        "        # Extract all county links\n",
        "        county_links = []\n",
        "        flow_columns = soup.select('.flow-columns-group')\n",
        "\n",
        "        for column in flow_columns:\n",
        "            links = column.select('a[href^=\"/legislation/ke-\"]')\n",
        "            for link in links:\n",
        "                county_name = link.get_text(strip=True)\n",
        "                county_url = urljoin(cfg.NEW_BASE_URL, link.get('href'))\n",
        "                county_links.append((county_name, county_url))\n",
        "\n",
        "        log.info(f\"Found {len(county_links)} counties to process\")\n",
        "\n",
        "        # Process each county\n",
        "        for county_name, county_url in county_links:\n",
        "            try:\n",
        "                log.info(f\"Processing county: {county_name}\")\n",
        "                time.sleep(1)  # Be respectful\n",
        "\n",
        "                # Get county page\n",
        "                county_resp = session.get(county_url, timeout=cfg.REQUEST_TIMEOUT)\n",
        "                county_resp.raise_for_status()\n",
        "                county_soup = BeautifulSoup(county_resp.text, \"lxml\")\n",
        "\n",
        "                # Extract county laws\n",
        "                county_laws = {}\n",
        "\n",
        "                # Look for laws in tables or lists\n",
        "                law_elements = county_soup.select('tr.has-children, .legislation-item, .law-item')\n",
        "\n",
        "                if not law_elements:\n",
        "                    # Try alternative selectors\n",
        "                    law_elements = county_soup.select('a[href*=\"/akn/ke/act/\"]')\n",
        "\n",
        "                laws_processed = 0\n",
        "\n",
        "                for law_element in law_elements:\n",
        "                    if laws_processed >= cfg.MAX_COUNTY_LAWS:\n",
        "                        break\n",
        "\n",
        "                    try:\n",
        "                        # Extract law link and title\n",
        "                        if law_element.name == 'tr':\n",
        "                            link_elem = law_element.select_one('td.cell-title a')\n",
        "                        else:\n",
        "                            link_elem = law_element\n",
        "\n",
        "                        if not link_elem or not link_elem.get('href'):\n",
        "                            continue\n",
        "\n",
        "                        law_title = link_elem.get_text(strip=True)\n",
        "                        law_url = urljoin(cfg.NEW_BASE_URL, link_elem.get('href'))\n",
        "\n",
        "                        # Skip if it's not a direct law link\n",
        "                        if '/akn/ke/act/' not in law_url:\n",
        "                            continue\n",
        "\n",
        "                        # Scrape the actual law content (now with PDF support)\n",
        "                        time.sleep(0.5)\n",
        "                        law_content = scrape_county_law_content(session, pdf_handler, law_url, log)\n",
        "\n",
        "                        if law_content and len(law_content.split()) > 50:  # Reduced threshold for PDFs\n",
        "                            county_laws[law_title] = {\n",
        "                                'url': law_url,\n",
        "                                'content': law_content,\n",
        "                                'word_count': len(law_content.split()),\n",
        "                                'content_type': 'pdf' if pdf_handler.is_pdf_url(law_url) else 'html'\n",
        "                            }\n",
        "                            laws_processed += 1\n",
        "                            total_county_laws += 1\n",
        "                            log.info(f\"    ✓ County law: {law_title} ({len(law_content.split())} words) [{county_laws[law_title]['content_type'].upper()}]\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        log.warning(f\"Failed to process county law in {county_name}: {e}\")\n",
        "                        continue\n",
        "\n",
        "                # Add county data\n",
        "                if county_laws:\n",
        "                    counties_data[county_name] = {\n",
        "                        'county_url': county_url,\n",
        "                        'laws': county_laws,\n",
        "                        'total_laws': len(county_laws),\n",
        "                        'scraped_at': datetime.now().isoformat()\n",
        "                    }\n",
        "                    log.info(f\"  ✓ {county_name}: {len(county_laws)} laws\")\n",
        "\n",
        "            except Exception as e:\n",
        "                log.error(f\"Failed to process county {county_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Save counties data\n",
        "        save_counties_data(cfg, log, counties_data)\n",
        "        log.info(f\"SUCCESS: County legislation scraped → {len(counties_data)} counties, {total_county_laws} total laws\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log.error(f\"County legislation scrape failed: {e}\", exc_info=True)\n",
        "\n",
        "def scrape_county_law_content(session: requests.Session, pdf_handler: PDFHandler, law_url: str, log: logging.Logger) -> Optional[str]:\n",
        "    \"\"\"Scrape content of an individual county law with PDF support\"\"\"\n",
        "\n",
        "    # Check if it's a PDF URL\n",
        "    if pdf_handler.is_pdf_url(law_url):\n",
        "        log.info(f\"  Detected PDF document: {law_url}\")\n",
        "        pdf_content = pdf_handler.download_pdf(session, law_url)\n",
        "        if pdf_content:\n",
        "            return pdf_handler.extract_text_from_pdf(law_url, pdf_content)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    # Handle HTML content\n",
        "    try:\n",
        "        resp = session.get(law_url, timeout=30)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "\n",
        "        # Check if the page contains a PDF link\n",
        "        pdf_links = soup.select('a[href$=\".pdf\"], a[href*=\"/pdf/\"]')\n",
        "        for pdf_link in pdf_links:\n",
        "            pdf_url = urljoin(law_url, pdf_link.get('href'))\n",
        "            log.info(f\"  Found embedded PDF link: {pdf_url}\")\n",
        "            pdf_content = pdf_handler.download_pdf(session, pdf_url)\n",
        "            if pdf_content:\n",
        "                return pdf_handler.extract_text_from_pdf(pdf_url, pdf_content)\n",
        "\n",
        "        # Try multiple content selectors for HTML laws\n",
        "        content_selectors = [\n",
        "            \"div.act-content\",\n",
        "            \"div.fr-view\",\n",
        "            \"div.content\",\n",
        "            \"article\",\n",
        "            \"main\",\n",
        "            \".law-content\",\n",
        "            \".document-content\"\n",
        "        ]\n",
        "\n",
        "        content = None\n",
        "        for selector in content_selectors:\n",
        "            content = soup.select_one(selector)\n",
        "            if content:\n",
        "                break\n",
        "\n",
        "        if not content:\n",
        "            # Fallback: get body content\n",
        "            content = soup.find('main') or soup.find('article') or soup.find('body')\n",
        "\n",
        "        if content:\n",
        "            # Clean up content\n",
        "            for element in content.select(\"script, style, nav, header, footer, .nav, .header, .footer, .tools\"):\n",
        "                element.decompose()\n",
        "\n",
        "            # Extract text\n",
        "            text = content.get_text(separator=\"\\n\", strip=True)\n",
        "            text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "            return text if len(text.split()) > 50 else None\n",
        "\n",
        "    except Exception as e:\n",
        "        log.warning(f\"Failed to scrape county law content from {law_url}: {e}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                                 CASE LAW SCRAPER                            #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "class KenyaLawScraper:\n",
        "    def __init__(self, cfg: Config, log: logging.Logger):\n",
        "        self.cfg = cfg\n",
        "        self.log = log\n",
        "        self.keywords = {k.lower() for k in cfg.KEYWORDS}\n",
        "        self.seen_case_ids = set()\n",
        "        self.driver = None\n",
        "        self.session = self._create_session()\n",
        "\n",
        "    def _create_session(self):\n",
        "        s = requests.Session()\n",
        "        retry = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
        "        adapter = HTTPAdapter(max_retries=retry)\n",
        "        s.mount(\"http://\", adapter)\n",
        "        s.mount(\"https://\", adapter)\n",
        "        s.headers.update({\n",
        "            \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\",\n",
        "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n",
        "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "            \"Accept-Encoding\": \"gzip, deflate, br\"\n",
        "        })\n",
        "        return s\n",
        "\n",
        "    def _setup_driver(self):\n",
        "        options = Options()\n",
        "        if self.cfg.CHROME_HEADLESS:\n",
        "            options.add_argument(\"--headless\")\n",
        "        options.add_argument(\"--no-sandbox\")\n",
        "        options.add_argument(\"--disable-dev-shm-usage\")\n",
        "        options.add_argument(\"--window-size=1920,1080\")\n",
        "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "        options.add_experimental_option('useAutomationExtension', False)\n",
        "\n",
        "        try:\n",
        "            if WEBDRIVER_MANAGER_SUPPORT:\n",
        "                service = Service(ChromeDriverManager().install())\n",
        "            else:\n",
        "                # Fallback if webdriver_manager is not installed\n",
        "                driver_path = self.cfg.LOCAL_CHROMEDRIVER_PATH or \"/usr/bin/chromedriver\" # Assuming default path\n",
        "                service = Service(driver_path)\n",
        "\n",
        "            driver = webdriver.Chrome(service=service, options=options)\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Failed to setup Chrome driver: {e}\")\n",
        "            return None\n",
        "\n",
        "        driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
        "        driver.execute_cdp_cmd('Network.setUserAgentOverride', {\n",
        "            \"userAgent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\"\n",
        "        })\n",
        "        return driver\n",
        "\n",
        "    def _case_id_from_url(self, url: str) -> Optional[str]:\n",
        "        # Extract case ID from new URL format: /akn/ke/judgment/kehc/2025/15858/eng@2025-11-07\n",
        "        patterns = [\n",
        "            r\"/akn/ke/judgment/[^/]+/(\\d+)/(?:eng@|\\d+)\",\n",
        "            r\"/judgments/view/(\\d+)\",\n",
        "            r\"/caselaw/cases/view/(\\d+)\"\n",
        "        ]\n",
        "        for pattern in patterns:\n",
        "            m = re.search(pattern, url)\n",
        "            if m:\n",
        "                return m.group(1)\n",
        "        return None\n",
        "\n",
        "    def fetch_case_urls_selenium(self) -> List[str]:\n",
        "        self.log.info(\"Collecting case URLs from new Kenya Law site...\")\n",
        "        urls = set()\n",
        "\n",
        "        try:\n",
        "            self.driver = self._setup_driver()\n",
        "            if not self.driver:\n",
        "                self.log.error(\"Failed to setup Selenium driver. Cannot collect case URLs.\")\n",
        "                return []\n",
        "\n",
        "            self.driver.get(self.cfg.JUDGMENTS_URL)\n",
        "            time.sleep(5)\n",
        "\n",
        "            # Wait for page to load\n",
        "            WebDriverWait(self.driver, 10).until(\n",
        "                EC.presence_of_element_located((By.TAG_NAME, \"table\"))\n",
        "            )\n",
        "\n",
        "            # Get initial page URLs\n",
        "            soup = BeautifulSoup(self.driver.page_source, \"lxml\")\n",
        "            self._extract_urls_from_page(soup, urls)\n",
        "\n",
        "            # Handle pagination\n",
        "            page_count = 0\n",
        "            while page_count < (self.cfg.MAX_PAGES or 10):  # Limit pages if specified\n",
        "                try:\n",
        "                    # Look for next button\n",
        "                    next_buttons = self.driver.find_elements(By.XPATH,\n",
        "                        \"//a[contains(text(), 'Next') or contains(@class, 'next') or contains(@aria-label, 'next')]\")\n",
        "\n",
        "                    if not next_buttons:\n",
        "                        break\n",
        "\n",
        "                    next_btn = next_buttons[0]\n",
        "                    if \"disabled\" in next_btn.get_attribute(\"class\") or not next_btn.is_enabled():\n",
        "                        break\n",
        "\n",
        "                    # Click next page\n",
        "                    self.driver.execute_script(\"arguments[0].click();\", next_btn)\n",
        "                    time.sleep(4)\n",
        "\n",
        "                    # Wait for new content to load\n",
        "                    WebDriverWait(self.driver, 10).until(\n",
        "                        EC.presence_of_element_located((By.TAG_NAME, \"table\"))\n",
        "                    )\n",
        "\n",
        "                    # Extract URLs from new page\n",
        "                    soup = BeautifulSoup(self.driver.page_source, \"lxml\")\n",
        "                    self._extract_urls_from_page(soup, urls)\n",
        "\n",
        "                    page_count += 1\n",
        "                    self.log.info(f\"Processed page {page_count}, total URLs: {len(urls)}\")\n",
        "\n",
        "                except (TimeoutException, NoSuchElementException) as e:\n",
        "                    self.log.info(\"No more pages or pagination failed\")\n",
        "                    break\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Error collecting URLs: {e}\")\n",
        "        finally:\n",
        "            if self.driver:\n",
        "                self.driver.quit()\n",
        "\n",
        "        final = list(urls)\n",
        "        if self.cfg.MAX_CASES:\n",
        "            final = final[:self.cfg.MAX_CASES]\n",
        "        self.log.info(f\"Collected {len(final)} case URLs\")\n",
        "        return final\n",
        "\n",
        "    def _extract_urls_from_page(self, soup: BeautifulSoup, urls: set) -> None:\n",
        "        \"\"\"Extract case URLs from a page\"\"\"\n",
        "        # Look for case links in tables\n",
        "        table_links = soup.find_all(\"a\", href=re.compile(r\"/akn/ke/judgment/\"))\n",
        "        for link in table_links:\n",
        "            href = link.get(\"href\")\n",
        "            if href:\n",
        "                full_url = urljoin(self.cfg.NEW_BASE_URL, href)\n",
        "                urls.add(full_url)\n",
        "\n",
        "        # Also check for any judgment links\n",
        "        judgment_links = soup.select('a[href*=\"/judgment/\"]')\n",
        "        for link in judgment_links:\n",
        "            href = link.get(\"href\")\n",
        "            if href and \"/akn/ke/judgment/\" in href:\n",
        "                full_url = urljoin(self.cfg.NEW_BASE_URL, href)\n",
        "                urls.add(full_url)\n",
        "\n",
        "    def scrape_one_case(self, url: str) -> Optional[Dict]:\n",
        "        case_id = self._case_id_from_url(url)\n",
        "        if not case_id or case_id in self.seen_case_ids:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            self.log.info(f\"Scraping case: {url}\")\n",
        "            resp = self.session.get(url, timeout=self.cfg.REQUEST_TIMEOUT)\n",
        "            resp.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "\n",
        "            # Extract case title\n",
        "            title_elem = soup.find(\"h1\") or soup.find(\"title\")\n",
        "            case_name = title_elem.get_text(strip=True) if title_elem else \"Unknown Case\"\n",
        "\n",
        "            # Extract case content - try multiple selectors for new site\n",
        "            content_selectors = [\n",
        "                \"div.fr-view\",  # Rich text content\n",
        "                \"div.content\",\n",
        "                \"article\",\n",
        "                \"main\",\n",
        "                \".judgment-content\",\n",
        "                \".case-content\"\n",
        "            ]\n",
        "\n",
        "            content = None\n",
        "            for selector in content_selectors:\n",
        "                content = soup.select_one(selector)\n",
        "                if content:\n",
        "                    break\n",
        "\n",
        "            if not content:\n",
        "                # Fallback: get main content area\n",
        "                content = soup.find(\"main\") or soup.find(\"article\") or soup.find(\"div\", class_=re.compile(\"content\"))\n",
        "\n",
        "            if not content:\n",
        "                self.log.warning(f\"No content found for case {case_id}\")\n",
        "                return None\n",
        "\n",
        "            # Clean up content\n",
        "            for element in content.select(\"script, style, nav, header, footer, .nav, .header, .footer, .tools, .act-tools\"):\n",
        "                element.decompose()\n",
        "\n",
        "            # Extract text\n",
        "            text = content.get_text(separator=\"\\n\", strip=True)\n",
        "            text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "            if len(text.split()) < 100:\n",
        "                self.log.warning(f\"Case {case_id} has insufficient text: {len(text.split())} words\")\n",
        "                return None\n",
        "\n",
        "            # Extract metadata\n",
        "            metadata = self._extract_case_metadata(soup)\n",
        "\n",
        "            data = {\n",
        "                \"case_id\": case_id,\n",
        "                \"case_name\": case_name,\n",
        "                \"url\": url,\n",
        "                \"text\": text,\n",
        "                \"text_length_words\": len(text.split()),\n",
        "                \"scraped_at\": datetime.now().isoformat(),\n",
        "                \"metadata\": metadata\n",
        "            }\n",
        "\n",
        "            self.seen_case_ids.add(case_id)\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Case failed {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _extract_case_metadata(self, soup: BeautifulSoup) -> Dict[str, str]:\n",
        "        \"\"\"Extract case metadata from the page\"\"\"\n",
        "        metadata = {}\n",
        "\n",
        "        try:\n",
        "            # Look for common metadata patterns\n",
        "            meta_selectors = {\n",
        "                \"court\": [\"span.court\", \"div.court\", \"td.cell-court\"],\n",
        "                \"date\": [\"span.date\", \"div.date\", \"td.cell-date\", \"time\"],\n",
        "                \"case_number\": [\"span.case-number\", \"div.case-number\", \"td.cell-case-number\"],\n",
        "                \"judges\": [\"span.judges\", \"div.judges\", \"p.judges\"],\n",
        "                \"citation\": [\"span.citation\", \"div.citation\", \"td.cell-citation\"]\n",
        "            }\n",
        "\n",
        "            for key, selectors in meta_selectors.items():\n",
        "                for selector in selectors:\n",
        "                    element = soup.select_one(selector)\n",
        "                    if element:\n",
        "                        metadata[key] = element.get_text(strip=True)\n",
        "                        break\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log.debug(f\"Metadata extraction failed: {e}\")\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    def run_case_scrape(self, urls: List[str], handler: DataHandler) -> int:\n",
        "        self.log.info(f\"Scraping {len(urls)} cases...\")\n",
        "        saved = 0\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=self.cfg.MAX_SCRAPE_WORKERS) as executor:\n",
        "            future_to_url = {executor.submit(self.scrape_one_case, url): url for url in urls}\n",
        "\n",
        "            for future in as_completed(future_to_url):\n",
        "                url = future_to_url[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    if result and handler.save_case(result):\n",
        "                        saved += 1\n",
        "                except Exception as e:\n",
        "                    self.log.error(f\"Case scraping failed for {url}: {e}\")\n",
        "\n",
        "        return saved\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                                   MAIN                                      #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "def main() -> None:\n",
        "    cfg = Config()\n",
        "    log = setup_logging(cfg.LOG_FILE)\n",
        "    log.info(\"=== KenyaLaw Scraper v6.0 FULL (Acts + Subsidiary + Cases + Counties with PDF support) ===\")\n",
        "\n",
        "    # Check PDF support\n",
        "    if cfg.ENABLE_PDF_EXTRACTION:\n",
        "        if not PDF_SUPPORT and not PDFPLUMBER_SUPPORT:\n",
        "            log.warning(\"PDF extraction enabled but no PDF libraries found. Install: pip install pypdf2 pdfplumber\")\n",
        "        else:\n",
        "            log.info(f\"PDF extraction enabled: PyPDF2={PDF_SUPPORT}, pdfplumber={PDFPLUMBER_SUPPORT}\")\n",
        "\n",
        "    # Scrape static content\n",
        "    scrape_constitution(cfg, log)\n",
        "    scrape_acts_of_kenya(cfg, log)\n",
        "    scrape_county_legislation(cfg, log)  # Now with PDF support\n",
        "\n",
        "    # Scrape case law\n",
        "    handler = DataHandler(cfg, log)\n",
        "    scraper = KenyaLawScraper(cfg, log)\n",
        "    scraper.seen_case_ids = handler.load_existing_case_ids()\n",
        "\n",
        "    urls = scraper.fetch_case_urls_selenium()\n",
        "    new_urls = [u for u in urls if scraper._case_id_from_url(u) not in scraper.seen_case_ids]\n",
        "\n",
        "    if new_urls:\n",
        "        log.info(f\"Found {len(new_urls)} new cases to scrape\")\n",
        "        saved = scraper.run_case_scrape(new_urls, handler)\n",
        "        log.info(f\"Completed: {saved} new cases saved.\")\n",
        "    else:\n",
        "        log.info(\"No new cases found.\")\n",
        "\n",
        "    log.info(\"=== ALL DONE ===\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "cfg = Config() # Define cfg in the global scope for subsequent cells\n"
      ],
      "id": "d8QasnxKPjVD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cb884161"
      },
      "source": [
        "**Reasoning**:\n",
        "Install the missing `selenium` library to resolve the ModuleNotFoundError.\n",
        "\n"
      ],
      "id": "cb884161"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnEouGIWPmZs"
      },
      "source": [
        "!pip install -q selenium"
      ],
      "id": "QnEouGIWPmZs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6447f7ae"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that `selenium` is installed, re-run the code cell that defines the `Config` class and creates the `cfg` object.\n",
        "\n"
      ],
      "id": "6447f7ae"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b1937e2"
      },
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime\n",
        "from typing import List, Set, Dict, Optional\n",
        "from urllib.parse import urljoin\n",
        "import io\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "# Try to import webdriver_manager, but make it optional\n",
        "try:\n",
        "    from webdriver_manager.chrome import ChromeDriverManager\n",
        "    WEBDRIVER_MANAGER_SUPPORT = True\n",
        "except ImportError:\n",
        "    WEBDRIVER_MANAGER_SUPPORT = False\n",
        "    print(\"Warning: webdriver_manager not installed. Chrome driver will not be automatically managed.\")\n",
        "\n",
        "\n",
        "# Try to import PDF libraries, but make them optional\n",
        "try:\n",
        "    import PyPDF2\n",
        "    PDF_SUPPORT = True\n",
        "except ImportError:\n",
        "    PDF_SUPPORT = False\n",
        "    print(\"Warning: PyPDF2 not installed. PDF content extraction will be limited.\")\n",
        "\n",
        "try:\n",
        "    import pdfplumber\n",
        "    PDFPLUMBER_SUPPORT = True\n",
        "except ImportError:\n",
        "    PDFPLUMBER_SUPPORT = False\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                               CONFIGURATION                                 #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    BASE_PROJECT_DIR: str = os.path.join(os.path.expanduser(\"~\"), \"projects\", \"kenya_law\")\n",
        "\n",
        "    LOG_DIR: str = field(init=False)\n",
        "    DATA_DIR: str = field(init=False)\n",
        "    LOG_FILE: str = field(init=False)\n",
        "    DATA_FILE: str = field(init=False)\n",
        "    DEBUG_DIR: str = field(init=False)\n",
        "    CONSTITUTION_FILE: str = field(init=False)\n",
        "    ACTS_FILE: str = field(init=False)\n",
        "    SUBSIDIARY_FILE: str = field(init=False)\n",
        "    COUNTIES_FILE: str = field(init=False)\n",
        "    PDF_DIR: str = field(init=False)  # NEW: Directory to store downloaded PDFs\n",
        "\n",
        "    MAX_CASES: int = None\n",
        "    MAX_PAGES: int = None\n",
        "    MAX_COUNTY_LAWS: int = 50\n",
        "    REQUEST_TIMEOUT: int = 30\n",
        "    SELENIUM_TIMEOUT: int = 45\n",
        "    YEAR_START: int = 2020\n",
        "\n",
        "    BASE_URL: str = \"https://kenyalaw.org\"\n",
        "    NEW_BASE_URL: str = \"https://new.kenyalaw.org\"\n",
        "    SEARCH_URL: str = \"https://new.kenyalaw.org/search/\"\n",
        "    JUDGMENTS_URL: str = \"https://new.kenyalaw.org/judgments/\"\n",
        "    COUNTIES_URL: str = \"https://new.kenyalaw.org/legislation/counties\"\n",
        "    ACTS_TOC_URL: str = \"https://new.kenyalaw.org/legislation/\"\n",
        "\n",
        "    LOCAL_CHROMEDRIVER_PATH: Optional[str] = None\n",
        "    CHROME_HEADLESS: bool = True\n",
        "\n",
        "    KEYWORDS: List[str] = field(default_factory=lambda: [\n",
        "        \"constitution\", \"human rights\", \"land\", \"election\", \"criminal\", \"civil\",\n",
        "        \"jurisdiction\", \"appeal\", \"judicial review\", \"injunction\", \"contract\"\n",
        "    ])\n",
        "\n",
        "    MAX_SCRAPE_WORKERS: int = 10\n",
        "    ENABLE_PDF_EXTRACTION: bool = True  # NEW: Control PDF processing\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        self.LOG_DIR = os.path.join(self.BASE_PROJECT_DIR, \"logs\")\n",
        "        self.DATA_DIR = os.path.join(self.BASE_PROJECT_DIR, \"data\")\n",
        "        self.DEBUG_DIR = os.path.join(self.BASE_PROJECT_DIR, \"debug\")\n",
        "        self.PDF_DIR = os.path.join(self.DATA_DIR, \"pdfs\")  # NEW\n",
        "        self.LOG_FILE = os.path.join(self.LOG_DIR, f\"kenyalaw_scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
        "        self.DATA_FILE = os.path.join(self.DATA_DIR, \"kenya_law_training_data.jsonl\")\n",
        "        self.CONSTITUTION_FILE = os.path.join(self.DATA_DIR, \"constitution.json\")\n",
        "        self.ACTS_FILE = os.path.join(self.DATA_DIR, \"acts_of_kenya.json\")\n",
        "        self.SUBSIDIARY_FILE = os.path.join(self.DATA_DIR, \"subsidiary_legislation.json\")\n",
        "        self.COUNTIES_FILE = os.path.join(self.DATA_DIR, \"county_legislation.json\")\n",
        "\n",
        "        for d in [self.LOG_DIR, self.DATA_DIR, self.DEBUG_DIR, self.PDF_DIR]:\n",
        "            os.makedirs(d, exist_ok=True)\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                                 LOGGING                                    #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "def setup_logging(log_file: str) -> logging.Logger:\n",
        "    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
        "    logger = logging.getLogger(\"KenyaLaw-Scraper-v6.0-FULL\")\n",
        "    logger.setLevel(logging.INFO)\n",
        "    logger.handlers.clear()\n",
        "    fh = logging.FileHandler(log_file, encoding=\"utf-8\")\n",
        "    ch = logging.StreamHandler()\n",
        "    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
        "    fh.setFormatter(formatter)\n",
        "    ch.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "    logger.addHandler(ch)\n",
        "    return logger\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                               DATA HANDLER                                 #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "class DataHandler:\n",
        "    def __init__(self, cfg: Config, log: logging.Logger):\n",
        "        self.cfg = cfg\n",
        "        self.log = log\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def load_existing_case_ids(self) -> Set[str]:\n",
        "        if not os.path.exists(self.cfg.DATA_FILE):\n",
        "            return set()\n",
        "        ids = set()\n",
        "        try:\n",
        "            with open(self.cfg.DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "                for line_num, line in enumerate(f, 1):\n",
        "                    line = line.strip()\n",
        "                    if not line: continue\n",
        "                    try:\n",
        "                        data = json.loads(line)\n",
        "                        if \"case_id\" in data:\n",
        "                            ids.add(data[\"case_id\"])\n",
        "                    except json.JSONDecodeError:\n",
        "                        self.log.warning(f\"Bad JSON at line {line_num}\")\n",
        "            self.log.info(f\"Loaded {len(ids)} existing case IDs\")\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Failed to load IDs: {e}\")\n",
        "        return ids\n",
        "\n",
        "    def save_case(self, case: Dict) -> bool:\n",
        "        try:\n",
        "            json_line = json.dumps(case, ensure_ascii=False)\n",
        "            with self.lock:\n",
        "                with open(self.cfg.DATA_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "                    f.write(json_line + \"\\n\")\n",
        "            self.log.info(f\"Saved case {case['case_id']} ({len(case['text'].split())} words)\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Save failed: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                              PDF HANDLING                                  #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "class PDFHandler:\n",
        "    def __init__(self, cfg: Config, log: logging.Logger):\n",
        "        self.cfg = cfg\n",
        "        self.log = log\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_url: str, pdf_content: bytes) -> Optional[str]:\n",
        "        \"\"\"Extract text from PDF content using multiple methods\"\"\"\n",
        "        if not self.cfg.ENABLE_PDF_EXTRACTION:\n",
        "            return \"PDF_CONTENT_AVAILABLE_BUT_EXTRACTION_DISABLED\"\n",
        "\n",
        "        # Save PDF file for reference\n",
        "        pdf_filename = self._save_pdf_file(pdf_url, pdf_content)\n",
        "\n",
        "        extracted_text = None\n",
        "\n",
        "        # Try pdfplumber first (better for scanned PDFs with OCR)\n",
        "        if PDFPLUMBER_SUPPORT:\n",
        "            extracted_text = self._extract_with_pdfplumber(pdf_content)\n",
        "\n",
        "        # Fallback to PyPDF2\n",
        "        if not extracted_text and PDF_SUPPORT:\n",
        "            extracted_text = self._extract_with_pypdf2(pdf_content)\n",
        "\n",
        "        # Final fallback\n",
        "        if not extracted_text:\n",
        "            extracted_text = self._extract_fallback(pdf_content, pdf_filename)\n",
        "\n",
        "        return extracted_text\n",
        "\n",
        "    def _save_pdf_file(self, pdf_url: str, pdf_content: bytes) -> str:\n",
        "        \"\"\"Save PDF file to disk for reference\"\"\"\n",
        "        try:\n",
        "            # Create filename from URL\n",
        "            filename = re.sub(r'[^a-zA-Z0-9]', '_', pdf_url) + '.pdf'\n",
        "            filepath = os.path.join(self.cfg.PDF_DIR, filename)\n",
        "\n",
        "            with open(filepath, 'wb') as f:\n",
        "                f.write(pdf_content)\n",
        "\n",
        "            return filename\n",
        "        except Exception as e:\n",
        "            self.log.warning(f\"Failed to save PDF file: {e}\")\n",
        "            return \"unknown.pdf\"\n",
        "\n",
        "    def _extract_with_pdfplumber(self, pdf_content: bytes) -> Optional[str]:\n",
        "        \"\"\"Extract text using pdfplumber (better for scanned PDFs)\"\"\"\n",
        "        try:\n",
        "            text_parts = []\n",
        "            with pdfplumber.open(io.BytesIO(pdf_content)) as pdf:\n",
        "                for page in pdf.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        text_parts.append(page_text.strip())\n",
        "\n",
        "            if text_parts:\n",
        "                full_text = '\\n'.join(text_parts)\n",
        "                if len(full_text.split()) > 10:  # Ensure we have substantial text\n",
        "                    return full_text\n",
        "        except Exception as e:\n",
        "            self.log.debug(f\"pdfplumber extraction failed: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _extract_with_pypdf2(self, pdf_content: bytes) -> Optional[str]:\n",
        "        \"\"\"Extract text using PyPDF2\"\"\"\n",
        "        try:\n",
        "            text_parts = []\n",
        "            pdf_file = io.BytesIO(pdf_content)\n",
        "            reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "            for page in reader.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text_parts.append(page_text.strip())\n",
        "\n",
        "            if text_parts:\n",
        "                full_text = '\\n'.join(text_parts)\n",
        "                if len(full_text.split()) > 10:\n",
        "                    return full_text\n",
        "        except Exception as e:\n",
        "            self.log.debug(f\"PyPDF2 extraction failed: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _extract_fallback(self, pdf_content: bytes, pdf_filename: str) -> str:\n",
        "        \"\"\"Fallback method when PDF text extraction fails\"\"\"\n",
        "        self.log.warning(f\"PDF text extraction failed for {pdf_filename}. Content saved to disk.\")\n",
        "        return f\"PDF_CONTENT_UNABLE_TO_EXTRACT_TEXT_SAVED_AS_{pdf_filename}\"\n",
        "\n",
        "    def is_pdf_url(self, url: str) -> bool:\n",
        "        \"\"\"Check if URL points to a PDF document\"\"\"\n",
        "        return url.lower().endswith('.pdf') or '/pdf/' in url.lower()\n",
        "\n",
        "    def download_pdf(self, session: requests.Session, pdf_url: str) -> Optional[bytes]:\n",
        "        \"\"\"Download PDF content\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\",\n",
        "                \"Accept\": \"application/pdf, */*\",\n",
        "                \"Referer\": \"https://new.kenyalaw.org/\"\n",
        "            }\n",
        "\n",
        "            response = session.get(pdf_url, timeout=30, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            content_type = response.headers.get('content-type', '').lower()\n",
        "            if 'pdf' in content_type or response.content[:4] == b'%PDF':\n",
        "                return response.content\n",
        "            else:\n",
        "                self.log.warning(f\"URL {pdf_url} doesn't contain PDF data\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Failed to download PDF from {pdf_url}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                      STATIC CONTENT SCRAPERS (Constitution & Acts)          #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "def save_constitution_data(cfg: Config, log: logging.Logger, data: Dict[str, str]) -> None:\n",
        "    try:\n",
        "        with open(cfg.CONSTITUTION_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "        log.info(f\"Constitution saved → {cfg.CONSTITUTION_FILE}\")\n",
        "    except Exception as e:\n",
        "        log.error(f\"Failed to save constitution: {e}\")\n",
        "\n",
        "def scrape_constitution(cfg: Config, log: logging.Logger) -> None:\n",
        "    log.info(\"Scraping Constitution of Kenya (2010)...\")\n",
        "    if os.path.exists(cfg.CONSTITUTION_FILE):\n",
        "        log.info(f\"Constitution already exists → {cfg.CONSTITUTION_FILE}. Skipping.\")\n",
        "        return\n",
        "\n",
        "    # Try multiple potential sources\n",
        "    SOURCES = [\n",
        "        \"https://new.kenyalaw.org/akn/ke/act/2010/constitution/eng@2010-09-03\",\n",
        "        \"https://new.kenyalaw.org/akn/ke/act/2010/constitution\",\n",
        "        \"https://kenyalaw.org/kl/index.php?id=398\"\n",
        "    ]\n",
        "\n",
        "    session = requests.Session()\n",
        "    session.headers.update({\n",
        "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\",\n",
        "        \"Accept\": \"application/xml, text/xml, text/html, */*\",\n",
        "        \"Referer\": \"https://new.kenyalaw.org/\",\n",
        "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "    })\n",
        "\n",
        "    for source_url in SOURCES:\n",
        "        try:\n",
        "            log.info(f\"Trying source: {source_url}\")\n",
        "            resp = session.get(source_url, timeout=60)\n",
        "            resp.raise_for_status()\n",
        "\n",
        "            content_type = resp.headers.get('content-type', '').lower()\n",
        "\n",
        "            if 'xml' in content_type:\n",
        "                # Parse as XML\n",
        "                soup = BeautifulSoup(resp.content, \"xml\")\n",
        "                body = soup.find(\"body\")\n",
        "                if not body:\n",
        "                    log.warning(f\"No <body> found in XML from {source_url}\")\n",
        "                    continue\n",
        "\n",
        "                data = {}\n",
        "                current_title = \"Preamble\"\n",
        "                current_lines = []\n",
        "\n",
        "                # Extract all relevant elements\n",
        "                for elem in body.find_all(['heading', 'num', 'p', 'chapter', 'part', 'section', 'article']):\n",
        "                    if elem.name in ['heading', 'chapter', 'part']:\n",
        "                        # Save previous section\n",
        "                        if current_lines:\n",
        "                            data[current_title] = \"\\n\".join(current_lines).strip()\n",
        "                            current_lines = []\n",
        "                        # Start new section\n",
        "                        current_title = elem.get_text(strip=True)\n",
        "                        if not current_title:\n",
        "                            current_title = \"Untitled Section\"\n",
        "\n",
        "                    elif elem.name == 'p' and elem.get_text(strip=True):\n",
        "                        text = elem.get_text(strip=True)\n",
        "                        if len(text) > 10:  # Filter out very short paragraphs\n",
        "                            current_lines.append(text)\n",
        "\n",
        "                # Save the last section\n",
        "                if current_lines:\n",
        "                    data[current_title] = \"\\n\".join(current_lines).strip()\n",
        "\n",
        "            else:\n",
        "                # Parse as HTML\n",
        "                soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "\n",
        "                # Try different content selectors\n",
        "                content_selectors = [\n",
        "                    \"div.act-content\",\n",
        "                    \"div.content\",\n",
        "                    \"article\",\n",
        "                    \"main\",\n",
        "                    \"div.container\",\n",
        "                    \"#content\"\n",
        "                ]\n",
        "\n",
        "                content = None\n",
        "                for selector in content_selectors:\n",
        "                    content = soup.select_one(selector)\n",
        "                    if content:\n",
        "                        break\n",
        "\n",
        "                if not content:\n",
        "                    log.warning(f\"No content found with selectors in {source_url}\")\n",
        "                    continue\n",
        "\n",
        "                # Clean up the content\n",
        "                for element in content.select(\"script, style, nav, header, footer, .nav, .header, .footer\"):\n",
        "                    element.decompose()\n",
        "\n",
        "                # Extract text and structure\n",
        "                data = {}\n",
        "                current_section = \"Constitution of Kenya\"\n",
        "                sections = [current_section]\n",
        "                text_lines = []\n",
        "\n",
        "                for element in content.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'div']):\n",
        "                    text = element.get_text(strip=True)\n",
        "                    if not text:\n",
        "                        continue\n",
        "\n",
        "                    # Detect section headers\n",
        "                    if element.name in ['h1', 'h2', 'h3', 'h4']:\n",
        "                        if text_lines:  # Save previous section\n",
        "                            data[current_section] = \"\\n\".join(text_lines).strip()\n",
        "                            text_lines = []\n",
        "                        current_section = text\n",
        "                        sections.append(current_section)\n",
        "                    elif len(text) > 20:  # Substantial content\n",
        "                        text_lines.append(text)\n",
        "\n",
        "                # Save the last section\n",
        "                if text_lines:\n",
        "                    data[current_section] = \"\\n\".join(text_lines).strip()\n",
        "\n",
        "            # Filter out empty or very short sections\n",
        "            filtered_data = {}\n",
        "            for title, content in data.items():\n",
        "                if content and len(content.split()) >= 10:\n",
        "                    clean_content = re.sub(r'\\s+', ' ', content).strip()\n",
        "                    filtered_data[title] = clean_content\n",
        "\n",
        "            # If we have substantial content, save it\n",
        "            if filtered_data and sum(len(c.split()) for c in filtered_data.values()) > 500:\n",
        "                save_constitution_data(cfg, log, filtered_data)\n",
        "                total_words = sum(len(v.split()) for v in filtered_data.values())\n",
        "                log.info(f\"SUCCESS: Constitution scraped → {len(filtered_data)} sections, {total_words:,} words\")\n",
        "                return\n",
        "            else:\n",
        "                log.warning(f\"Insufficient content from {source_url}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            log.warning(f\"Source {source_url} failed: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Fallback: manual structure if all sources fail\n",
        "    log.warning(\"All automated sources failed. Creating placeholder structure.\")\n",
        "    fallback_data = {\n",
        "        \"Preamble\": \"We, the people of Kenya—ACKNOWLEDGING the supremacy of the Almighty God of all creation...\",\n",
        "        \"Chapter One - Sovereignty of the People\": \"1. (1) All sovereign power belongs to the people of Kenya...\",\n",
        "        \"Chapter Two - The Republic\": \"4. (1) Kenya is a sovereign Republic. (2) The Republic of Kenya shall be a multi-party democratic state...\",\n",
        "        \"Note\": \"This is a placeholder. The actual constitution text could not be scraped automatically. Consider manual entry.\"\n",
        "    }\n",
        "    save_constitution_data(cfg, log, fallback_data)\n",
        "    log.info(\"Created fallback constitution structure\")\n",
        "\n",
        "def save_acts_data(cfg: Config, log: logging.Logger, acts: Dict[str, str], subs: Dict[str, str]) -> None:\n",
        "    with open(cfg.ACTS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(acts, f, ensure_ascii=False, indent=2)\n",
        "    log.info(f\"Saved {len(acts)} Acts → {cfg.ACTS_FILE}\")\n",
        "\n",
        "    with open(cfg.SUBSIDIARY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(subs, f, ensure_ascii=False, indent=2)\n",
        "    log.info(f\"Saved {len(subs)} Subsidiary Laws → {cfg.SUBSIDIARY_FILE}\")\n",
        "\n",
        "def save_counties_data(cfg: Config, log: logging.Logger, counties_data: Dict[str, Dict]) -> None:\n",
        "    try:\n",
        "        with open(cfg.COUNTIES_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(counties_data, f, ensure_ascii=False, indent=2)\n",
        "        log.info(f\"County legislation saved → {cfg.COUNTIES_FILE}\")\n",
        "    except Exception as e:\n",
        "        log.error(f\"Failed to save county legislation: {e}\")\n",
        "\n",
        "\n",
        "def scrape_county_legislation(cfg: Config, log: logging.Logger) -> None:\n",
        "    \"\"\"Scrape county legislation from all 47 counties\"\"\"\n",
        "    log.info(\"Scraping County Legislation from all 47 counties...\")\n",
        "    if os.path.exists(cfg.COUNTIES_FILE):\n",
        "        log.info(f\"County legislation already exists → {cfg.COUNTIES_FILE}. Skipping.\")\n",
        "        return\n",
        "\n",
        "    session = requests.Session()\n",
        "    pdf_handler = PDFHandler(cfg, log)  # NEW: PDF handler\n",
        "\n",
        "    session.headers.update({\n",
        "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\",\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "        \"Referer\": cfg.NEW_BASE_URL\n",
        "    })\n",
        "\n",
        "    counties_data = {}\n",
        "    total_county_laws = 0\n",
        "\n",
        "    try:\n",
        "        # Get the main counties page\n",
        "        log.info(f\"Accessing counties page: {cfg.COUNTIES_URL}\")\n",
        "        resp = session.get(cfg.COUNTIES_URL, timeout=cfg.REQUEST_TIMEOUT)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "\n",
        "        # Extract all county links\n",
        "        county_links = []\n",
        "        flow_columns = soup.select('.flow-columns-group')\n",
        "\n",
        "        for column in flow_columns:\n",
        "            links = column.select('a[href^=\"/legislation/ke-\"]')\n",
        "            for link in links:\n",
        "                county_name = link.get_text(strip=True)\n",
        "                county_url = urljoin(cfg.NEW_BASE_URL, link.get('href'))\n",
        "                county_links.append((county_name, county_url))\n",
        "\n",
        "        log.info(f\"Found {len(county_links)} counties to process\")\n",
        "\n",
        "        # Process each county\n",
        "        for county_name, county_url in county_links:\n",
        "            try:\n",
        "                log.info(f\"Processing county: {county_name}\")\n",
        "                time.sleep(1)  # Be respectful\n",
        "\n",
        "                # Get county page\n",
        "                county_resp = session.get(county_url, timeout=cfg.REQUEST_TIMEOUT)\n",
        "                county_resp.raise_for_status()\n",
        "                county_soup = BeautifulSoup(county_resp.text, \"lxml\")\n",
        "\n",
        "                # Extract county laws\n",
        "                county_laws = {}\n",
        "\n",
        "                # Look for laws in tables or lists\n",
        "                law_elements = county_soup.select('tr.has-children, .legislation-item, .law-item')\n",
        "\n",
        "                if not law_elements:\n",
        "                    # Try alternative selectors\n",
        "                    law_elements = county_soup.select('a[href*=\"/akn/ke/act/\"]')\n",
        "\n",
        "                laws_processed = 0\n",
        "\n",
        "                for law_element in law_elements:\n",
        "                    if laws_processed >= cfg.MAX_COUNTY_LAWS:\n",
        "                        break\n",
        "\n",
        "                    try:\n",
        "                        # Extract law link and title\n",
        "                        if law_element.name == 'tr':\n",
        "                            link_elem = law_element.select_one('td.cell-title a')\n",
        "                        else:\n",
        "                            link_elem = law_element\n",
        "\n",
        "                        if not link_elem or not link_elem.get('href'):\n",
        "                            continue\n",
        "\n",
        "                        law_title = link_elem.get_text(strip=True)\n",
        "                        law_url = urljoin(cfg.NEW_BASE_URL, link_elem.get('href'))\n",
        "\n",
        "                        # Skip if it's not a direct law link\n",
        "                        if '/akn/ke/act/' not in law_url:\n",
        "                            continue\n",
        "\n",
        "                        # Scrape the actual law content (now with PDF support)\n",
        "                        time.sleep(0.5)\n",
        "                        law_content = scrape_county_law_content(session, pdf_handler, law_url, log)\n",
        "\n",
        "                        if law_content and len(law_content.split()) > 50:  # Reduced threshold for PDFs\n",
        "                            county_laws[law_title] = {\n",
        "                                'url': law_url,\n",
        "                                'content': law_content,\n",
        "                                'word_count': len(law_content.split()),\n",
        "                                'content_type': 'pdf' if pdf_handler.is_pdf_url(law_url) else 'html'\n",
        "                            }\n",
        "                            laws_processed += 1\n",
        "                            total_county_laws += 1\n",
        "                            log.info(f\"    ✓ County law: {law_title} ({len(law_content.split())} words) [{county_laws[law_title]['content_type'].upper()}]\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        log.warning(f\"Failed to process county law in {county_name}: {e}\")\n",
        "                        continue\n",
        "\n",
        "                # Add county data\n",
        "                if county_laws:\n",
        "                    counties_data[county_name] = {\n",
        "                        'county_url': county_url,\n",
        "                        'laws': county_laws,\n",
        "                        'total_laws': len(county_laws),\n",
        "                        'scraped_at': datetime.now().isoformat()\n",
        "                    }\n",
        "                    log.info(f\"  ✓ {county_name}: {len(county_laws)} laws\")\n",
        "\n",
        "            except Exception as e:\n",
        "                log.error(f\"Failed to process county {county_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Save counties data\n",
        "        save_counties_data(cfg, log, counties_data)\n",
        "        log.info(f\"SUCCESS: County legislation scraped → {len(counties_data)} counties, {total_county_laws} total laws\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log.error(f\"County legislation scrape failed: {e}\", exc_info=True)\n",
        "\n",
        "def scrape_county_law_content(session: requests.Session, pdf_handler: PDFHandler, law_url: str, log: logging.Logger) -> Optional[str]:\n",
        "    \"\"\"Scrape content of an individual county law with PDF support\"\"\"\n",
        "\n",
        "    # Check if it's a PDF URL\n",
        "    if pdf_handler.is_pdf_url(law_url):\n",
        "        log.info(f\"  Detected PDF document: {law_url}\")\n",
        "        pdf_content = pdf_handler.download_pdf(session, law_url)\n",
        "        if pdf_content:\n",
        "            return pdf_handler.extract_text_from_pdf(law_url, pdf_content)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    # Handle HTML content\n",
        "    try:\n",
        "        resp = session.get(law_url, timeout=30)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "\n",
        "        # Check if the page contains a PDF link\n",
        "        pdf_links = soup.select('a[href$=\".pdf\"], a[href*=\"/pdf/\"]')\n",
        "        for pdf_link in pdf_links:\n",
        "            pdf_url = urljoin(law_url, pdf_link.get('href'))\n",
        "            log.info(f\"  Found embedded PDF link: {pdf_url}\")\n",
        "            pdf_content = pdf_handler.download_pdf(session, pdf_url)\n",
        "            if pdf_content:\n",
        "                return pdf_handler.extract_text_from_pdf(pdf_url, pdf_content)\n",
        "\n",
        "        # Try multiple content selectors for HTML laws\n",
        "        content_selectors = [\n",
        "            \"div.act-content\",\n",
        "            \"div.fr-view\",\n",
        "            \"div.content\",\n",
        "            \"article\",\n",
        "            \"main\",\n",
        "            \".law-content\",\n",
        "            \".document-content\"\n",
        "        ]\n",
        "\n",
        "        content = None\n",
        "        for selector in content_selectors:\n",
        "            content = soup.select_one(selector)\n",
        "            if content:\n",
        "                break\n",
        "\n",
        "        if not content:\n",
        "            # Fallback: get body content\n",
        "            content = soup.find('main') or soup.find('article') or soup.find('body')\n",
        "\n",
        "        if content:\n",
        "            # Clean up content\n",
        "            for element in content.select(\"script, style, nav, header, footer, .nav, .header, .footer, .tools\"):\n",
        "                element.decompose()\n",
        "\n",
        "            # Extract text\n",
        "            text = content.get_text(separator=\"\\n\", strip=True)\n",
        "            text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "            return text if len(text.split()) > 50 else None\n",
        "\n",
        "    except Exception as e:\n",
        "        log.warning(f\"Failed to scrape county law content from {law_url}: {e}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                                 CASE LAW SCRAPER                            #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "class KenyaLawScraper:\n",
        "    def __init__(self, cfg: Config, log: logging.Logger):\n",
        "        self.cfg = cfg\n",
        "        self.log = log\n",
        "        self.keywords = {k.lower() for k in cfg.KEYWORDS}\n",
        "        self.seen_case_ids = set()\n",
        "        self.driver = None\n",
        "        self.session = self._create_session()\n",
        "\n",
        "    def _create_session(self):\n",
        "        s = requests.Session()\n",
        "        retry = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
        "        adapter = HTTPAdapter(max_retries=retry)\n",
        "        s.mount(\"http://\", adapter)\n",
        "        s.mount(\"https://\", adapter)\n",
        "        s.headers.update({\n",
        "            \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\",\n",
        "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n",
        "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "            \"Accept-Encoding\": \"gzip, deflate, br\"\n",
        "        })\n",
        "        return s\n",
        "\n",
        "    def _setup_driver(self):\n",
        "        options = Options()\n",
        "        if self.cfg.CHROME_HEADLESS:\n",
        "            options.add_argument(\"--headless\")\n",
        "        options.add_argument(\"--no-sandbox\")\n",
        "        options.add_argument(\"--disable-dev-shm-usage\")\n",
        "        options.add_argument(\"--window-size=1920,1080\")\n",
        "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "        options.add_experimental_option('useAutomationExtension', False)\n",
        "\n",
        "        try:\n",
        "            if WEBDRIVER_MANAGER_SUPPORT:\n",
        "                service = Service(ChromeDriverManager().install())\n",
        "            else:\n",
        "                # Fallback if webdriver_manager is not installed\n",
        "                driver_path = self.cfg.LOCAL_CHROMEDRIVER_PATH or \"/usr/bin/chromedriver\" # Assuming default path\n",
        "                service = Service(driver_path)\n",
        "\n",
        "            driver = webdriver.Chrome(service=service, options=options)\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Failed to setup Chrome driver: {e}\")\n",
        "            return None\n",
        "\n",
        "        driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
        "        driver.execute_cdp_cmd('Network.setUserAgentOverride', {\n",
        "            \"userAgent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\"\n",
        "        })\n",
        "        return driver\n",
        "\n",
        "    def _case_id_from_url(self, url: str) -> Optional[str]:\n",
        "        # Extract case ID from new URL format: /akn/ke/judgment/kehc/2025/15858/eng@2025-11-07\n",
        "        patterns = [\n",
        "            r\"/akn/ke/judgment/[^/]+/(\\d+)/(?:eng@|\\d+)\",\n",
        "            r\"/judgments/view/(\\d+)\",\n",
        "            r\"/caselaw/cases/view/(\\d+)\"\n",
        "        ]\n",
        "        for pattern in patterns:\n",
        "            m = re.search(pattern, url)\n",
        "            if m:\n",
        "                return m.group(1)\n",
        "        return None\n",
        "\n",
        "    def fetch_case_urls_selenium(self) -> List[str]:\n",
        "        self.log.info(\"Collecting case URLs from new Kenya Law site...\")\n",
        "        urls = set()\n",
        "\n",
        "        try:\n",
        "            self.driver = self._setup_driver()\n",
        "            if not self.driver:\n",
        "                self.log.error(\"Failed to setup Selenium driver. Cannot collect case URLs.\")\n",
        "                return []\n",
        "\n",
        "            self.driver.get(self.cfg.JUDGMENTS_URL)\n",
        "            time.sleep(5)\n",
        "\n",
        "            # Wait for page to load\n",
        "            WebDriverWait(self.driver, 10).until(\n",
        "                EC.presence_of_element_located((By.TAG_NAME, \"table\"))\n",
        "            )\n",
        "\n",
        "            # Get initial page URLs\n",
        "            soup = BeautifulSoup(self.driver.page_source, \"lxml\")\n",
        "            self._extract_urls_from_page(soup, urls)\n",
        "\n",
        "            # Handle pagination\n",
        "            page_count = 0\n",
        "            while page_count < (self.cfg.MAX_PAGES or 10):  # Limit pages if specified\n",
        "                try:\n",
        "                    # Look for next button\n",
        "                    next_buttons = self.driver.find_elements(By.XPATH,\n",
        "                        \"//a[contains(text(), 'Next') or contains(@class, 'next') or contains(@aria-label, 'next')]\")\n",
        "\n",
        "                    if not next_buttons:\n",
        "                        break\n",
        "\n",
        "                    next_btn = next_buttons[0]\n",
        "                    if \"disabled\" in next_btn.get_attribute(\"class\") or not next_btn.is_enabled():\n",
        "                        break\n",
        "\n",
        "                    # Click next page\n",
        "                    self.driver.execute_script(\"arguments[0].click();\", next_btn)\n",
        "                    time.sleep(4)\n",
        "\n",
        "                    # Wait for new content to load\n",
        "                    WebDriverWait(self.driver, 10).until(\n",
        "                        EC.presence_of_element_located((By.TAG_NAME, \"table\"))\n",
        "                    )\n",
        "\n",
        "                    # Extract URLs from new page\n",
        "                    soup = BeautifulSoup(self.driver.page_source, \"lxml\")\n",
        "                    self._extract_urls_from_page(soup, urls)\n",
        "\n",
        "                    page_count += 1\n",
        "                    self.log.info(f\"Processed page {page_count}, total URLs: {len(urls)}\")\n",
        "\n",
        "                except (TimeoutException, NoSuchElementException) as e:\n",
        "                    self.log.info(\"No more pages or pagination failed\")\n",
        "                    break\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Error collecting URLs: {e}\")\n",
        "        finally:\n",
        "            if self.driver:\n",
        "                self.driver.quit()\n",
        "\n",
        "        final = list(urls)\n",
        "        if self.cfg.MAX_CASES:\n",
        "            final = final[:self.cfg.MAX_CASES]\n",
        "        self.log.info(f\"Collected {len(final)} case URLs\")\n",
        "        return final\n",
        "\n",
        "    def scrape_one_case(self, url: str) -> Optional[Dict]:\n",
        "        case_id = self._case_id_from_url(url)\n",
        "        if not case_id or case_id in self.seen_case_ids:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            self.log.info(f\"Scraping case: {url}\")\n",
        "            resp = self.session.get(url, timeout=self.cfg.REQUEST_TIMEOUT)\n",
        "            resp.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "\n",
        "            # Extract case title\n",
        "            title_elem = soup.find(\"h1\") or soup.find(\"title\")\n",
        "            case_name = title_elem.get_text(strip=True) if title_elem else \"Unknown Case\"\n",
        "\n",
        "            # Extract case content - try multiple selectors for new site\n",
        "            content_selectors = [\n",
        "                \"div.fr-view\",  # Rich text content\n",
        "                \"div.content\",\n",
        "                \"article\",\n",
        "                \"main\",\n",
        "                \".judgment-content\",\n",
        "                \".case-content\"\n",
        "            ]\n",
        "\n",
        "            content = None\n",
        "            for selector in content_selectors:\n",
        "                content = soup.select_one(selector)\n",
        "                if content:\n",
        "                    break\n",
        "\n",
        "            if not content:\n",
        "                # Fallback: get main content area\n",
        "                content = soup.find(\"main\") or soup.find(\"article\") or soup.find(\"div\", class_=re.compile(\"content\"))\n",
        "\n",
        "            if not content:\n",
        "                self.log.warning(f\"No content found for case {case_id}\")\n",
        "                return None\n",
        "\n",
        "            # Clean up content\n",
        "            for element in content.select(\"script, style, nav, header, footer, .nav, .header, .footer, .tools, .act-tools\"):\n",
        "                element.decompose()\n",
        "\n",
        "            # Extract text\n",
        "            text = content.get_text(separator=\"\\n\", strip=True)\n",
        "            text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "            if len(text.split()) < 100:\n",
        "                self.log.warning(f\"Case {case_id} has insufficient text: {len(text.split())} words\")\n",
        "                return None\n",
        "\n",
        "            # Extract metadata\n",
        "            metadata = self._extract_case_metadata(soup)\n",
        "\n",
        "            data = {\n",
        "                \"case_id\": case_id,\n",
        "                \"case_name\": case_name,\n",
        "                \"url\": url,\n",
        "                \"text\": text,\n",
        "                \"text_length_words\": len(text.split()),\n",
        "                \"scraped_at\": datetime.now().isoformat(),\n",
        "                \"metadata\": metadata\n",
        "            }\n",
        "\n",
        "            self.seen_case_ids.add(case_id)\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Case failed {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _extract_case_metadata(self, soup: BeautifulSoup) -> Dict[str, str]:\n",
        "        \"\"\"Extract case metadata from the page\"\"\"\n",
        "        metadata = {}\n",
        "\n",
        "        try:\n",
        "            # Look for common metadata patterns\n",
        "            meta_selectors = {\n",
        "                \"court\": [\"span.court\", \"div.court\", \"td.cell-court\"],\n",
        "                \"date\": [\"span.date\", \"div.date\", \"td.cell-date\", \"time\"],\n",
        "                \"case_number\": [\"span.case-number\", \"div.case-number\", \"td.cell-case-number\"],\n",
        "                \"judges\": [\"span.judges\", \"div.judges\", \"p.judges\"],\n",
        "                \"citation\": [\"span.citation\", \"div.citation\", \"td.cell-citation\"]\n",
        "            }\n",
        "\n",
        "            for key, selectors in meta_selectors.items():\n",
        "                for selector in selectors:\n",
        "                    element = soup.select_one(selector)\n",
        "                    if element:\n",
        "                        metadata[key] = element.get_text(strip=True)\n",
        "                        break\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log.debug(f\"Metadata extraction failed: {e}\")\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    def run_case_scrape(self, urls: List[str], handler: DataHandler) -> int:\n",
        "        self.log.info(f\"Scraping {len(urls)} cases...\")\n",
        "        saved = 0\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=self.cfg.MAX_SCRAPE_WORKERS) as executor:\n",
        "            future_to_url = {executor.submit(self.scrape_one_case, url): url for url in urls}\n",
        "\n",
        "            for future in as_completed(future_to_url):\n",
        "                url = future_to_url[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    if result and handler.save_case(result):\n",
        "                        saved += 1\n",
        "                except Exception as e:\n",
        "                    self.log.error(f\"Case scraping failed for {url}: {e}\")\n",
        "\n",
        "        return saved\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                                   MAIN                                      #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "def main() -> None:\n",
        "    cfg = Config()\n",
        "    log = setup_logging(cfg.LOG_FILE)\n",
        "    log.info(\"=== KenyaLaw Scraper v6.0 FULL (Acts + Subsidiary + Cases + Counties with PDF support) ===\")\n",
        "\n",
        "    # Check PDF support\n",
        "    if cfg.ENABLE_PDF_EXTRACTION:\n",
        "        if not PDF_SUPPORT and not PDFPLUMBER_SUPPORT:\n",
        "            log.warning(\"PDF extraction enabled but no PDF libraries found. Install: pip install pypdf2 pdfplumber\")\n",
        "        else:\n",
        "            log.info(f\"PDF extraction enabled: PyPDF2={PDF_SUPPORT}, pdfplumber={PDFPLUMBER_SUPPORT}\")\n",
        "\n",
        "    # Scrape static content\n",
        "    scrape_constitution(cfg, log)\n",
        "    scrape_acts_of_kenya(cfg, log)\n",
        "    scrape_county_legislation(cfg, log)  # Now with PDF support\n",
        "\n",
        "    # Scrape case law\n",
        "    handler = DataHandler(cfg, log)\n",
        "    scraper = KenyaLawScraper(cfg, log)\n",
        "    scraper.seen_case_ids = handler.load_existing_case_ids()\n",
        "\n",
        "    urls = scraper.fetch_case_urls_selenium()\n",
        "    new_urls = [u for u in urls if scraper._case_id_from_url(u) not in scraper.seen_case_ids]\n",
        "\n",
        "    if new_urls:\n",
        "        log.info(f\"Found {len(new_urls)} new cases to scrape\")\n",
        "        saved = scraper.run_case_scrape(new_urls, handler)\n",
        "        log.info(f\"Completed: {saved} new cases saved.\")\n",
        "    else:\n",
        "        log.info(\"No new cases found.\")\n",
        "\n",
        "    log.info(\"=== ALL DONE ===\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "cfg = Config() # Define cfg in the global scope for subsequent cells"
      ],
      "id": "1b1937e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31a44a0a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution of the large code block failed because the `main` function was called within the script itself, causing it to execute the scraping functions before they were needed for the current subtask. The error `NameError: name 'scrape_acts_of_kenya' is not defined` confirms this. To fix this and make the `cfg` object available for the loading step, the `if __name__ == \"__main__\": main()` block should be removed, and only the `cfg = Config()` line should be kept at the end to ensure `cfg` is defined in the global scope. Then, the model loading code should be re-run.\n",
        "\n"
      ],
      "id": "31a44a0a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdGkEM6JQGNH"
      },
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "import threading\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from dataclasses import dataclass, field\n",
        "from datetime import datetime\n",
        "from typing import List, Set, Dict, Optional\n",
        "from urllib.parse import urljoin\n",
        "import io\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "# Try to import webdriver_manager, but make it optional\n",
        "try:\n",
        "    from webdriver_manager.chrome import ChromeDriverManager\n",
        "    WEBDRIVER_MANAGER_SUPPORT = True\n",
        "except ImportError:\n",
        "    WEBDRIVER_MANAGER_SUPPORT = False\n",
        "    print(\"Warning: webdriver_manager not installed. Chrome driver will not be automatically managed.\")\n",
        "\n",
        "\n",
        "# Try to import PDF libraries, but make them optional\n",
        "try:\n",
        "    import PyPDF2\n",
        "    PDF_SUPPORT = True\n",
        "except ImportError:\n",
        "    PDF_SUPPORT = False\n",
        "    print(\"Warning: PyPDF2 not installed. PDF content extraction will be limited.\")\n",
        "\n",
        "try:\n",
        "    import pdfplumber\n",
        "    PDFPLUMBER_SUPPORT = True\n",
        "except ImportError:\n",
        "    PDFPLUMBER_SUPPORT = False\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                               CONFIGURATION                                 #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    BASE_PROJECT_DIR: str = os.path.join(os.path.expanduser(\"~\"), \"projects\", \"kenya_law\")\n",
        "\n",
        "    LOG_DIR: str = field(init=False)\n",
        "    DATA_DIR: str = field(init=False)\n",
        "    LOG_FILE: str = field(init=False)\n",
        "    DATA_FILE: str = field(init=False)\n",
        "    DEBUG_DIR: str = field(init=False)\n",
        "    CONSTITUTION_FILE: str = field(init=False)\n",
        "    ACTS_FILE: str = field(init=False)\n",
        "    SUBSIDIARY_FILE: str = field(init=False)\n",
        "    COUNTIES_FILE: str = field(init=False)\n",
        "    PDF_DIR: str = field(init=False)  # NEW: Directory to store downloaded PDFs\n",
        "\n",
        "    MAX_CASES: int = None\n",
        "    MAX_PAGES: int = None\n",
        "    MAX_COUNTY_LAWS: int = 50\n",
        "    REQUEST_TIMEOUT: int = 30\n",
        "    SELENIUM_TIMEOUT: int = 45\n",
        "    YEAR_START: int = 2020\n",
        "\n",
        "    BASE_URL: str = \"https://kenyalaw.org\"\n",
        "    NEW_BASE_URL: str = \"https://new.kenyalaw.org\"\n",
        "    SEARCH_URL: str = \"https://new.kenyalaw.org/search/\"\n",
        "    JUDGMENTS_URL: str = \"https://new.kenyalaw.org/judgments/\"\n",
        "    COUNTIES_URL: str = \"https://new.kenyalaw.org/legislation/counties\"\n",
        "    ACTS_TOC_URL: str = \"https://new.kenyalaw.org/legislation/\"\n",
        "\n",
        "    LOCAL_CHROMEDRIVER_PATH: Optional[str] = None\n",
        "    CHROME_HEADLESS: bool = True\n",
        "\n",
        "    KEYWORDS: List[str] = field(default_factory=lambda: [\n",
        "        \"constitution\", \"human rights\", \"land\", \"election\", \"criminal\", \"civil\",\n",
        "        \"jurisdiction\", \"appeal\", \"judicial review\", \"injunction\", \"contract\"\n",
        "    ])\n",
        "\n",
        "    MAX_SCRAPE_WORKERS: int = 10\n",
        "    ENABLE_PDF_EXTRACTION: bool = True  # NEW: Control PDF processing\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        self.LOG_DIR = os.path.join(self.BASE_PROJECT_DIR, \"logs\")\n",
        "        self.DATA_DIR = os.path.join(self.BASE_PROJECT_DIR, \"data\")\n",
        "        self.DEBUG_DIR = os.path.join(self.BASE_PROJECT_DIR, \"debug\")\n",
        "        self.PDF_DIR = os.path.join(self.DATA_DIR, \"pdfs\")  # NEW\n",
        "        self.LOG_FILE = os.path.join(self.LOG_DIR, f\"kenyalaw_scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
        "        self.DATA_FILE = os.path.join(self.DATA_DIR, \"kenya_law_training_data.jsonl\")\n",
        "        self.CONSTITUTION_FILE = os.path.join(self.DATA_DIR, \"constitution.json\")\n",
        "        self.ACTS_FILE = os.path.join(self.DATA_DIR, \"acts_of_kenya.json\")\n",
        "        self.SUBSIDIARY_FILE = os.path.join(self.DATA_DIR, \"subsidiary_legislation.json\")\n",
        "        self.COUNTIES_FILE = os.path.join(self.DATA_DIR, \"county_legislation.json\")\n",
        "\n",
        "        for d in [self.LOG_DIR, self.DATA_DIR, self.DEBUG_DIR, self.PDF_DIR]:\n",
        "            os.makedirs(d, exist_ok=True)\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                                 LOGGING                                    #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "def setup_logging(log_file: str) -> logging.Logger:\n",
        "    os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
        "    logger = logging.getLogger(\"KenyaLaw-Scraper-v6.0-FULL\")\n",
        "    logger.setLevel(logging.INFO)\n",
        "    logger.handlers.clear()\n",
        "    fh = logging.FileHandler(log_file, encoding=\"utf-8\")\n",
        "    ch = logging.StreamHandler()\n",
        "    formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
        "    fh.setFormatter(formatter)\n",
        "    ch.setFormatter(formatter)\n",
        "    logger.addHandler(fh)\n",
        "    logger.addHandler(ch)\n",
        "    return logger\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                               DATA HANDLER                                 #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "class DataHandler:\n",
        "    def __init__(self, cfg: Config, log: logging.Logger):\n",
        "        self.cfg = cfg\n",
        "        self.log = log\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    def load_existing_case_ids(self) -> Set[str]:\n",
        "        if not os.path.exists(self.cfg.DATA_FILE):\n",
        "            return set()\n",
        "        ids = set()\n",
        "        try:\n",
        "            with open(self.cfg.DATA_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "                for line_num, line in enumerate(f, 1):\n",
        "                    line = line.strip()\n",
        "                    if not line: continue\n",
        "                    try:\n",
        "                        data = json.loads(line)\n",
        "                        if \"case_id\" in data:\n",
        "                            ids.add(data[\"case_id\"])\n",
        "                    except json.JSONDecodeError:\n",
        "                        self.log.warning(f\"Bad JSON at line {line_num}\")\n",
        "            self.log.info(f\"Loaded {len(ids)} existing case IDs\")\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Failed to load IDs: {e}\")\n",
        "        return ids\n",
        "\n",
        "    def save_case(self, case: Dict) -> bool:\n",
        "        try:\n",
        "            json_line = json.dumps(case, ensure_ascii=False)\n",
        "            with self.lock:\n",
        "                with open(self.cfg.DATA_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "                    f.write(json_line + \"\\n\")\n",
        "            self.log.info(f\"Saved case {case['case_id']} ({len(case['text'].split())} words)\")\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Save failed: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                              PDF HANDLING                                  #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "class PDFHandler:\n",
        "    def __init__(self, cfg: Config, log: logging.Logger):\n",
        "        self.cfg = cfg\n",
        "        self.log = log\n",
        "\n",
        "    def extract_text_from_pdf(self, pdf_url: str, pdf_content: bytes) -> Optional[str]:\n",
        "        \"\"\"Extract text from PDF content using multiple methods\"\"\"\n",
        "        if not self.cfg.ENABLE_PDF_EXTRACTION:\n",
        "            return \"PDF_CONTENT_AVAILABLE_BUT_EXTRACTION_DISABLED\"\n",
        "\n",
        "        # Save PDF file for reference\n",
        "        pdf_filename = self._save_pdf_file(pdf_url, pdf_content)\n",
        "\n",
        "        extracted_text = None\n",
        "\n",
        "        # Try pdfplumber first (better for scanned PDFs with OCR)\n",
        "        if PDFPLUMBER_SUPPORT:\n",
        "            extracted_text = self._extract_with_pdfplumber(pdf_content)\n",
        "\n",
        "        # Fallback to PyPDF2\n",
        "        if not extracted_text and PDF_SUPPORT:\n",
        "            extracted_text = self._extract_with_pypdf2(pdf_content)\n",
        "\n",
        "        # Final fallback\n",
        "        if not extracted_text:\n",
        "            extracted_text = self._extract_fallback(pdf_content, pdf_filename)\n",
        "\n",
        "        return extracted_text\n",
        "\n",
        "    def _save_pdf_file(self, pdf_url: str, pdf_content: bytes) -> str:\n",
        "        \"\"\"Save PDF file to disk for reference\"\"\"\n",
        "        try:\n",
        "            # Create filename from URL\n",
        "            filename = re.sub(r'[^a-zA-Z0-9]', '_', pdf_url) + '.pdf'\n",
        "            filepath = os.path.join(self.cfg.PDF_DIR, filename)\n",
        "\n",
        "            with open(filepath, 'wb') as f:\n",
        "                f.write(pdf_content)\n",
        "\n",
        "            return filename\n",
        "        except Exception as e:\n",
        "            self.log.warning(f\"Failed to save PDF file: {e}\")\n",
        "            return \"unknown.pdf\"\n",
        "\n",
        "    def _extract_with_pdfplumber(self, pdf_content: bytes) -> Optional[str]:\n",
        "        \"\"\"Extract text using pdfplumber (better for scanned PDFs)\"\"\"\n",
        "        try:\n",
        "            text_parts = []\n",
        "            with pdfplumber.open(io.BytesIO(pdf_content)) as pdf:\n",
        "                for page in pdf.pages:\n",
        "                    page_text = page.extract_text()\n",
        "                    if page_text:\n",
        "                        text_parts.append(page_text.strip())\n",
        "\n",
        "            if text_parts:\n",
        "                full_text = '\\n'.join(text_parts)\n",
        "                if len(full_text.split()) > 10:  # Ensure we have substantial text\n",
        "                    return full_text\n",
        "        except Exception as e:\n",
        "            self.log.debug(f\"pdfplumber extraction failed: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _extract_with_pypdf2(self, pdf_content: bytes) -> Optional[str]:\n",
        "        \"\"\"Extract text using PyPDF2\"\"\"\n",
        "        try:\n",
        "            text_parts = []\n",
        "            pdf_file = io.BytesIO(pdf_content)\n",
        "            reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "            for page in reader.pages:\n",
        "                page_text = page.extract_text()\n",
        "                if page_text:\n",
        "                    text_parts.append(page_text.strip())\n",
        "\n",
        "            if text_parts:\n",
        "                full_text = '\\n'.join(text_parts)\n",
        "                if len(full_text.split()) > 10:\n",
        "                    return full_text\n",
        "        except Exception as e:\n",
        "            self.log.debug(f\"PyPDF2 extraction failed: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def _extract_fallback(self, pdf_content: bytes, pdf_filename: str) -> str:\n",
        "        \"\"\"Fallback method when PDF text extraction fails\"\"\"\n",
        "        self.log.warning(f\"PDF text extraction failed for {pdf_filename}. Content saved to disk.\")\n",
        "        return f\"PDF_CONTENT_UNABLE_TO_EXTRACT_TEXT_SAVED_AS_{pdf_filename}\"\n",
        "\n",
        "    def is_pdf_url(self, url: str) -> bool:\n",
        "        \"\"\"Check if URL points to a PDF document\"\"\"\n",
        "        return url.lower().endswith('.pdf') or '/pdf/' in url.lower()\n",
        "\n",
        "    def download_pdf(self, session: requests.Session, pdf_url: str) -> Optional[bytes]:\n",
        "        \"\"\"Download PDF content\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36\",\n",
        "                \"Accept\": \"application/pdf, */*\",\n",
        "                \"Referer\": \"https://new.kenyalaw.org/\"\n",
        "            }\n",
        "\n",
        "            response = session.get(pdf_url, timeout=30, headers=headers)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            content_type = response.headers.get('content-type', '').lower()\n",
        "            if 'pdf' in content_type or response.content[:4] == b'%PDF':\n",
        "                return response.content\n",
        "            else:\n",
        "                self.log.warning(f\"URL {pdf_url} doesn't contain PDF data\")\n",
        "                return None\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Failed to download PDF from {pdf_url}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                      STATIC CONTENT SCRAPERS (Constitution & Acts)          #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "def save_constitution_data(cfg: Config, log: logging.Logger, data: Dict[str, str]) -> None:\n",
        "    try:\n",
        "        with open(cfg.CONSTITUTION_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
        "        log.info(f\"Constitution saved → {cfg.CONSTITUTION_FILE}\")\n",
        "    except Exception as e:\n",
        "        log.error(f\"Failed to save constitution: {e}\")\n",
        "\n",
        "def scrape_constitution(cfg: Config, log: logging.Logger) -> None:\n",
        "    log.info(\"Scraping Constitution of Kenya (2010)...\")\n",
        "    if os.path.exists(cfg.CONSTITUTION_FILE):\n",
        "        log.info(f\"Constitution already exists → {cfg.CONSTITUTION_FILE}. Skipping.\")\n",
        "        return\n",
        "\n",
        "    # Try multiple potential sources\n",
        "    SOURCES = [\n",
        "        \"https://new.kenyalaw.org/akn/ke/act/2010/constitution/eng@2010-09-03\",\n",
        "        \"https://new.kenyalaw.org/akn/ke/act/2010/constitution\",\n",
        "        \"https://kenyalaw.org/kl/index.php?id=398\"\n",
        "    ]\n",
        "\n",
        "    session = requests.Session()\n",
        "    session.headers.update({\n",
        "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\",\n",
        "        \"Accept\": \"application/xml, text/xml, text/html, */*\",\n",
        "        \"Referer\": \"https://new.kenyalaw.org/\",\n",
        "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "    })\n",
        "\n",
        "    for source_url in SOURCES:\n",
        "        try:\n",
        "            log.info(f\"Trying source: {source_url}\")\n",
        "            resp = session.get(source_url, timeout=60)\n",
        "            resp.raise_for_status()\n",
        "\n",
        "            content_type = resp.headers.get('content-type', '').lower()\n",
        "\n",
        "            if 'xml' in content_type:\n",
        "                # Parse as XML\n",
        "                soup = BeautifulSoup(resp.content, \"xml\")\n",
        "                body = soup.find(\"body\")\n",
        "                if not body:\n",
        "                    log.warning(f\"No <body> found in XML from {source_url}\")\n",
        "                    continue\n",
        "\n",
        "                data = {}\n",
        "                current_title = \"Preamble\"\n",
        "                current_lines = []\n",
        "\n",
        "                # Extract all relevant elements\n",
        "                for elem in body.find_all(['heading', 'num', 'p', 'chapter', 'part', 'section', 'article']):\n",
        "                    if elem.name in ['heading', 'chapter', 'part']:\n",
        "                        # Save previous section\n",
        "                        if current_lines:\n",
        "                            data[current_title] = \"\\n\".join(current_lines).strip()\n",
        "                            current_lines = []\n",
        "                        # Start new section\n",
        "                        current_title = elem.get_text(strip=True)\n",
        "                        if not current_title:\n",
        "                            current_title = \"Untitled Section\"\n",
        "\n",
        "                    elif elem.name == 'p' and elem.get_text(strip=True):\n",
        "                        text = elem.get_text(strip=True)\n",
        "                        if len(text) > 10:  # Filter out very short paragraphs\n",
        "                            current_lines.append(text)\n",
        "\n",
        "                # Save the last section\n",
        "                if current_lines:\n",
        "                    data[current_title] = \"\\n\".join(current_lines).strip()\n",
        "\n",
        "            else:\n",
        "                # Parse as HTML\n",
        "                soup = BeautifulSoup(resp.content, \"html.parser\")\n",
        "\n",
        "                # Try different content selectors\n",
        "                content_selectors = [\n",
        "                    \"div.act-content\",\n",
        "                    \"div.content\",\n",
        "                    \"article\",\n",
        "                    \"main\",\n",
        "                    \"div.container\",\n",
        "                    \"#content\"\n",
        "                ]\n",
        "\n",
        "                content = None\n",
        "                for selector in content_selectors:\n",
        "                    content = soup.select_one(selector)\n",
        "                    if content:\n",
        "                        break\n",
        "\n",
        "                if not content:\n",
        "                    log.warning(f\"No content found with selectors in {source_url}\")\n",
        "                    continue\n",
        "\n",
        "                # Clean up the content\n",
        "                for element in content.select(\"script, style, nav, header, footer, .nav, .header, .footer\"):\n",
        "                    element.decompose()\n",
        "\n",
        "                # Extract text and structure\n",
        "                data = {}\n",
        "                current_section = \"Constitution of Kenya\"\n",
        "                sections = [current_section]\n",
        "                text_lines = []\n",
        "\n",
        "                for element in content.find_all(['h1', 'h2', 'h3', 'h4', 'p', 'div']):\n",
        "                    text = element.get_text(strip=True)\n",
        "                    if not text:\n",
        "                        continue\n",
        "\n",
        "                    # Detect section headers\n",
        "                    if element.name in ['h1', 'h2', 'h3', 'h4']:\n",
        "                        if text_lines:  # Save previous section\n",
        "                            data[current_section] = \"\\n\".join(text_lines).strip()\n",
        "                            text_lines = []\n",
        "                        current_section = text\n",
        "                        sections.append(current_section)\n",
        "                    elif len(text) > 20:  # Substantial content\n",
        "                        text_lines.append(text)\n",
        "\n",
        "                # Save the last section\n",
        "                if text_lines:\n",
        "                    data[current_section] = \"\\n\".join(text_lines).strip()\n",
        "\n",
        "            # Filter out empty or very short sections\n",
        "            filtered_data = {}\n",
        "            for title, content in data.items():\n",
        "                if content and len(content.split()) >= 10:\n",
        "                    clean_content = re.sub(r'\\s+', ' ', content).strip()\n",
        "                    filtered_data[title] = clean_content\n",
        "\n",
        "            # If we have substantial content, save it\n",
        "            if filtered_data and sum(len(c.split()) for c in filtered_data.values()) > 500:\n",
        "                save_constitution_data(cfg, log, filtered_data)\n",
        "                total_words = sum(len(v.split()) for v in filtered_data.values())\n",
        "                log.info(f\"SUCCESS: Constitution scraped → {len(filtered_data)} sections, {total_words:,} words\")\n",
        "                return\n",
        "            else:\n",
        "                log.warning(f\"Insufficient content from {source_url}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            log.warning(f\"Source {source_url} failed: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Fallback: manual structure if all sources fail\n",
        "    log.warning(\"All automated sources failed. Creating placeholder structure.\")\n",
        "    fallback_data = {\n",
        "        \"Preamble\": \"We, the people of Kenya—ACKNOWLEDGING the supremacy of the Almighty God of all creation...\",\n",
        "        \"Chapter One - Sovereignty of the People\": \"1. (1) All sovereign power belongs to the people of Kenya...\",\n",
        "        \"Chapter Two - The Republic\": \"4. (1) Kenya is a sovereign Republic. (2) The Republic of Kenya shall be a multi-party democratic state...\",\n",
        "        \"Note\": \"This is a placeholder. The actual constitution text could not be scraped automatically. Consider manual entry.\"\n",
        "    }\n",
        "    save_constitution_data(cfg, log, fallback_data)\n",
        "    log.info(\"Created fallback constitution structure\")\n",
        "\n",
        "def save_acts_data(cfg: Config, log: logging.Logger, acts: Dict[str, str], subs: Dict[str, str]) -> None:\n",
        "    with open(cfg.ACTS_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(acts, f, ensure_ascii=False, indent=2)\n",
        "    log.info(f\"Saved {len(acts)} Acts → {cfg.ACTS_FILE}\")\n",
        "\n",
        "    with open(cfg.SUBSIDIARY_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(subs, f, ensure_ascii=False, indent=2)\n",
        "    log.info(f\"Saved {len(subs)} Subsidiary Laws → {cfg.SUBSIDIARY_FILE}\")\n",
        "\n",
        "def save_counties_data(cfg: Config, log: logging.Logger, counties_data: Dict[str, Dict]) -> None:\n",
        "    try:\n",
        "        with open(cfg.COUNTIES_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(counties_data, f, ensure_ascii=False, indent=2)\n",
        "        log.info(f\"County legislation saved → {cfg.COUNTIES_FILE}\")\n",
        "    except Exception as e:\n",
        "        log.error(f\"Failed to save county legislation: {e}\")\n",
        "\n",
        "\n",
        "def scrape_county_legislation(cfg: Config, log: logging.Logger) -> None:\n",
        "    \"\"\"Scrape county legislation from all 47 counties\"\"\"\n",
        "    log.info(\"Scraping County Legislation from all 47 counties...\")\n",
        "    if os.path.exists(cfg.COUNTIES_FILE):\n",
        "        log.info(f\"County legislation already exists → {cfg.COUNTIES_FILE}. Skipping.\")\n",
        "        return\n",
        "\n",
        "    session = requests.Session()\n",
        "    pdf_handler = PDFHandler(cfg, log)  # NEW: PDF handler\n",
        "\n",
        "    session.headers.update({\n",
        "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\",\n",
        "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
        "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "        \"Referer\": cfg.NEW_BASE_URL\n",
        "    })\n",
        "\n",
        "    counties_data = {}\n",
        "    total_county_laws = 0\n",
        "\n",
        "    try:\n",
        "        # Get the main counties page\n",
        "        log.info(f\"Accessing counties page: {cfg.COUNTIES_URL}\")\n",
        "        resp = session.get(cfg.COUNTIES_URL, timeout=cfg.REQUEST_TIMEOUT)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "\n",
        "        # Extract all county links\n",
        "        county_links = []\n",
        "        flow_columns = soup.select('.flow-columns-group')\n",
        "\n",
        "        for column in flow_columns:\n",
        "            links = column.select('a[href^=\"/legislation/ke-\"]')\n",
        "            for link in links:\n",
        "                county_name = link.get_text(strip=True)\n",
        "                county_url = urljoin(cfg.NEW_BASE_URL, link.get('href'))\n",
        "                county_links.append((county_name, county_url))\n",
        "\n",
        "        log.info(f\"Found {len(county_links)} counties to process\")\n",
        "\n",
        "        # Process each county\n",
        "        for county_name, county_url in county_links:\n",
        "            try:\n",
        "                log.info(f\"Processing county: {county_name}\")\n",
        "                time.sleep(1)  # Be respectful\n",
        "\n",
        "                # Get county page\n",
        "                county_resp = session.get(county_url, timeout=cfg.REQUEST_TIMEOUT)\n",
        "                county_resp.raise_for_status()\n",
        "                county_soup = BeautifulSoup(county_resp.text, \"lxml\")\n",
        "\n",
        "                # Extract county laws\n",
        "                county_laws = {}\n",
        "\n",
        "                # Look for laws in tables or lists\n",
        "                law_elements = county_soup.select('tr.has-children, .legislation-item, .law-item')\n",
        "\n",
        "                if not law_elements:\n",
        "                    # Try alternative selectors\n",
        "                    law_elements = county_soup.select('a[href*=\"/akn/ke/act/\"]')\n",
        "\n",
        "                laws_processed = 0\n",
        "\n",
        "                for law_element in law_elements:\n",
        "                    if laws_processed >= cfg.MAX_COUNTY_LAWS:\n",
        "                        break\n",
        "\n",
        "                    try:\n",
        "                        # Extract law link and title\n",
        "                        if law_element.name == 'tr':\n",
        "                            link_elem = law_element.select_one('td.cell-title a')\n",
        "                        else:\n",
        "                            link_elem = law_element\n",
        "\n",
        "                        if not link_elem or not link_elem.get('href'):\n",
        "                            continue\n",
        "\n",
        "                        law_title = link_elem.get_text(strip=True)\n",
        "                        law_url = urljoin(cfg.NEW_BASE_URL, link_elem.get('href'))\n",
        "\n",
        "                        # Skip if it's not a direct law link\n",
        "                        if '/akn/ke/act/' not in law_url:\n",
        "                            continue\n",
        "\n",
        "                        # Scrape the actual law content (now with PDF support)\n",
        "                        time.sleep(0.5)\n",
        "                        law_content = scrape_county_law_content(session, pdf_handler, law_url, log)\n",
        "\n",
        "                        if law_content and len(law_content.split()) > 50:  # Reduced threshold for PDFs\n",
        "                            county_laws[law_title] = {\n",
        "                                'url': law_url,\n",
        "                                'content': law_content,\n",
        "                                'word_count': len(law_content.split()),\n",
        "                                'content_type': 'pdf' if pdf_handler.is_pdf_url(law_url) else 'html'\n",
        "                            }\n",
        "                            laws_processed += 1\n",
        "                            total_county_laws += 1\n",
        "                            log.info(f\"    ✓ County law: {law_title} ({len(law_content.split())} words) [{county_laws[law_title]['content_type'].upper()}]\")\n",
        "\n",
        "                    except Exception as e:\n",
        "                        log.warning(f\"Failed to process county law in {county_name}: {e}\")\n",
        "                        continue\n",
        "\n",
        "                # Add county data\n",
        "                if county_laws:\n",
        "                    counties_data[county_name] = {\n",
        "                        'county_url': county_url,\n",
        "                        'laws': county_laws,\n",
        "                        'total_laws': len(county_laws),\n",
        "                        'scraped_at': datetime.now().isoformat()\n",
        "                    }\n",
        "                    log.info(f\"  ✓ {county_name}: {len(county_laws)} laws\")\n",
        "\n",
        "            except Exception as e:\n",
        "                log.error(f\"Failed to process county {county_name}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Save counties data\n",
        "        save_counties_data(cfg, log, counties_data)\n",
        "        log.info(f\"SUCCESS: County legislation scraped → {len(counties_data)} counties, {total_county_laws} total laws\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log.error(f\"County legislation scrape failed: {e}\", exc_info=True)\n",
        "\n",
        "def scrape_county_law_content(session: requests.Session, pdf_handler: PDFHandler, law_url: str, log: logging.Logger) -> Optional[str]:\n",
        "    \"\"\"Scrape content of an individual county law with PDF support\"\"\"\n",
        "\n",
        "    # Check if it's a PDF URL\n",
        "    if pdf_handler.is_pdf_url(law_url):\n",
        "        log.info(f\"  Detected PDF document: {law_url}\")\n",
        "        pdf_content = pdf_handler.download_pdf(session, law_url)\n",
        "        if pdf_content:\n",
        "            return pdf_handler.extract_text_from_pdf(law_url, pdf_content)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    # Handle HTML content\n",
        "    try:\n",
        "        resp = session.get(law_url, timeout=30)\n",
        "        resp.raise_for_status()\n",
        "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "\n",
        "        # Check if the page contains a PDF link\n",
        "        pdf_links = soup.select('a[href$=\".pdf\"], a[href*=\"/pdf/\"]')\n",
        "        for pdf_link in pdf_links:\n",
        "            pdf_url = urljoin(law_url, pdf_link.get('href'))\n",
        "            log.info(f\"  Found embedded PDF link: {pdf_url}\")\n",
        "            pdf_content = pdf_handler.download_pdf(session, pdf_url)\n",
        "            if pdf_content:\n",
        "                return pdf_handler.extract_text_from_pdf(pdf_url, pdf_content)\n",
        "\n",
        "        # Try multiple content selectors for HTML laws\n",
        "        content_selectors = [\n",
        "            \"div.act-content\",\n",
        "            \"div.fr-view\",\n",
        "            \"div.content\",\n",
        "            \"article\",\n",
        "            \"main\",\n",
        "            \".law-content\",\n",
        "            \".document-content\"\n",
        "        ]\n",
        "\n",
        "        content = None\n",
        "        for selector in content_selectors:\n",
        "            content = soup.select_one(selector)\n",
        "            if content:\n",
        "                break\n",
        "\n",
        "        if not content:\n",
        "            # Fallback: get body content\n",
        "            content = soup.find('main') or soup.find('article') or soup.find('body')\n",
        "\n",
        "        if content:\n",
        "            # Clean up content\n",
        "            for element in content.select(\"script, style, nav, header, footer, .nav, .header, .footer, .tools\"):\n",
        "                element.decompose()\n",
        "\n",
        "            # Extract text\n",
        "            text = content.get_text(separator=\"\\n\", strip=True)\n",
        "            text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "            return text if len(text.split()) > 50 else None\n",
        "\n",
        "    except Exception as e:\n",
        "        log.warning(f\"Failed to scrape county law content from {law_url}: {e}\")\n",
        "\n",
        "    return None\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                                 CASE LAW SCRAPER                            #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "class KenyaLawScraper:\n",
        "    def __init__(self, cfg: Config, log: logging.Logger):\n",
        "        self.cfg = cfg\n",
        "        self.log = log\n",
        "        self.keywords = {k.lower() for k in cfg.KEYWORDS}\n",
        "        self.seen_case_ids = set()\n",
        "        self.driver = None\n",
        "        self.session = self._create_session()\n",
        "\n",
        "    def _create_session(self):\n",
        "        s = requests.Session()\n",
        "        retry = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
        "        adapter = HTTPAdapter(max_retries=retry)\n",
        "        s.mount(\"http://\", adapter)\n",
        "        s.mount(\"https://\", adapter)\n",
        "        s.headers.update({\n",
        "            \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\",\n",
        "            \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8\",\n",
        "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "            \"Accept-Encoding\": \"gzip, deflate, br\"\n",
        "        })\n",
        "        return s\n",
        "\n",
        "    def _setup_driver(self):\n",
        "        options = Options()\n",
        "        if self.cfg.CHROME_HEADLESS:\n",
        "            options.add_argument(\"--headless\")\n",
        "        options.add_argument(\"--no-sandbox\")\n",
        "        options.add_argument(\"--disable-dev-shm-usage\")\n",
        "        options.add_argument(\"--window-size=1920,1080\")\n",
        "        options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
        "        options.add_experimental_option(\"excludeSwitches\", [\"enable-automation\"])\n",
        "        options.add_experimental_option('useAutomationExtension', False)\n",
        "\n",
        "        try:\n",
        "            if WEBDRIVER_MANAGER_SUPPORT:\n",
        "                service = Service(ChromeDriverManager().install())\n",
        "            else:\n",
        "                # Fallback if webdriver_manager is not installed\n",
        "                driver_path = self.cfg.LOCAL_CHROMEDRIVER_PATH or \"/usr/bin/chromedriver\" # Assuming default path\n",
        "                service = Service(driver_path)\n",
        "\n",
        "            driver = webdriver.Chrome(service=service, options=options)\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Failed to setup Chrome driver: {e}\")\n",
        "            return None\n",
        "\n",
        "        driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
        "        driver.execute_cdp_cmd('Network.setUserAgentOverride', {\n",
        "            \"userAgent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36\"\n",
        "        })\n",
        "        return driver\n",
        "\n",
        "    def _case_id_from_url(self, url: str) -> Optional[str]:\n",
        "        # Extract case ID from new URL format: /akn/ke/judgment/kehc/2025/15858/eng@2025-11-07\n",
        "        patterns = [\n",
        "            r\"/akn/ke/judgment/[^/]+/(\\d+)/(?:eng@|\\d+)\",\n",
        "            r\"/judgments/view/(\\d+)\",\n",
        "            r\"/caselaw/cases/view/(\\d+)\"\n",
        "        ]\n",
        "        for pattern in patterns:\n",
        "            m = re.search(pattern, url)\n",
        "            if m:\n",
        "                return m.group(1)\n",
        "        return None\n",
        "\n",
        "    def fetch_case_urls_selenium(self) -> List[str]:\n",
        "        self.log.info(\"Collecting case URLs from new Kenya Law site...\")\n",
        "        urls = set()\n",
        "\n",
        "        try:\n",
        "            self.driver = self._setup_driver()\n",
        "            if not self.driver:\n",
        "                self.log.error(\"Failed to setup Selenium driver. Cannot collect case URLs.\")\n",
        "                return []\n",
        "\n",
        "            self.driver.get(self.cfg.JUDGMENTS_URL)\n",
        "            time.sleep(5)\n",
        "\n",
        "            # Wait for page to load\n",
        "            WebDriverWait(self.driver, 10).until(\n",
        "                EC.presence_of_element_located((By.TAG_NAME, \"table\"))\n",
        "            )\n",
        "\n",
        "            # Get initial page URLs\n",
        "            soup = BeautifulSoup(self.driver.page_source, \"lxml\")\n",
        "            self._extract_urls_from_page(soup, urls)\n",
        "\n",
        "            # Handle pagination\n",
        "            page_count = 0\n",
        "            while page_count < (self.cfg.MAX_PAGES or 10):  # Limit pages if specified\n",
        "                try:\n",
        "                    # Look for next button\n",
        "                    next_buttons = self.driver.find_elements(By.XPATH,\n",
        "                        \"//a[contains(text(), 'Next') or contains(@class, 'next') or contains(@aria-label, 'next')]\")\n",
        "\n",
        "                    if not next_buttons:\n",
        "                        break\n",
        "\n",
        "                    next_btn = next_buttons[0]\n",
        "                    if \"disabled\" in next_btn.get_attribute(\"class\") or not next_btn.is_enabled():\n",
        "                        break\n",
        "\n",
        "                    # Click next page\n",
        "                    self.driver.execute_script(\"arguments[0].click();\", next_btn)\n",
        "                    time.sleep(4)\n",
        "\n",
        "                    # Wait for new content to load\n",
        "                    WebDriverWait(self.driver, 10).until(\n",
        "                        EC.presence_of_element_located((By.TAG_NAME, \"table\"))\n",
        "                    )\n",
        "\n",
        "                    # Extract URLs from new page\n",
        "                    soup = BeautifulSoup(self.driver.page_source, \"lxml\")\n",
        "                    self._extract_urls_from_page(soup, urls)\n",
        "\n",
        "                    page_count += 1\n",
        "                    self.log.info(f\"Processed page {page_count}, total URLs: {len(urls)}\")\n",
        "\n",
        "                except (TimeoutException, NoSuchElementException) as e:\n",
        "                    self.log.info(\"No more pages or pagination failed\")\n",
        "                    break\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Error collecting URLs: {e}\")\n",
        "        finally:\n",
        "            if self.driver:\n",
        "                self.driver.quit()\n",
        "\n",
        "        final = list(urls)\n",
        "        if self.cfg.MAX_CASES:\n",
        "            final = final[:self.cfg.MAX_CASES]\n",
        "        self.log.info(f\"Collected {len(final)} case URLs\")\n",
        "        return final\n",
        "\n",
        "    def scrape_one_case(self, url: str) -> Optional[Dict]:\n",
        "        case_id = self._case_id_from_url(url)\n",
        "        if not case_id or case_id in self.seen_case_ids:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            self.log.info(f\"Scraping case: {url}\")\n",
        "            resp = self.session.get(url, timeout=self.cfg.REQUEST_TIMEOUT)\n",
        "            resp.raise_for_status()\n",
        "\n",
        "            soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "\n",
        "            # Extract case title\n",
        "            title_elem = soup.find(\"h1\") or soup.find(\"title\")\n",
        "            case_name = title_elem.get_text(strip=True) if title_elem else \"Unknown Case\"\n",
        "\n",
        "            # Extract case content - try multiple selectors for new site\n",
        "            content_selectors = [\n",
        "                \"div.fr-view\",  # Rich text content\n",
        "                \"div.content\",\n",
        "                \"article\",\n",
        "                \"main\",\n",
        "                \".judgment-content\",\n",
        "                \".case-content\"\n",
        "            ]\n",
        "\n",
        "            content = None\n",
        "            for selector in content_selectors:\n",
        "                content = soup.select_one(selector)\n",
        "                if content:\n",
        "                    break\n",
        "\n",
        "            if not content:\n",
        "                # Fallback: get main content area\n",
        "                content = soup.find(\"main\") or soup.find(\"article\") or soup.find(\"div\", class_=re.compile(\"content\"))\n",
        "\n",
        "            if not content:\n",
        "                self.log.warning(f\"No content found for case {case_id}\")\n",
        "                return None\n",
        "\n",
        "            # Clean up content\n",
        "            for element in content.select(\"script, style, nav, header, footer, .nav, .header, .footer, .tools, .act-tools\"):\n",
        "                element.decompose()\n",
        "\n",
        "            # Extract text\n",
        "            text = content.get_text(separator=\"\\n\", strip=True)\n",
        "            text = re.sub(r'\\n{3,}', '\\n\\n', text)\n",
        "            text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "            if len(text.split()) < 100:\n",
        "                self.log.warning(f\"Case {case_id} has insufficient text: {len(text.split())} words\")\n",
        "                return None\n",
        "\n",
        "            # Extract metadata\n",
        "            metadata = self._extract_case_metadata(soup)\n",
        "\n",
        "            data = {\n",
        "                \"case_id\": case_id,\n",
        "                \"case_name\": case_name,\n",
        "                \"url\": url,\n",
        "                \"text\": text,\n",
        "                \"text_length_words\": len(text.split()),\n",
        "                \"scraped_at\": datetime.now().isoformat(),\n",
        "                \"metadata\": metadata\n",
        "            }\n",
        "\n",
        "            self.seen_case_ids.add(case_id)\n",
        "            return data\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log.error(f\"Case failed {url}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def _extract_case_metadata(self, soup: BeautifulSoup) -> Dict[str, str]:\n",
        "        \"\"\"Extract case metadata from the page\"\"\"\n",
        "        metadata = {}\n",
        "\n",
        "        try:\n",
        "            # Look for common metadata patterns\n",
        "            meta_selectors = {\n",
        "                \"court\": [\"span.court\", \"div.court\", \"td.cell-court\"],\n",
        "                \"date\": [\"span.date\", \"div.date\", \"td.cell-date\", \"time\"],\n",
        "                \"case_number\": [\"span.case-number\", \"div.case-number\", \"td.cell-case-number\"],\n",
        "                \"judges\": [\"span.judges\", \"div.judges\", \"p.judges\"],\n",
        "                \"citation\": [\"span.citation\", \"div.citation\", \"td.cell-citation\"]\n",
        "            }\n",
        "\n",
        "            for key, selectors in meta_selectors.items():\n",
        "                for selector in selectors:\n",
        "                    element = soup.select_one(selector)\n",
        "                    if element:\n",
        "                        metadata[key] = element.get_text(strip=True)\n",
        "                        break\n",
        "\n",
        "        except Exception as e:\n",
        "            self.log.debug(f\"Metadata extraction failed: {e}\")\n",
        "\n",
        "        return metadata\n",
        "\n",
        "    def run_case_scrape(self, urls: List[str], handler: DataHandler) -> int:\n",
        "        self.log.info(f\"Scraping {len(urls)} cases...\")\n",
        "        saved = 0\n",
        "\n",
        "        with ThreadPoolExecutor(max_workers=self.cfg.MAX_SCRAPE_WORKERS) as executor:\n",
        "            future_to_url = {executor.submit(self.scrape_one_case, url): url for url in urls}\n",
        "\n",
        "            for future in as_completed(future_to_url):\n",
        "                url = future_to_url[future]\n",
        "                try:\n",
        "                    result = future.result()\n",
        "                    if result and handler.save_case(result):\n",
        "                        saved += 1\n",
        "                except Exception as e:\n",
        "                    self.log.error(f\"Case scraping failed for {url}: {e}\")\n",
        "\n",
        "        return saved\n",
        "\n",
        "\n",
        "# --------------------------------------------------------------------------- #\n",
        "#                                   MAIN                                      #\n",
        "# --------------------------------------------------------------------------- #\n",
        "\n",
        "# Define cfg in the global scope for subsequent cells\n",
        "cfg = Config()"
      ],
      "id": "pdGkEM6JQGNH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "868a7ef9"
      },
      "source": [
        "**Reasoning**:\n",
        "The `cfg` object is now defined. Proceed with loading the fine-tuned model and tokenizer using `FastLanguageModel`.\n",
        "\n"
      ],
      "id": "868a7ef9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bab0fedf"
      },
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Define the path to the saved Hugging Face model directory\n",
        "model_path = cfg.HF_OUTPUT\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=str(model_path),  # Use the path to the saved model\n",
        "    max_seq_length=cfg.MAX_SEQ_LENGTH,\n",
        "    dtype=None,  # Auto detect\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "id": "bab0fedf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "57b47480"
      },
      "source": [
        "## Define a generation function\n",
        "\n",
        "### Subtask:\n",
        "Create a Python function that takes a user query, formats it into the appropriate prompt for the model, and generates a response.\n"
      ],
      "id": "57b47480"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bc93431"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the `generate_response` function as instructed, implementing the logic for formatting the prompt, tokenizing the input, generating the response using the loaded model, decoding the output, and extracting the relevant response text.\n",
        "\n"
      ],
      "id": "1bc93431"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "958b6962"
      },
      "source": [
        "import torch\n",
        "\n",
        "def generate_response(query: str, model) -> str:\n",
        "    \"\"\"\n",
        "    Generates a legal response to a user query using the fine-tuned model.\n",
        "\n",
        "    Args:\n",
        "        query: The user's input query string.\n",
        "        model: The loaded language model object.\n",
        "        tokenizer: The loaded tokenizer object.\n",
        "\n",
        "    Returns:\n",
        "        The extracted response string from the model.\n",
        "    \"\"\"\n",
        "    # Construct the prompt in Alpaca format\n",
        "    prompt = f\"\"\"### Instruction:\n",
        "You are a helpful and knowledgeable Kenyan legal expert. Provide accurate information and guidance based on Kenyan law.\n",
        "\n",
        "### Input:\n",
        "{query}\n",
        "\n",
        "### Response:\n",
        "\"\"\"\n",
        "\n",
        "    # Tokenize the prompt\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "    # Generate the response\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=2048,  # Adjust as needed\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        use_cache=True,\n",
        "        pad_token_id=tokenizer.eos_token_id # Set pad_token_id\n",
        "    )\n",
        "\n",
        "    # Decode the generated tokens\n",
        "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract the response part\n",
        "    # The response should start after the \"### Response:\" tag\n",
        "    response_tag = \"### Response:\\n\"\n",
        "    response_start_index = generated_text.find(response_tag)\n",
        "\n",
        "    if response_start_index != -1:\n",
        "        response = generated_text[response_start_index + len(response_tag):].strip()\n",
        "    else:\n",
        "        # Fallback if the tag is not found (shouldn't happen with correct prompt)\n",
        "        response = generated_text.strip()\n",
        "\n",
        "    return response\n"
      ],
      "id": "958b6962",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b87cc31"
      },
      "source": [
        "## Create a chat interface\n",
        "\n",
        "### Subtask:\n",
        "Implement a simple loop or function that allows the user to input queries and receive responses from the model.\n"
      ],
      "id": "0b87cc31"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c5f8bcd"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement a loop to interact with the user, get their input, call the generate_response function, print the response, and provide an exit mechanism.\n",
        "\n"
      ],
      "id": "2c5f8bcd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9bd89423"
      },
      "source": [
        "print(\"Kenya Legal Chatbot (Type 'quit' or 'exit' to end)\")\n",
        "\n",
        "while True:\n",
        "    query = input(\"\\nYour query: \")\n",
        "    if query.lower() in ['quit', 'exit']:\n",
        "        print(\"Exiting chatbot. Goodbye!\")\n",
        "        break\n",
        "\n",
        "    if not query.strip():\n",
        "        print(\"Please enter a query.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        # Ensure model and tokenizer are accessible (assuming they are loaded globally)\n",
        "        response = generate_response(query, model)\n",
        "        print(\"\\nChatbot:\", response)\n",
        "    except NameError:\n",
        "        print(\"Error: Model and tokenizer are not loaded. Please run the model loading step.\")\n",
        "        break\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during response generation: {e}\")\n",
        "        break\n"
      ],
      "id": "9bd89423",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "088e5fd5"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt failed because the `model` and `tokenizer` objects were not accessible in the current scope. The model loading code is in a previous cell but was not executed in the current runtime. I need to execute the cell that loads the model and tokenizer before I can use the `generate_response` function.\n",
        "\n"
      ],
      "id": "088e5fd5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2183015f"
      },
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Define the path to the saved Hugging Face model directory\n",
        "model_path = cfg.HF_OUTPUT\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=str(model_path),  # Use the path to the saved model\n",
        "    max_seq_length=cfg.MAX_SEQ_LENGTH,\n",
        "    dtype=None,  # Auto detect\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "id": "2183015f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82430422"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that the `cfg` object does not have the `HF_OUTPUT` attribute, which is needed to specify the model path. Looking back at the notebook state, the `Config` class definition in cell `007c8229` was executed, but the `__post_init__` method, which sets `HF_OUTPUT`, seems not to have been called when `cfg = Config()` was executed in the previous successful subtask's final step. I need to re-define and instantiate `cfg` ensuring that the `__post_init__` method runs correctly to set the `HF_OUTPUT` attribute.\n",
        "\n"
      ],
      "id": "82430422"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNunD8hEVxu5"
      },
      "source": [
        "from dataclasses import dataclass, field\n",
        "import os\n",
        "from typing import List, Set, Dict, Optional\n",
        "from datetime import datetime\n",
        "import json\n",
        "import re\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    BASE_PROJECT_DIR: str = os.path.join(os.path.expanduser(\"~\"), \"projects\", \"kenya_law\")\n",
        "\n",
        "    LOG_DIR: str = field(init=False)\n",
        "    DATA_DIR: str = field(init=False)\n",
        "    LOG_FILE: str = field(init=False)\n",
        "    DATA_FILE: str = field(init=False)\n",
        "    DEBUG_DIR: str = field(init=False)\n",
        "    CONSTITUTION_FILE: str = field(init=False)\n",
        "    ACTS_FILE: str = field(init=False)\n",
        "    SUBSIDIARY_FILE: str = field(init=False)\n",
        "    COUNTIES_FILE: str = field(init=False)\n",
        "    PDF_DIR: str = field(init=False)\n",
        "\n",
        "    MAX_CASES: int = None\n",
        "    MAX_PAGES: int = None\n",
        "    MAX_COUNTY_LAWS: int = 50\n",
        "    REQUEST_TIMEOUT: int = 30\n",
        "    SELENIUM_TIMEOUT: int = 45\n",
        "    YEAR_START: int = 2020\n",
        "\n",
        "    BASE_URL: str = \"https://kenyalaw.org\"\n",
        "    NEW_BASE_URL: str = \"https://new.kenyalaw.org\"\n",
        "    SEARCH_URL: str = \"https://new.kenyalaw.org/search/\"\n",
        "    JUDGMENTS_URL: str = \"https://new.kenyalaw.org/judgments/\"\n",
        "    COUNTIES_URL: str = \"https://new.kenyalaw.org/legislation/counties\"\n",
        "    ACTS_TOC_URL: str = \"https://new.kenyalaw.org/legislation/\"\n",
        "\n",
        "    LOCAL_CHROMEDRIVER_PATH: Optional[str] = None\n",
        "    CHROME_HEADLESS: bool = True\n",
        "\n",
        "    KEYWORDS: List[str] = field(default_factory=lambda: [\n",
        "        \"constitution\", \"human rights\", \"land\", \"election\", \"criminal\", \"civil\",\n",
        "        \"jurisdiction\", \"appeal\", \"judicial review\", \"injunction\", \"contract\"\n",
        "    ])\n",
        "\n",
        "    MAX_SCRAPE_WORKERS: int = 10\n",
        "    ENABLE_PDF_EXTRACTION: bool = True\n",
        "\n",
        "    # Training Config specific additions\n",
        "    OUTPUT_DIR: str = field(init=False)\n",
        "    GGUF_OUTPUT: str = field(init=False)\n",
        "    HF_OUTPUT: str = field(init=False)\n",
        "    MODEL_NAME: str = \"unsloth/Mistral-7B-Instruct-v0.3-bnb-4bit\"\n",
        "    MAX_SEQ_LENGTH: int = 8192\n",
        "\n",
        "\n",
        "    def __post_init__(self) -> None:\n",
        "        self.LOG_DIR = os.path.join(self.BASE_PROJECT_DIR, \"logs\")\n",
        "        self.DATA_DIR = os.path.join(self.BASE_PROJECT_DIR, \"data\")\n",
        "        self.DEBUG_DIR = os.path.join(self.BASE_PROJECT_DIR, \"debug\")\n",
        "        self.PDF_DIR = os.path.join(self.DATA_DIR, \"pdfs\")\n",
        "        self.LOG_FILE = os.path.join(self.LOG_DIR, f\"kenyalaw_scraper_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log\")\n",
        "        self.DATA_FILE = os.path.join(self.DATA_DIR, \"kenya_law_training_data.jsonl\")\n",
        "        self.CONSTITUTION_FILE = os.path.join(self.DATA_DIR, \"constitution.json\")\n",
        "        self.ACTS_FILE = os.path.join(self.DATA_DIR, \"acts_of_kenya.json\")\n",
        "        self.SUBSIDIARY_FILE = os.path.join(self.DATA_DIR, \"subsidiary_legislation.json\")\n",
        "        self.COUNTIES_FILE = os.path.join(self.DATA_DIR, \"county_legislation.json\")\n",
        "\n",
        "        for d in [self.LOG_DIR, self.DATA_DIR, self.DEBUG_DIR, self.PDF_DIR]:\n",
        "            os.makedirs(d, exist_ok=True)\n",
        "\n",
        "        # Training Config specific paths\n",
        "        self.OUTPUT_DIR = os.path.join(self.BASE_PROJECT_DIR, \"kenya-legal-llm\")\n",
        "        self.GGUF_OUTPUT = os.path.join(self.OUTPUT_DIR, \"gguf\")\n",
        "        self.HF_OUTPUT = os.path.join(self.OUTPUT_DIR, \"hf\")\n",
        "\n",
        "        # Ensure output directories exist\n",
        "        for d in [self.OUTPUT_DIR, self.GGUF_OUTPUT, self.HF_OUTPUT]:\n",
        "             os.makedirs(d, exist_ok=True)\n",
        "\n",
        "\n",
        "# Re-instantiate cfg\n",
        "cfg = Config()"
      ],
      "id": "LNunD8hEVxu5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bb007ce"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `cfg` object is correctly instantiated with the `HF_OUTPUT` attribute, I can proceed with loading the model and tokenizer using Unsloth. This was the goal of the previous failed attempt and is necessary before implementing the chat loop.\n",
        "\n"
      ],
      "id": "9bb007ce"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f53ec056"
      },
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Define the path to the saved Hugging Face model directory\n",
        "model_path = cfg.HF_OUTPUT\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=str(model_path),  # Use the path to the saved model\n",
        "    max_seq_length=cfg.MAX_SEQ_LENGTH,\n",
        "    dtype=None,  # Auto detect\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\",\n",
        ")"
      ],
      "id": "f53ec056",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3359bc5"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial attempt to load the fine-tuned model failed due to the `unsloth` library not being installed.\n",
        "*   After installing the necessary libraries, the model loading failed because the `cfg` object, which should contain the path to the saved model, was not correctly defined in the execution environment.\n",
        "*   Correctly defining the `cfg` object still did not resolve the model loading issue, as the expected model files were not found at the path specified by `cfg.HF_OUTPUT`. This suggests that the preceding fine-tuning process did not successfully complete and save the model.\n",
        "*   Despite the inability to load the model, the logic for the `generate_response` function and the basic chat interface loop were successfully defined in the code.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The primary next step is to ensure the fine-tuning process successfully runs and saves the model artifacts to the location specified by `cfg.HF_OUTPUT`.\n",
        "*   Once the model is successfully saved, re-running the model loading and chat interface steps should allow the chatbot to function.\n"
      ],
      "id": "c3359bc5"
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}